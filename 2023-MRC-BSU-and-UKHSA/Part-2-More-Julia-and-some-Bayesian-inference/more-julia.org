#+SETUPFILE: ~/org-blog/setup.org
#+OPTIONS: tex:t toc:nil date:nil
#+PROPERTY: header-args:R :session :exports both :eval no
#+PROPERTY: header-args:julia :session mrc-biostats-2023-more-julia :tangle more-julia.jl :exports both :kernel julia-4-threads-1.9 :async yes :file (f-join "assets" "outputs" "more-julia" (sha1 (plist-get (cadr (org-element-at-point)) :value)))
#+EXCLUDE_TAGS: noexport
#+TODO: TODO(t) TASK(q) WARNING(w) | DONE(d) SOLUTION(s)

#+REVEAL_ROOT: https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.1.0/
#+REVEAL_MATHJAX_URL: https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML
#+REVEAL_TITLE_SLIDE: <div><div style="margin: -200px auto; opacity: 0.2;"><p><object data="assets/images/turing-logo-wide.svg"></object></p></div><h1>More Julia and some Bayesian Inference</h1><h2>with the TuringLang ecosystem</h2><p><a href="https://github.com/TuringLang">https://github.com/TuringLang</a></p><p><a href="https://github.com/TuringLang/Turing-Workshop/tree/main/2023-Geilo-Winter-School/Part-2-Turing-and-other-things">The workshop is found here</a></p></div>
#+REVEAL_EXTRA_CSS: assets/css/custom.css
#+REVEAL_THEME: white
#+REVEAL_PLUGINS: (markdown zoom)
#+REVEAL_INIT_OPTIONS: slideNumber:true
#+HTML_HEAD: <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

#+AUTHOR: Tor Erlend Fjelde
#+TITLE: More Julia and some Bayesian Inference

* Aim of these two days

/Ideally/, you walk away from this workshop with the ability to _solve whatever research problem you have with Julia and Turing.jl_

#+HTML: <div class="fragment (appear)>"
/Likely/, you walk away from this workshop with _slightly_ better understanding of how to solve your research problems with Julia and Turing.jl + _a whole lot of questions_.
#+HTML: </div>

#+HTML: <div class="fragment (appear)>"
But we will do our best
#+HTML: </div>

* The story of a little Norwegian boy
:PROPERTIES:
:ID:       2023-01-29-16-57-28-The-story-of-a-little-Norwegian-boy
:CUSTOM_ID: 2023-01-29-16-57-28-The-story-of-a-little-Norwegian-boy
:END:

#+REVEAL: split

There once was a little Norwegian boy

#+DOWNLOADED: file:///home/tor/Downloads/471337_3317365246956_1262712540_o.jpg @ 2023-01-18 14:49:24
#+ATTR_HTML: :height 400px
#+ATTR_ORG: :width 600
[[file:assets/attachments/2023-01-18_14-49-24_471337_3317365246956_1262712540_o.jpg]]


#+REVEAL: split

When this little boy was 20 years old, he was working as a parking guard near Preikestolen/Pulpit rock


#+DOWNLOADED: file:///home/tor/Downloads/Preikestolen-plateau-Go-Fjords-Bob-Engelsen-P1026771_kljg5o.jpeg @ 2023-01-18 14:57:08
#+ATTR_HTML: :height 400px
#+ATTR_ORG: :width 600
[[file:assets/attachments/2023-01-18_14-57-08_Preikestolen-plateau-Go-Fjords-Bob-Engelsen-P1026771_kljg5o.jpeg]]


#+REVEAL: split

One day it was raining and there was nobody hiking, and so there was no cars in sight for the little boy to point

#+HTML: <div class="fragment (appear)">

When his boss wasn't looking, the little 20 year-old boy had an amazing idea

#+begin_quote
Maybe I can use this method of Mr. Bayes I learned a bit about yesterday to model football / Premier League?
#+end_quote

#+HTML: </div>

#+ATTR_REVEAL: :frag (appear)
The little boy got very excited and started looking for stuff on the big interwebs

#+REVEAL: split

The little boy came across this

#+DOWNLOADED: file:///tmp/Spectacle.jWiYMk/Screenshot_20230118_144454.png @ 2023-01-18 14:46:02
[[file:assets/attachments/2023-01-18_14-46-02_Screenshot_20230118_144454.png]]

And got _very_ excited

#+REVEAL: split

But at the time, the little boy knew next to _nothing_ about programming

The little boy couldn't write the code to do the inference

#+ATTR_REVEAL: :frag (appear)
Whence the little boy became a _sad_ little boy :(

#+REVEAL: split

But time heals all wounds, and at some point the little boy learned Python

And in Python, the boy found the /probabilistic programming language/ =pymc3=

#+HTML: <div class="fragment (appear)">
#+begin_quote
Maybe I can use =pymc3= to perform inference in that football / Premier League model?
#+end_quote

And so the sad boy once more became an _excited_ little boy :)
#+HTML: </div>

#+REVEAL: split

But there was a problem

The boy wanted to write a for-loop in his model, but the model didn't want it to be so and complained!

#+ATTR_REVEAL: :frag (appear)
The boy got frustrated and gave up, once more becoming a _sad_ little boy :(

#+HTML: <div class="small-text">

#+ATTR_REVEAL: :frag (appear)
The boy should have known that the computational backend =theano= that was used by =pymc3= at the time couldn't handle a for-loop, and instead he should have used =scan=. But the boy was only 20-something years old; he didn't know.

#+HTML: </div>

#+REVEAL: split

Some years later the boy discovers a programming language called _Julia_

#+HTML: <div class="fragment (appear)">
Julia makes a few promises
#+ATTR_REVEAL: :frag (appear)
1. It's fast. Like /really/ fast.
2. It's interactive; doesn't require full compilation for you to play with it.
3. You don't have to specify types everywhere.
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
The boy thinks

#+begin_quote
Wait, but this sounds like Python but the only difference is that...I CAN WRITE FOR-LOOPS WITHOUT FEELING BAD ABOUT IT?!
#+end_quote

Yes, yes he could

#+ATTR_REVEAL: :frag (appear)
And 3.5 years later, he's still writing for-loops. Well, sort of.
#+HTML: </div>

** But it really is fast
:PROPERTIES:
:ID:       2023-01-29-16-57-28-But-it-really-is-fast
:CUSTOM_ID: 2023-01-29-16-57-28-But-it-really-is-fast
:END:


#+DOWNLOADED: file:///tmp/Spectacle.jWiYMk/Screenshot_20230118_153122.png @ 2023-01-18 15:31:28
#+CAPTION: https://julialang.org/benchmarks/ (2023-01-18)
#+ATTR_HTML: :height 400px
#+ATTR_ORG: :width 600
[[file:assets/attachments/2023-01-18_15-31-28_Screenshot_20230118_153122.png]]

#+REVEAL: split

And the consequences are
#+ATTR_REVEAL: :frag (appear)
- Even a naive implementation will be fairly fast!
  - If you want to go faster, you just optimize the code /in Julia/!
  - No need to drop down to C(++)
- ‚üπ "Every" package is written in Julia!
  - Encountered a bug? Have to debug the _Julia_ code
  - Same language as you're writing in!
- ‚üπ Same for /extending/ packages!
  - Can change functions to experiment with code you don't even own!


#+HTML: <div class="fragment (appear)"
So all in all, it can be quite nice
#+HTML: </div>

* Before we begin
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Before-we-begin
:CUSTOM_ID: 2023-01-29-16-57-28-Before-we-begin
:END:

Make sure you're in the correct directory

#+begin_src julia
pwd()
#+end_src

#+RESULTS:
: "/drive-2/Projects/public/Turing-Workshop/2023-MRC-BSU-and-UKHSA/Part-2-More-Julia-and-some-Bayesian-inference"

Then run something like (depending on which OS you are on)

#+begin_src sh :eval no
julia --project
#+end_src

or if you're already in a REPL, do

#+begin_src julia :tangle no
]activate .
#+end_src

#+RESULTS:
: [32m[1m  Activating[22m[39m project at `/drive-2/Projects/public/Turing-Workshop/2023-MRC-BSU-and-UKHSA/Part-2-More-Julia-and-some-Bayesian-inference`

to activate the project

#+REVEAL: split

And just to check that you're in the correct one

#+begin_src julia :tangle no
]status
#+end_src

#+RESULTS:
#+begin_example
[36m[1mProject[22m[39m Part2 v0.1.0
[32m[1mStatus[22m[39m `/drive-2/Projects/public/Turing-Workshop/2023-MRC-BSU-and-UKHSA/Part-2-More-Julia-and-some-Bayesian-inference/Project.toml`
  [90m[336ed68f] [39mCSV v0.10.11
[32m‚åÉ[39m [90m[052768ef] [39mCUDA v4.4.1
  [90m[124859b0] [39mDataDeps v0.7.11
  [90m[a93c6f00] [39mDataFrames v1.6.1
  [90m[2b5f629d] [39mDiffEqBase v6.129.0
  [90m[0c46a032] [39mDifferentialEquations v7.9.1
  [90m[31c24e10] [39mDistributions v0.25.100
  [90m[7073ff75] [39mIJulia v1.24.2
  [90m[7f7a1694] [39mOptimization v3.17.0
  [90m[36348300] [39mOptimizationOptimJL v0.1.9
  [90m[91a5bcdd] [39mPlots v1.39.0
  [90m[37e2e3b7] [39mReverseDiff v1.15.1
  [90m[1ed8b502] [39mSciMLSensitivity v7.39.0
  [90m[4c63d2b9] [39mStatsFuns v1.3.0
  [90m[f3b207a7] [39mStatsPlots v0.15.6
  [90m[fce5fe82] [39mTuring v0.29.1
  [90m[0db1332d] [39mTuringBenchmarking v0.3.2
  [90m[ea0860ee] [39mTuringCallbacks v0.4.0
  [90m[0004c1f4] [39mTuringGLM v2.8.1
  [90m[ade2ca70] [39mDates
[36m[1mInfo[22m[39m Packages marked with [32m‚åÉ[39m have new versions available and may be upgradable.
#+end_example

Download and install dependencies

#+begin_src julia :tangle no
]instantiate
#+end_src

#+RESULTS:

#+REVEAL: split

And finally, do

#+begin_src julia 
using Part2
#+end_src

#+RESULTS:


to get some functionality I've implemented for the occasion

* Base & Standard library
Julia is mainly a programing language for scientific computing

‚üπ Julia comes with tons of useful functionality built-in
** =Base=


[[https://docs.julialang.org/en/v1/base/base/][=Base=]] is the only module which is /always/ imported

It contains the most fundamental functionality of the language, e.g.

#+begin_src julia
@which map
#+end_src

#+RESULTS:
: Base

#+REVEAL: split

Relevant modules you'll find in =Base=

- [[https://docs.julialang.org/en/v1/base/file/][Filesystem]]
- [[https://docs.julialang.org/en/v1/base/io-network/][I/O and Network]]
- [[https://docs.julialang.org/en/v1/base/iterators/][Iterators]]
- [[https://docs.julialang.org/en/v1/base/multi-threading/][Threads]]

*** Filesystem
#+begin_src julia
pwd()  # current working directory
#+end_src

#+RESULTS:
: "/drive-2/Projects/public/Turing-Workshop/2023-MRC-BSU-and-UKHSA/Part-2-More-Julia-and-some-Bayesian-inference"

#+begin_src julia
@which pwd
#+end_src

#+RESULTS:
: Base.Filesystem

https://docs.julialang.org/en/v1/base/file/

#+REVEAL: split

While we're at it, let's make ourselves a directory for the outputs

#+begin_src julia 
outputdir(args...) = joinpath("assets", "outputs", "more-julia", args...)
#+end_src

#+RESULTS:
: outputdir (generic function with 1 method)

#+begin_src julia
outputdir()
#+end_src

#+RESULTS:
: "assets/outputs/more-julia"

#+begin_src julia
# Create it, if it doesn't exist
mkpath(outputdir())
#+end_src

#+RESULTS:
: "assets/outputs/more-julia"

*** Multi-threading

#+begin_src julia 
Threads
#+end_src

#+RESULTS:
: Base.Threads

#+begin_src julia
Threads.nthreads()
#+end_src

#+RESULTS:
: 4

Or we can call =using Threads= so so we don't have to write =Threads.=

#+begin_src julia 
using Base.Threads
#+end_src

#+RESULTS:

#+begin_src julia
nthreads()
#+end_src

#+RESULTS:
: 4

#+REVEAL: split

Making use of the threads is trivial

#+begin_src julia
Threads.@threads for i in 1:10
    println("Thread $(Threads.threadid()): $i")
end
#+end_src

#+RESULTS:
: Thread 1: 1
: Thread 4: 2
: Thread 4: 3
: Thread 4: 9
: Thread 4: 10
: Thread 3: 7
: Thread 3: 8
: Thread 2: 4
: Thread 2: 5
: Thread 2: 6

https://docs.julialang.org/en/v1/base/multi-threading/

** Standard library
These are all the packages that come with Julia but you explicitly have to load with =using=

#+HTML: <div class="side-by-side">
- [[https://docs.julialang.org/en/v1/stdlib/Pkg/][Pkg]]
- [[https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/][LinearAlgebra]]
- [[https://docs.julialang.org/en/v1/stdlib/SparseArrays/][SparseArrays]]
- [[https://docs.julialang.org/en/v1/stdlib/Statistics/][Statistics]]
- [[https://docs.julialang.org/en/v1/stdlib/Random/][Random]]
- [[https://docs.julialang.org/en/v1/stdlib/Distributed/][Distributed]]
#+HTML: </div>

#+HTML: <div class="side-by-side">
- [[https://docs.julialang.org/en/v1/stdlib/Logging/][Logging]]
- [[https://docs.julialang.org/en/v1/stdlib/Dates/][Dates]]
- [[https://docs.julialang.org/en/v1/stdlib/Serialization/][Serialization]]
- [[https://docs.julialang.org/en/v1/stdlib/Downloads/][Downloads]]
- [[https://docs.julialang.org/en/v1/stdlib/Test/][Unit testing]]
#+HTML: </div>

*** =Dates=

#+begin_src julia 
using Dates
before = Dates.now()
#+end_src

#+RESULTS:
: 2023-09-20T23:59:36.753

#+begin_src julia
Dates.now() - before
#+end_src

#+RESULTS:
: 781 milliseconds

#+begin_src julia
dump(before)
#+end_src

#+RESULTS:
: DateTime
:   instant: Dates.UTInstant{Millisecond}
:     periods: Millisecond
:       value: Int64 63830937576753

https://docs.julialang.org/en/v1/stdlib/Dates/

*** =Random=

#+begin_src julia 
using Random
#+end_src

#+RESULTS:

We can set the "global" seed

#+begin_src julia 
Random.seed!(1234)
#+end_src

#+RESULTS:
: TaskLocalRNG()

#+begin_src julia
rand()
#+end_src

#+RESULTS:
: 0.32597672886359486

#+HTML: <div class="fragment (appear)>"
Or provide the RNG explicitly

#+begin_src julia
# Xoshiro is what Julia uses by default
rng = Random.Xoshiro(1234)
rand(rng) # <= same as before
#+end_src

#+RESULTS:
: 0.32597672886359486
#+HTML: </div>

#+HTML: <div class="fragment (appear)>"
Most functions using RNGs follow this pattern of optionally accepting an RNG as the first argument
#+HTML: </div>

#+REVEAL: split

To sample multiple values, we just specify how many we want

#+begin_src julia
rand(3)
#+end_src

#+RESULTS:
: 3-element Vector{Float64}:
:  0.5490511363155669
:  0.21858665481883066
:  0.8942454282009883

#+begin_src julia
rand(3, 3)
#+end_src

#+RESULTS:
: 3√ó3 Matrix{Float64}:
:  0.520355  0.967143  0.951162
:  0.639562  0.205168  0.0739957
:  0.839622  0.527184  0.571586

#+HTML: <div class="fragment (appear)>"
And we can also specify the type of the output

#+begin_src julia
rand(Float32)
#+end_src

#+RESULTS:
: 0.07271612f0
#+HTML: </div>

#+REVEAL: split

And of course other standard sampling functions are available
#+begin_src julia 
randn()
#+end_src

#+RESULTS:
: 1.724189934074888

#+begin_src julia
randexp()
#+end_src

#+RESULTS:
: 0.04221258127478853

#+begin_src julia
# Sample uniformly from a vector
rand([1, 2, 3])
#+end_src

#+RESULTS:
: 3

And more: https://docs.julialang.org/en/v1/stdlib/Random/

*** =LinearAlgebra=

#+begin_src julia 
A = [1 2 3; 4 1 6; 7 8 1]
#+end_src

#+RESULTS:
: 3√ó3 Matrix{Int64}:
:  1  2  3
:  4  1  6
:  7  8  1

#+begin_src julia :results scalar
using LinearAlgebra

norm(A), dot(A[:, 1], A[:, 3])
#+end_src

#+RESULTS:
: (13.45362404707371, 34)

#+begin_src julia
@which norm
#+end_src

#+RESULTS:
: LinearAlgebra

Other functions are =det=, =dot=, =cholesky=, and much, much more.

https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/

*** =SparseArrays=

#+begin_src julia 
using SparseArrays
A_sparse = sparse([1, 1, 2, 3], [1, 3, 2, 3], [0, 1, 2, 0])
#+end_src

#+RESULTS:
: 3√ó3 SparseMatrixCSC{Int64, Int64} with 4 stored entries:
:  0  ‚ãÖ  1
:  ‚ãÖ  2  ‚ãÖ
:  ‚ãÖ  ‚ãÖ  0

#+begin_src julia
dropzeros(A_sparse)
#+end_src

#+RESULTS:
: 3√ó3 SparseMatrixCSC{Int64, Int64} with 2 stored entries:
:  ‚ãÖ  ‚ãÖ  1
:  ‚ãÖ  2  ‚ãÖ
:  ‚ãÖ  ‚ãÖ  ‚ãÖ

And standard array methods are applicable

#+begin_src julia
# `A` is the dense array from earlier
A * A_sparse
#+end_src

#+RESULTS:
: 3√ó3 Matrix{Int64}:
:  0   4  1
:  0   2  4
:  0  16  7

https://docs.julialang.org/en/v1/stdlib/SparseArrays/

*** =Statistics=

#+begin_src julia 
using Statistics
#+end_src

#+RESULTS:

#+begin_src julia :results scalar
mean(A), std(A)
#+end_src

#+RESULTS:
: (3.6666666666666665, 2.7386127875258306)

https://docs.julialang.org/en/v1/stdlib/Statistics/

*** =Distributed=

Functionality for parallel computation across workers (either local or remote)

#+begin_src julia 
using Distributed
nprocs()
#+end_src

#+RESULTS:
: 1

#+begin_src julia
# Spawn a local worker
addprocs(1)
#+end_src

#+RESULTS:
: 1-element Vector{Int64}:
:  2

#+begin_src julia
# Spawn a remote worker (this machine won't work on your computer)
addprocs(["tor@beastly"], tunnel=true, dir="/tmp/")
#+end_src

#+RESULTS:
: 1-element Vector{Int64}:
:  3

#+begin_src julia
nprocs()
#+end_src

#+RESULTS:
: 3

#+REVEAL: split

#+begin_src julia
# Define something on all workers
@everywhere function hostname_and_number(i)
    # Execute shell command on worker to get hostname.
    # NOTE: Using `...` syntax for shell commands.
    # This creates a `Cmd`, which is run once we call `read on it.
    hostname = read(`hostname`, String)
    # Return a tuple of worker ID, hostname and the number.
    return (myid(), i, chomp(hostname))
end
#+end_src

#+RESULTS:

#+begin_src julia
# Run the function on all workers
pmap(hostname_and_number, 1:12)
#+end_src

#+RESULTS:
#+begin_example
12-element Vector{Tuple{Int64, Int64, SubString{String}}}:
 (2, 1, "tor-Prestige-15-A10SC")
 (3, 2, "beastly")
 (2, 3, "tor-Prestige-15-A10SC")
 (3, 4, "beastly")
 (2, 5, "tor-Prestige-15-A10SC")
 (2, 6, "tor-Prestige-15-A10SC")
 (2, 7, "tor-Prestige-15-A10SC")
 (2, 8, "tor-Prestige-15-A10SC")
 (2, 9, "tor-Prestige-15-A10SC")
 (2, 10, "tor-Prestige-15-A10SC")
 (2, 11, "tor-Prestige-15-A10SC")
 (2, 12, "tor-Prestige-15-A10SC")
#+end_example

https://docs.julialang.org/en/v1/stdlib/Distributed/

*** =Logging=

#+begin_src julia 
A = ones(Int, 4, 4)
v = ones(100)
@info "Some variables"  A  s=sum(v)
#+end_src

#+RESULTS:
: [36m[1m‚îå [22m[39m[36m[1mInfo: [22m[39mSome variables
: [36m[1m‚îÇ [22m[39m  A =
: [36m[1m‚îÇ [22m[39m   4√ó4 Matrix{Int64}:
: [36m[1m‚îÇ [22m[39m    1  1  1  1
: [36m[1m‚îÇ [22m[39m    1  1  1  1
: [36m[1m‚îÇ [22m[39m    1  1  1  1
: [36m[1m‚îÇ [22m[39m    1  1  1  1
: [36m[1m‚îî [22m[39m  s = 100.0

We can also change the logger for a particular block of code

#+begin_src julia
using Logging
with_logger(NullLogger()) do
    @info "Some variables"  A  s=sum(v)
end
#+end_src

#+RESULTS:

https://docs.julialang.org/en/v1/stdlib/Logging/

** TASK Estimate $\pi$

[[./assets/outputs/more-julia/pi.gif]]

*Extra:* Parallelize it.


#+begin_src julia :exports (by-backend (reveal "none") (t "code")) :tangle yes
# Some space so you don't cheat.



















# Are you sure?
#+end_src

#+RESULTS:


** SOLUTION Estimate $\pi$

Well, Julia has irrational numbers built-in

#+begin_src julia
œÄ
#+end_src

#+RESULTS:
: œÄ = 3.1415926535897...

So you could just do

#+begin_src julia
Float64(œÄ)
#+end_src

#+RESULTS:
: 3.141592653589793

But that's not fair.

#+begin_src julia 
num_within = 0; num_total = 1_000_000
for i in 1:num_total
    x = 2 .* rand(2) .- 1
    if norm(x) < 1
        num_within += 1
    end
end
# Area of a circle = œÄr^2 = œÄ * (1/2)^2 = œÄ/4
4 * num_within / num_total
#+end_src

#+RESULTS:
: 3.143988

#+begin_src julia :exports none :exports none :tangle no
using Plots

ts = -œÄ:0.01:œÄ
p = plot(
    cos.(ts), sin.(ts);
    label="",
    size=(400, 400),
    xlim=(-1.1, 1.1),
    ylim=(-1.1, 1.1),
    ticks=:none,
    border=:none
)

anim = @animate for i=1:100
    x, y = 2 .* rand(2) .- 1
    c = x^2 + y^2 < 1 ? :green : :red
    scatter!(p, [x], [y], color=c, label="", markersize=2, markerstrokewidth=0.1)
    p
end every 1
gif(anim, outputdir("pi.gif"));
#+end_src

#+RESULTS:
: [36m[1m[ [22m[39m[36m[1mInfo: [22m[39mSaved animation to /drive-2/Projects/public/Turing-Workshop/2023-MRC-BSU-and-UKHSA/Part-2-More-Julia-and-some-Bayesian-inference/assets/outputs/more-julia/pi.gif

[[./assets/outputs/more-julia/pi.gif]]

* Scientific computing ecosystem
#+HTML: <div class="side-by-side">
- [[https://dataframes.juliadata.org/stable/][DataFrames.jl]], etc.
- [[https://docs.juliaplots.org/stable/][Plots.jl]], etc.
- [[https://juliastats.org/Distributions.jl/stable/][Distributions.jl]], etc.
- [[https://docs.sciml.ai/Optimization/stable/][Optimization.jl]] and all it contains
#+HTML: </div>

#+HTML: <div class="side-by-side">
- [[https://docs.sciml.ai/DiffEqDocs/stable/][DifferentialEquations.jl]]
- Deep learning, e.g. [[https://fluxml.ai/Flux.jl/stable/][Flux.jl]]
- Automatic Differentiation
- BenchmarkTools.jl
#+HTML: </div>

And more, of course

* Running example

An outbreak of influenza A (H1N1) in 1978 at a British boarding school

- 763 male students -> 512 of which became ill
- Reported that one infected boy started the epidemic
- Observations are number of boys in bed over 14 days

Data are freely available in the R package =outbreaks=, maintained as part of the [[http://www.repidemicsconsortium.org/][R Epidemics Consortium]]

* DataFrames.jl

#+begin_src julia :display text/plain
using DataFrames
#+end_src

#+RESULTS:

In Julia, the go-to for working with datasets is =DataFrames.jl=

#+HTML: <div class="small-text">

If you don't want to let go of the =tidyverse= and you don't mind a bunch of magic, you can use https://github.com/TidierOrg/Tidier.jl

#+HTML: </div>

#+REVEAL: split

If you're already familiar with equivalents in R or Python, the following is a great reference: https://dataframes.juliadata.org/stable/man/comparisons/

#+DOWNLOADED: file:///tmp/Spectacle.paVgAL/Screenshot_20230915_114925.png @ 2023-09-15 11:49:35
#+attr_org: :width 600px
[[file:.more-julia/attachments/2023-09-15_11-49-35_Screenshot_20230915_114925.png]]

#+REVEAL: split

There are many different ways to construct a =DataFrame=

#+begin_src julia :display text/plain
df = DataFrame(A=1:3, B=5:7, fixed=1)
#+end_src

#+RESULTS:
: [1m3√ó3 DataFrame[0m
: [1m Row [0m‚îÇ[1m A     [0m[1m B     [0m[1m fixed [0m
:      ‚îÇ[90m Int64 [0m[90m Int64 [0m[90m Int64 [0m
: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
:    1 ‚îÇ     1      5      1
:    2 ‚îÇ     2      6      1
:    3 ‚îÇ     3      7      1

#+begin_src julia :display text/plain
DataFrame(Dict("A" => 1:3, "B" => 5:7, "fixed" => 1))
#+end_src

#+RESULTS:
: [1m3√ó3 DataFrame[0m
: [1m Row [0m‚îÇ[1m A     [0m[1m B     [0m[1m fixed [0m
:      ‚îÇ[90m Int64 [0m[90m Int64 [0m[90m Int64 [0m
: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
:    1 ‚îÇ     1      5      1
:    2 ‚îÇ     2      6      1
:    3 ‚îÇ     3      7      1

Notice that columns are typed

#+REVEAL: split

Then we can interact with the =DataFrame= in a variety of ways

#+REVEAL: split

For example: indexing

#+begin_src julia 
df.A
#+end_src

#+RESULTS:
: 3-element Vector{Int64}:
:  1
:  2
:  3

#+begin_src julia
df."A"  # useful when column-names aren't valid Julia symbols
#+end_src

#+RESULTS:
: 3-element Vector{Int64}:
:  1
:  2
:  3

#+begin_src julia
df[:, "A"]
#+end_src

#+RESULTS:
: 3-element Vector{Int64}:
:  1
:  2
:  3

#+REVEAL: split

#+begin_src julia :display text/plain
df[:, [:A, :B]]
#+end_src

#+RESULTS:
: [1m3√ó2 DataFrame[0m
: [1m Row [0m‚îÇ[1m A     [0m[1m B     [0m
:      ‚îÇ[90m Int64 [0m[90m Int64 [0m
: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
:    1 ‚îÇ     1      5
:    2 ‚îÇ     2      6
:    3 ‚îÇ     3      7

#+begin_src julia :display text/plain
df[:, Not(:fixed)]
#+end_src

#+RESULTS:
: [1m3√ó2 DataFrame[0m
: [1m Row [0m‚îÇ[1m A     [0m[1m B     [0m
:      ‚îÇ[90m Int64 [0m[90m Int64 [0m
: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
:    1 ‚îÇ     1      5
:    2 ‚îÇ     2      6
:    3 ‚îÇ     3      7

#+REVEAL: split

All the standard functions are available, e.g.
- =select=
- =transform=
- =groupby=
- Different forms of =join=
- Etc.

There are too many functions to go through

See the docs https://dataframes.juliadata.org/stable/ for a thorough overview

** Actual data

Let's load the actual data

Our data is a CSV file

#+begin_src julia 
readdir("data")
#+end_src

#+RESULTS:
: 3-element Vector{String}:
:  "influenza_england_1978_school.csv"
:  "pest_data.csv"
:  "time-series.csv"

Functionality for different file formats is usually provided by separate packages:
- [[https://github.com/JuliaData/CSV.jl][CSV.jl]]
- [[https://github.com/JuliaData/Arrow.jl][Arrow.jl]] (Apache Arrow)
- [[https://github.com/JuliaData/RData.jl][RData.jl]] (R data files)
- [[https://felipenoris.github.io/XLSX.jl/stable/][XLSX.jl]] (Excel files)
- And more.

#+REVEAL: split

In our case, we're working with a CSV file, so we'll use =CSV.jl=:

#+begin_src julia :display text/plain
using CSV
datafile = CSV.File(joinpath("data", "influenza_england_1978_school.csv"));
#+end_src

#+RESULTS:

And then we can convert this =CSV.File= into a =DataFrame=

#+begin_src julia :display text/plain
data = DataFrame(datafile)
#+end_src

#+RESULTS:
#+begin_example
[1m14√ó4 DataFrame[0m
[1m Row [0m‚îÇ[1m Column1 [0m[1m date       [0m[1m in_bed [0m[1m convalescent [0m
     ‚îÇ[90m Int64   [0m[90m Date       [0m[90m Int64  [0m[90m Int64        [0m
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   1 ‚îÇ       1  1978-01-22       3             0
   2 ‚îÇ       2  1978-01-23       8             0
   3 ‚îÇ       3  1978-01-24      26             0
   4 ‚îÇ       4  1978-01-25      76             0
   5 ‚îÇ       5  1978-01-26     225             9
   6 ‚îÇ       6  1978-01-27     298            17
   7 ‚îÇ       7  1978-01-28     258           105
   8 ‚îÇ       8  1978-01-29     233           162
   9 ‚îÇ       9  1978-01-30     189           176
  10 ‚îÇ      10  1978-01-31     128           166
  11 ‚îÇ      11  1978-02-01      68           150
  12 ‚îÇ      12  1978-02-02      29            85
  13 ‚îÇ      13  1978-02-03      14            47
  14 ‚îÇ      14  1978-02-04       4            20
#+end_example

#+REVEAL: split

#+begin_quote
Woah, how does this work? We just passed a =CSV.File= to =DataFrames.DataFrame=, and _it just works_?!
#+end_quote

Aye, that's right

This is thanks to [[https://tables.juliadata.org/stable/][Tables.jl]], a simple interface for tabular data

Such light-weight interface packages allow modules to seemlessly interact with each other without explicit dependencies

This is a very typical pattern in Julia

* Distributions.jl

#+begin_src julia
using Distributions
#+end_src

#+RESULTS:

In Julia, the go-to for working with distributions is [[https://juliastats.org/Distributions.jl/stable/][=Distributions.jl=]]

This package provides a large number of distributions

Used throughout the Julia community, e.g. =Turing= uses this

#+REVEAL: split

#+begin_src julia 
dist = Normal()
#+end_src

#+RESULTS:
: Normal{Float64}(Œº=0.0, œÉ=1.0)

#+begin_src julia :results scalar
mean(dist), var(dist)
#+end_src

#+RESULTS:
: (0.0, 1.0)

Remeber the =Random.rand= function from earlier? This now also accepts a =Distribution=

#+begin_src julia 
x = rand(dist)
#+end_src

#+RESULTS:
: 0.7638013704669433

#+REVEAL: split

#+begin_src julia 
logpdf(dist, x)
#+end_src

#+RESULTS:
: -1.2106347999682632

#+begin_src julia
cdf(dist, 0.5)
#+end_src

#+RESULTS:
: 0.6914624612740131

#+begin_src julia 
quantile.(Normal(), [0.05, 0.5, 0.95])
#+end_src

#+RESULTS:
: 3-element Vector{Float64}:
:  -1.6448536269514724
:   0.0
:   1.6448536269514717

There is also maximum likelihood estimation (MLE)

#+begin_src julia
xs = rand(Normal(1, 2), 100)
fit(Normal, xs)
#+end_src

#+RESULTS:
: Normal{Float64}(Œº=1.0397121670324303, œÉ=1.8650444781860653)

#+REVEAL: split

But exactly what distributions are there?

Well, we can just check by inspecting the subtypes of =Distribution=

#+begin_src julia
# Filter away abstract types.
nonabstract_dist_subtypes = filter(!isabstracttype, subtypes(Distribution))
# Filter away types which are not found in Distributions.jl.
dist_types_from_distributions = filter(
    Base.Fix1(hasproperty, Distributions) ‚àò Symbol,
    nonabstract_dist_subtypes
)
#+end_src

#+RESULTS:
#+begin_example
84-element Vector{Any}:
 Arcsine
 Bernoulli
 BernoulliLogit
 Beta
 BetaBinomial
 BetaPrime
 Binomial
 Biweight
 Cauchy
 Chernoff
 Chi
 Chisq
 Cosine
 ‚ãÆ
 Soliton
 StudentizedRange
 SymTriangularDist
 TDist
 TriangularDist
 Triweight
 Truncated
 Uniform
 VonMises
 VonMisesFisher
 Weibull
 Wishart
#+end_example

#+REVEAL: split

Okay, there are a bit too many

Let's separate between different variate types

#+begin_src julia
filter(x -> x <: UnivariateDistribution, dist_types_from_distributions)
#+end_src

#+RESULTS:
#+begin_example
70-element Vector{Any}:
 Arcsine
 Bernoulli
 BernoulliLogit
 Beta
 BetaBinomial
 BetaPrime
 Binomial
 Biweight
 Cauchy
 Chernoff
 Chi
 Chisq
 Cosine
 ‚ãÆ
 SkewNormal
 SkewedExponentialPower
 Soliton
 StudentizedRange
 SymTriangularDist
 TDist
 TriangularDist
 Triweight
 Truncated
 Uniform
 VonMises
 Weibull
#+end_example

#+REVEAL: split

Too many

Let's convert it into a =Matrix= and force Julia to show all columns

#+begin_src julia
show(
    IOContext(stdout, :limit => false),
    "text/plain",
    reshape(filter(x -> x <: UnivariateDistribution, dist_types_from_distributions), 10, :)
)
#+end_src

#+RESULTS:
#+begin_example
10√ó7 Matrix{Any}:
 Arcsine         Chi                    Frechet                  KSDist       LogitNormal            PGeneralizedGaussian    Soliton
 Bernoulli       Chisq                  Gamma                    KSOneSided   NegativeBinomial       Pareto                  StudentizedRange
 BernoulliLogit  Cosine                 GeneralizedExtremeValue  Kolmogorov   NoncentralBeta         Poisson                 SymTriangularDist
 Beta            Dirac                  GeneralizedPareto        Kumaraswamy  NoncentralChisq        PoissonBinomial         TDist
 BetaBinomial    DiscreteNonParametric  Geometric                Laplace      NoncentralF            Rayleigh                TriangularDist
 BetaPrime       DiscreteUniform        Gumbel                   Levy         NoncentralT            Rician                  Triweight
 Binomial        Epanechnikov           Hypergeometric           Lindley      Normal                 Semicircle              Truncated
 Biweight        Erlang                 InverseGamma             LogNormal    NormalCanon            Skellam                 Uniform
 Cauchy          Exponential            InverseGaussian          LogUniform   NormalInverseGaussian  SkewNormal              VonMises
 Chernoff        FDist                  JohnsonSU                Logistic     OrderStatistic         SkewedExponentialPower  Weibull
#+end_example

#+REVEAL: split

Now for multivariate distributions

#+begin_src julia
filter(x -> x <: MultivariateDistribution, dist_types_from_distributions)
#+end_src

#+RESULTS:
: 6-element Vector{Any}:
:  Dirichlet
:  DirichletMultinomial
:  JointOrderStatistics
:  Multinomial
:  Product
:  VonMisesFisher

#+HTML: <div class="fragment (appear)>"
And matrix distributions

#+begin_src julia
filter(x -> x <: MatrixDistribution, dist_types_from_distributions)
#+end_src

#+RESULTS:
: 7-element Vector{Any}:
:  InverseWishart
:  LKJ
:  MatrixBeta
:  MatrixFDist
:  MatrixNormal
:  MatrixTDist
:  Wishart
#+HTML: </div>

* Plots.jl

#+begin_src julia 
using Plots
#+end_src

#+RESULTS:

The most commonly used plotting library is [[https://docs.juliaplots.org/stable/][Plots.jl]]

#+REVEAL: split

Has many backends, including:
- GR
- PyPlot
- Plotly
- Unicode
- PGFPlots
- And more

_But_ the code is the same for all backends

#+begin_src julia
# GR is used by default
Plots.backend()
#+end_src

#+RESULTS:
: Plots.GRBackend()

#+REVEAL: split

#+HTML: <div class="side-by-side">

#+HTML: <div>

#+begin_src julia
p1 = plot(1:10, rand(10), size=(450, 200))
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/16d623348c3f18f48e1304d2e2300b5e289e1b86.svg]]

#+HTML: </div>

#+HTML: <div>

#+begin_src julia
p2 = scatter(1:10, rand(10), size=(450, 200))
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/7cc0dffb8818815a0ac1bd1a2ed82b37365dceba.svg]]

#+HTML: </div>

#+HTML: </div>

#+begin_src julia
plot(p1, p2, layout=(1, 2), size=(800, 200))
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/98e1cf87a13e33c358badda9dd87043e02cc227a.svg]]

#+REVEAL: split

A neat example from [[https://docs.juliaplots.org/stable/#simple-is-beautiful][the docs]]

#+begin_src julia 
# Define the Lorenz attractor
Base.@kwdef mutable struct Lorenz
    dt::Float64 = 0.02
    œÉ::Float64 = 10
    œÅ::Float64 = 28
    Œ≤::Float64 = 8/3
    x::Float64 = 1
    y::Float64 = 1
    z::Float64 = 1
end

function step!(l::Lorenz)
    dx = l.œÉ * (l.y - l.x)
    dy = l.x * (l.œÅ - l.z) - l.y
    dz = l.x * l.y - l.Œ≤ * l.z
    l.x += l.dt * dx
    l.y += l.dt * dy
    l.z += l.dt * dz
end

attractor = Lorenz()
#+end_src

#+RESULTS:
: Lorenz(0.02, 10.0, 28.0, 2.6666666666666665, 1.0, 1.0, 1.0)

#+REVEAL: split

#+begin_src julia :eval no
# Initialize a 3D plot with 1 empty series
plt = plot3d(
    1,
    xlim = (-30, 30),
    ylim = (-30, 30),
    zlim = (0, 60),
    title = "Lorenz Attractor",
    legend = false,
    marker = 2,
)

# Build an animated gif by pushing new points to the plot, saving every 10th frame
anim = @animate for i=1:1500
    step!(attractor)
    push!(plt, attractor.x, attractor.y, attractor.z)
end every 10
gif(anim, outputdir("lorenz.gif"));
#+end_src

#+RESULTS:
: [36m[1m[ [22m[39m[36m[1mInfo: [22m[39mSaved animation to /drive-2/Projects/public/Turing-Workshop/2023-MRC-BSU-and-UKHSA/Part-2-More-Julia-and-some-Bayesian-inference/assets/outputs/more-julia/lorenz.gif

#+REVEAL: split

[[./assets/outputs/more-julia/lorenz.gif]]

** Ecosystem

Plots.jl also has a very nice recipe-system

Allows you to define how to plot your own types

As a result, packages often define customized plotting recipes for their types

https://docs.juliaplots.org/latest/ecosystem/#Community-packages

** StatsPlots.jl

For us, [[https://github.com/JuliaPlots/StatsPlots.jl][StatsPlots.jl]] is particularly relevant

#+begin_src julia 
using StatsPlots
#+end_src

#+RESULTS:

It contains custom plotting functionality for dataframes and distibutions

#+REVEAL: split

#+begin_src julia 
plot(Normal())
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/348bd16b3b818fcfbedfaf14f50cb941ca1b7ca3.svg]]

#+REVEAL: split

It also contains the macro =@df= for working with dataframes

#+begin_src julia 
@df data scatter(:date, :in_bed, label=nothing, ylabel="Number of students in bed")
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/1425b863a8661542a4b00f6bce46205730eb2531.svg]]

** TASK Recreate the above plot using standard Plots.jl functionality :noexport:

That is, don't use =@df data ...=, but call the =scatter= function directly

* DifferentialEquations.jl

#+begin_src julia 
using DifferentialEquations
#+end_src

#+RESULTS:

Everything related to differential equations is provided by [[https://docs.sciml.ai/DiffEqDocs/stable/][=DifferentialEquations.jl=]] and the [[https://sciml.ai/][SciML ecosystem]]

#+REVEAL: split

And I really do mean [[https://docs.sciml.ai/DiffEqDocs/stable/][/everything/]]

#+HTML: <div class="side-by-side">

#+DOWNLOADED: file:///tmp/Spectacle.jWiYMk/Screenshot_20230119_194737.png @ 2023-01-19 19:48:23
[[file:assets/attachments/2023-01-19_19-48-23_Screenshot_20230119_194737.png]]

#+DOWNLOADED: file:///tmp/Spectacle.jWiYMk/Screenshot_20230119_194838.png @ 2023-01-19 19:48:41
[[file:assets/attachments/2023-01-19_19-48-41_Screenshot_20230119_194838.png]]

#+HTML: </div>


** Differential equations

Suppose we have some function $f$ which describes how a state $x$ evolves wrt. $t$
\begin{equation*}
\frac{\mathrm{d} x}{\mathrm{d} t} = f(x, t)
\end{equation*}
which we then need to integrate to obtain the actual state at some time $t$
\begin{equation*}
x(t) = \int_{0}^{t} \frac{\mathrm{d} x}{\mathrm{d} t} \mathrm{d} t = \int_{0}^{t} f(x, t) \mathrm{d} t
\end{equation*}

In many interesting scenarios numerical methods are required to obtain $x(t)$

** Example: SIR model
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Example-SIR-model
:CUSTOM_ID: 2023-01-29-16-57-28-Example-SIR-model
:END:
One particular example of an (ordinary) differential equation that you might have seen recently is the *SIR model* used in epidemiology

#+DOWNLOADED: file:///home/tor/Downloads/sir_illu.png @ 2023-01-19 19:56:00
#+ATTR_ORG: :width 600
#+CAPTION: https://covid19.uclaml.org/model.html (2023-01-19)
[[file:assets/attachments/2023-01-19_19-56-00_sir_illu.png]]

#+REVEAL: split

The temporal dynamics of the sizes of each of the compartments are governed by the following system of ODEs:
\begin{equation*}
\begin{split}
  \frac{\mathrm{d} S}{\mathrm{d} t} &= - \beta S \frac{I}{N} \\
  \frac{\mathrm{d} I}{\mathrm{d} t} &= \beta S \frac{I}{N} - \gamma I \\
  \frac{\mathrm{d} R}{\mathrm{d} t} &= \gamma I
\end{split}
\end{equation*}
where
- $S(t)$ is the number of people susceptible to becoming infected,
- $I(t)$ is the number of people currently infected,
- $R(t)$ is the number of recovered people,
- $Œ≤$ is the constant rate of infectious contact between people,
- $\gamma$ the constant recovery rate of infected individuals

#+REVEAL: split

Converting this ODE into code is just

#+begin_src julia
const N = 763 # size of population

function SIR!(
    du,  # buffer for the updated differential equation
    u,   # current state
    p,   # parameters
    t    # current time
)
    S, I, R = u
    Œ≤, Œ≥ = p

    du[1] = dS = -Œ≤ * I * S / N
    du[2] = dI = Œ≤ * I * S / N - Œ≥ * I
    du[3] = dR = Œ≥ * I
end
#+end_src

#+RESULTS:
: SIR! (generic function with 1 method)

Not too bad!

#+REVEAL: split

Initial conditions are then
\begin{equation*}
\begin{split}
  S(0) &= N - 1 \\
  I(0) &= 1 \\
  R(0) &= 0
\end{split}
\end{equation*}
and we want to integrate from $t = 0$ to $t = 14$

#+begin_src julia
# Include 0 because that's the initial condition before any observations.
tspan = (0.0, 14.0)

# Initial conditions are:
#   S(0) = N - 1; I(0) = 1; R(0) = 0
u0 = [N - 1, 1, 0.0]
#+end_src

#+RESULTS:
: 3-element Vector{Float64}:
:  762.0
:    1.0
:    0.0

#+REVEAL: split

Now we just need to define the overall problem and we can solve:

#+begin_src julia
# Just to check that everything works, we'll just use some "totally random" values for Œ≤ and Œ≥:
problem_sir = let Œ≤ = 2.0, Œ≥ = 0.6
    ODEProblem(SIR!, u0, tspan, (Œ≤, Œ≥))
end
#+end_src

#+RESULTS:
: [38;2;86;182;194mODEProblem[0m with uType [38;2;86;182;194mVector{Float64}[0m and tType [38;2;86;182;194mFloat64[0m. In-place: [38;2;86;182;194mtrue[0m
: timespan: (0.0, 14.0)
: u0: 3-element Vector{Float64}:
:  762.0
:    1.0
:    0.0

#+REVEAL: split

Aaaand

#+begin_src julia
sol = solve(problem_sir)
#+end_src

#+RESULTS:
#+begin_example
retcode: Success
Interpolation: specialized 4th order "free" interpolation, specialized 2nd order "free" stiffness-aware interpolation
t: 23-element Vector{Float64}:
  0.0
  0.0023558376404244326
  0.025914214044668756
  0.11176872871946908
  0.26714420676761075
  0.47653584778586056
  0.7436981238065388
  1.0701182881347182
  1.4556696154809898
  1.8994815718103506
  2.4015425820305163
  2.9657488203418048
  3.6046024613854746
  4.325611232479916
  5.234036476235002
  6.073132270491685
  7.323851265223563
  8.23100744184026
  9.66046960467715
 11.027717843180652
 12.506967592177675
 13.98890399536329
 14.0
u: 23-element Vector{Vector{Float64}}:
 [762.0, 1.0, 0.0]
 [761.9952867607622, 1.003297407481751, 0.001415831756055325]
 [761.9472927630898, 1.036873767352754, 0.015833469557440357]
 [761.7584189579304, 1.1690001128296739, 0.0725809292398516]
 [761.353498610305, 1.4522140137552049, 0.19428737593979384]
 [760.6490369821046, 1.9447820690728455, 0.4061809488225752]
 [759.3950815454128, 2.8210768113583082, 0.7838416432288186]
 [757.0795798160242, 4.437564277195732, 1.4828559067800167]
 [752.6094742865345, 7.552145919430467, 2.8383797940350495]
 [743.573784947305, 13.823077731564027, 5.603137321131049]
 [724.5575481927715, 26.909267078762316, 11.533184728466205]
 [683.6474029897502, 54.51612001957392, 24.836476990675976]
 [598.1841629858786, 109.41164143668018, 55.40419557744127]
 [450.08652743810205, 192.396449154863, 120.51702340703504]
 [259.11626253270623, 256.9925778114915, 246.89115965580237]
 [148.3573731526537, 240.10301213899098, 374.53961470835543]
 [76.52998017846475, 160.6373332952353, 525.8326865263001]
 [55.70519994004921, 108.7634182279299, 598.531381832021]
 [41.39587834423381, 55.09512088924873, 666.5090007665176]
 [35.87067243374374, 27.821838135708532, 699.3074894305479]
 [33.252184333490774, 13.087185981359177, 716.6606296851502]
 [32.08996839417716, 6.105264616193066, 724.8047669896299]
 [32.08428686823946, 6.070415830241046, 724.8452973015196]
#+end_example


#+REVEAL: split

We didn't specify a solver

DifferentialEquations.jl uses =AutoTsit5(Rosenbrock32())= by default 

Which is a composition between

- =Tsit5= (4th order Runge-Kutta), and
- =Rosenbrock32= (3rd order stiff solver)

with automatic switching between the two

#+REVEAL: split

=AutoTsit5(Rosenbrock32())= covers many use-cases well, but see

- https://docs.sciml.ai/DiffEqDocs/stable/solvers/ode_solve/
- https://www.stochasticlifestyle.com/comparison-differential-equation-solver-suites-matlab-r-julia-python-c-fortran/

for more info on choosing a solver

#+REVEAL: split

This is the resulting solution

#+begin_src julia
plot(
    sol,
    linewidth=2, xaxis="Time in days", label=["Suspectible" "Infected" "Recovered"],
    alpha=0.5, size=(500, 300)
)
scatter!(1:14, data.in_bed, label="Data", color="black")
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/780e8cc1c00812abcfba1fe711f0a8b7b168b7e7.svg]]

This doesn't really match the data though; let's do better

#+REVEAL: split

*Approach #1:* find optimal values of $\beta$ and $\gamma$ by minimizing some loss, e.g. sum-of-squares

\begin{equation*}
\ell(\beta, \gamma) = \sum_{i = 1}^{14} \bigg( F(u_0, t_i;\ \beta, \gamma) - y_i \bigg)^2
\end{equation*}

where $\big( y_i \big)_{i = 1}^{14}$ are the observations, $F$ is the integrated system

** Optimization.jl

In Julia, there are /tons/ of packages for performing all kinds of optimization

[[https://docs.sciml.ai/Optimization/stable/][Optimization.jl]] provides a convenient interface to many of them

#+begin_src julia 
using Optimization
#+end_src

#+RESULTS:

#+DOWNLOADED: file:///tmp/Spectacle.paVgAL/Screenshot_20230915_130639.png @ 2023-09-15 13:06:48
#+CAPTION: https://docs.sciml.ai/Optimization/stable/#Overview-of-the-Optimizers (2023-09-15)
#+attr_org: :width 600px
#+attr_html: :width 400px
[[file:.more-julia/attachments/2023-09-15_13-06-48_Screenshot_20230915_130639.png]]


#+REVEAL: split

#+HTML: <div class="small-text">

Recall we want to solve

\begin{equation*}
\min_{\beta, \gamma} \sum_{i = 1}^{14} \bigg( F(u_0, t_i;\ \beta, \gamma) - y_i \bigg)^2
\end{equation*}

where $\big( y_i \big)_{i = 1}^{14}$ are the observations, $F$ is the integrated system

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

First we define the loss

#+begin_src julia
# Define the loss function.
function loss_sir(problem_orig, p)
    # `remake` just, well, remakes the `problem` with parameters `p` replaced.
    problem = remake(problem_orig, p=p)
    # To ensure we get solutions _exactly_ at the timesteps of interest,
    # i.e. every day we have observations, we use `saveat=1` to tell `solve`
    # to save at every timestep (which is one day).
    sol = solve(problem, saveat=1)
    # Extract the 2nd state, the (I)infected, for the dates with observations.
    sol_for_observed = sol[2, 2:15]
    # Compute the sum-of-squares of the infected vs. data.
    return sum(abs2.(sol_for_observed - data.in_bed))
end
#+end_src

#+RESULTS:
: loss_sir (generic function with 1 method)

#+HTML: </div>

#+REVEAL: split

Then we can define our =OptimizationProblem=

#+begin_src julia 
opt_problem = OptimizationProblem(
    OptimizationFunction(
        (p,_) -> loss_sir(problem_sir, p), # function to minimize
        Optimization.AutoForwardDiff()     # use ForwardDiff for automatic differentiation
    ),
    [2.0, 0.5],                            # initial values
    lb = [0, 0],                           # lower bounds on variables
    ub = [Inf, Inf],                       # upper bounds on variables
) 
#+end_src

#+RESULTS:
: [38;2;86;182;194mOptimizationProblem[0m. In-place: [38;2;86;182;194mtrue[0m
: u0: 2-element Vector{Float64}:
:  2.0
:  0.5

#+REVEAL: split

And for general /deterministic/ problems, [[https://julianlsolvers.github.io/Optim.jl/stable/][Optim.jl]] is a good choice

#+begin_src julia
using OptimizationOptimJL
opt = solve(opt_problem, NelderMead())
#+end_src

#+RESULTS:
: u: 2-element Vector{Float64}:
:  1.6692320164955483
:  0.44348639177622445

#+begin_src julia :results scalar
Œ≤, Œª = opt
Œ≤, Œª
#+end_src

#+RESULTS:
: (1.6692320164955483, 0.44348639177622445)

#+REVEAL: split

#+begin_src julia
# Solve the problem with the obtained parameters.
problem_sir = remake(problem_sir, p=(Œ≤, Œª))
sol = solve(problem_sir)

# Plot the solution.
plot(sol, linewidth=2, xaxis="Time in days", label=["Susceptible" "Infected" "Recovered"], alpha=0.5)
# And the data.
scatter!(1:14, data.in_bed, label="Data", color="black")
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/c4a9d9cc8632820164ac0b1c77cded80e46b9a39.svg]]

That's better than our /totally/ "random" guess from earlier!

** Example: SEIR model
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Example-SEIR-model
:CUSTOM_ID: 2023-01-29-16-57-28-Example-SEIR-model
:END:

Adding another compartment to our SIR model: the _(E)xposed_ state

\begin{equation*}
\begin{split}
  \frac{\mathrm{d} S}{\mathrm{d} t} &= - \beta S \frac{I}{N} \\
  \frac{\mathrm{d} {\color{blue} E}}{\mathrm{d} t} &= \beta S \frac{I}{N} - {\color{orange} \sigma} {\color{blue} E} \\
  \frac{\mathrm{d} I}{\mathrm{d} t} &= {\color{orange} \sigma} {\color{blue} E} - \gamma I \\
  \frac{\mathrm{d} R}{\mathrm{d} t} &= \gamma I
\end{split}
\end{equation*}

where we've added a new parameter ${\color{orange} \sigma}$ describing the fraction of people who develop observable symptoms in this time

** TASK Solve the SEIR model using Julia
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Solve-the-SEIR-model-using-Julia
:CUSTOM_ID: 2023-01-29-16-57-28-Solve-the-SEIR-model-using-Julia
:END:

#+begin_src julia :eval no
function SEIR!(
    du,  # buffer for the updated differential equation
    u,   # current state
    p,   # parameters
    t    # current time
)
    N = 763  # population

    S, E, I, R = u  # have ourselves an additional state!
    Œ≤, Œ≥, œÉ = p     # and an additional parameter!

    # TODO: Implement yah fool!
    du[1] = nothing
    du[2] = nothing
    du[3] = nothing
    du[4] = nothing
end
#+end_src

*BONUS:* find minimizers of sum-of-squares

#+begin_src julia :exports (by-backend (reveal "none") (t "code"))
# Some space so you don't cheat.



















# Are you sure?
#+end_src

#+RESULTS:

** SOLUTION Solve the SEIR model using Julia
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Solve-the-SEIR-model-using-Julia
:CUSTOM_ID: 2023-01-29-16-57-28-Solve-the-SEIR-model-using-Julia
:END:

#+begin_src julia
function SEIR!(
    du,  # buffer for the updated differential equation
    u,   # current state
    p,   # parameters
    t    # current time
)
    N = 763  # population
    S, E, I, R = u  # have ourselves an additional state!
    Œ≤, Œ≥, œÉ = p     # and an additional parameter!

    # Might as well cache these computations.
    Œ≤SI = Œ≤ * S * I / N
    œÉE = œÉ * E
    Œ≥I = Œ≥ * I

    du[1] = -Œ≤SI
    du[2] = Œ≤SI - œÉE
    du[3] = œÉE - Œ≥I
    du[4] = Œ≥I
end
#+end_src

#+RESULTS:
: SEIR! (generic function with 1 method)

#+REVEAL: split

#+begin_src julia
problem_seir = let u0 = [N - 1, 0, 1, 0], Œ≤ = 2.0, Œ≥ = 0.6, œÉ = 0.8
    ODEProblem(SEIR!, u0, tspan, (Œ≤, Œ≥, œÉ))
end
#+end_src

#+RESULTS:
: [38;2;86;182;194mODEProblem[0m with uType [38;2;86;182;194mVector{Int64}[0m and tType [38;2;86;182;194mFloat64[0m. In-place: [38;2;86;182;194mtrue[0m
: timespan: (0.0, 14.0)
: u0: 4-element Vector{Int64}:
:  762
:    0
:    1
:    0

#+begin_src julia
sol_seir = solve(problem_seir, saveat=1)
#+end_src

#+RESULTS:
#+begin_example
retcode: Success
Interpolation: 1st order linear
t: 15-element Vector{Float64}:
  0.0
  1.0
  2.0
  3.0
  4.0
  5.0
  6.0
  7.0
  8.0
  9.0
 10.0
 11.0
 12.0
 13.0
 14.0
u: 15-element Vector{Vector{Float64}}:
 [762.0, 0.0, 1.0, 0.0]
 [760.1497035901518, 1.277915971753478, 1.015887135649055, 0.5564933024456415]
 [757.5476928906271, 2.425869618233348, 1.6850698824327135, 1.341367608706787]
 [753.081189706403, 4.277014534677882, 2.9468385687120784, 2.6949571902067637]
 [745.3234082630842, 7.455598293492681, 5.155811621098982, 5.065181822323939]
 [731.9851682751213, 12.855816151849933, 8.960337047554939, 9.198678525473571]
 [709.5042941973462, 21.77178343781762, 15.384985521594785, 16.338936843241182]
 [672.8733895183619, 35.77263271085456, 25.88133104438007, 28.472646726403138]
 [616.390571176038, 55.9717775696742, 42.09614416178475, 48.54150709250277]
 [536.453596476594, 81.2428045994271, 64.9673325777641, 80.33626634621449]
 [436.43708330634297, 106.04037246704702, 92.9550757379631, 127.56746848864664]
 [329.60092931771436, 121.08020372279418, 120.48402926084937, 191.83483769864185]
 [233.8471941518982, 119.43669383157659, 139.3233304893263, 270.3927815271987]
 [160.88805352426687, 102.7399386960996, 143.3826208089892, 355.98938697064415]
 [111.72261866282292, 79.02493776169311, 132.78384886713565, 439.46859470834806]
#+end_example

#+REVEAL: split

#+begin_src julia
plot(sol_seir, linewidth=2, xaxis="Time in days", label=["Susceptible" "Exposed" "Infected" "Recovered"], alpha=0.5)
scatter!(1:14, data.in_bed, label="Data")
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/cf61c8af8f10097a63ca84e48ff0b4831f7d8a59.svg]]

Don't look so good. Let's try Optim.jl again.

#+REVEAL: split

#+begin_src julia
function loss_seir(problem, p)
    problem = remake(problem, p=p)
    sol = solve(problem, saveat=1)
    # NOTE: 3rd state is now the (I)nfectious compartment!!!
    sol_for_observed = sol[3, 2:15]
    return sum(abs2.(sol_for_observed - data.in_bed))
end
#+end_src

#+RESULTS:
: loss_seir (generic function with 1 method)

#+begin_src julia 
opt_problem = OptimizationProblem(
    OptimizationFunction(
        (p,_) -> loss_seir(problem_seir, p), # function to minimize
        Optimization.AutoForwardDiff()       # use ForwardDiff for automatic differentiation
    ),
    [2.0, 0.5, 0.9],                         # initial values
    lb = [0, 0, 0],                          # lower bounds on variables
    ub = [Inf, Inf, Inf],                    # upper bounds on variables
)
#+end_src

#+RESULTS:
: [38;2;86;182;194mOptimizationProblem[0m. In-place: [38;2;86;182;194mtrue[0m
: u0: 3-element Vector{Float64}:
:  2.0
:  0.5
:  0.9

#+begin_src julia
opt = solve(opt_problem, NelderMead())
#+end_src

#+RESULTS:
: u: 3-element Vector{Float64}:
:  4.853892250588215
:  0.46714672936112517
:  0.8150220601014526

#+REVEAL: split

#+begin_src julia
Œ≤, Œ≥, œÉ = opt
#+end_src

#+RESULTS:
: u: 3-element Vector{Float64}:
:  4.853892250588215
:  0.46714672936112517
:  0.8150220601014526

#+begin_src julia
sol_seir = solve(remake(problem_seir, p=(Œ≤, Œ≥, œÉ)), saveat=1)
plot(sol_seir, linewidth=2, xaxis="Time in days", label=["Susceptible" "Exposed" "Infected" "Recovered"], alpha=0.5)
scatter!(1:14, data.in_bed, label="Data", color="black")
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/42df237b1d7d830e89b857e29552483eb45ebc0c.svg]]

#+REVEAL: split

#+begin_quote
But...but these are _point estimates_! What about distributions? WHAT ABOUT UNCERTAINTY?!
#+end_quote

No, no that's fair.

Let's do some Bayesian inference then.

BUT FIRST!

** Making our future selves less annoyed
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Making-our-future-selves-less-annoyed
:CUSTOM_ID: 2023-01-29-16-57-28-Making-our-future-selves-less-annoyed
:END:

It's annoying to have all these different loss-functions for /both/ =SIR!= and =SEIR!=

#+HTML: <div class="fragment (appear)">

#+begin_src julia
# Abstract type which we can use to dispatch on.
abstract type AbstractEpidemicProblem end

struct SIRProblem{P} <: AbstractEpidemicProblem
    problem::P
    N::Int
end

function SIRProblem(N::Int; u0 = [N - 1, 1, 0.], tspan = (0, 14), p = [2.0, 0.6])
    return SIRProblem(ODEProblem(SIR!, u0, tspan, p), N)
end
#+end_src

#+RESULTS:
: SIRProblem

Then we can just construct the problem as

#+begin_src julia
sir = SIRProblem(N);
#+end_src

#+RESULTS:

#+HTML: </div>

#+REVEAL: split

And to make it a bit easier to work with, we add some utility functions

#+begin_src julia
# General.
parameters(prob::AbstractEpidemicProblem) = prob.problem.p
initial_state(prob::AbstractEpidemicProblem) = prob.problem.u0
population(prob::AbstractEpidemicProblem) = prob.N

# Specializations.
susceptible(::SIRProblem, u::AbstractMatrix) = u[1, :]
infected(::SIRProblem, u::AbstractMatrix) = u[2, :]
recovered(::SIRProblem, u::AbstractMatrix) = u[3, :]
#+end_src

#+RESULTS:
: recovered (generic function with 1 method)

So that once we've solved the problem, we can easily extract the compartment we want, e.g.

#+begin_src julia
sol = solve(sir.problem, saveat=1)
infected(sir, sol)
#+end_src

#+RESULTS:
#+begin_example
15-element Vector{Float64}:
   1.0
   4.026799533924022
  15.824575905720003
  56.779007685250534
 154.43105799061686
 248.98982384839158
 243.67838619968526
 181.93939659551984
 120.64627375763273
  75.92085282572398
  46.58644927641269
  28.214678599716414
  16.96318676577873
  10.158687874394722
   6.070415830241046
#+end_example

** TASK Implement =SEIRProblem=
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Implement-SEIRProblem
:CUSTOM_ID: 2023-01-29-16-57-28-Implement-SEIRProblem
:END:

#+begin_src julia :eval no
struct SEIRProblem <: AbstractEpidemicProblem
    # ...
end

function SEIRProblem end

susceptible
exposed
infected
recovered
#+end_src

#+begin_src julia :exports (by-backend (reveal "none") (t "code"))
# Some space so you don't cheat.



















# Are you sure?
#+end_src

#+RESULTS:

** SOLUTION Implement =SEIRProblem=
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Implement-SEIRProblem
:CUSTOM_ID: 2023-01-29-16-57-28-Implement-SEIRProblem
:END:

#+begin_src julia
struct SEIRProblem{P} <: AbstractEpidemicProblem
    problem::P
    N::Int
end

function SEIRProblem(N::Int; u0 = [N - 1, 0, 1, 0.], tspan = (0, 14), p = [4.5, 0.45, 0.8])
    return SEIRProblem(ODEProblem(SEIR!, u0, tspan, p), N)
end

susceptible(::SEIRProblem, u::AbstractMatrix) = u[1, :]
exposed(::SEIRProblem, u::AbstractMatrix) = u[2, :]
infected(::SEIRProblem, u::AbstractMatrix) = u[3, :]
recovered(::SEIRProblem, u::AbstractMatrix) = u[4, :]
#+end_src

#+RESULTS:
: recovered (generic function with 2 methods)

#+REVEAL: split

Now, given a =problem= and a =sol=, we can query the =sol= for the =infected= state _without explicit handling of which =problem= we're working with_

#+begin_src julia
seir = SEIRProblem(N);
sol = solve(seir.problem, saveat=1)
infected(seir, sol)
#+end_src

#+RESULTS:
#+begin_example
15-element Vector{Float64}:
   1.0
   1.9941817088874336
   6.9585823072029
  23.926233517606498
  74.23638542794971
 176.98368495653585
 276.06126059898344
 293.92632518571605
 249.92836195453708
 189.07578975511504
 134.2373192679034
  91.82578430804273
  61.38108478932364
  40.42264366743211
  26.357816296754425
#+end_example

** Same =loss= for both!
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Same-loss-for-both
:CUSTOM_ID: 2023-01-29-16-57-28-Same-loss-for-both
:END:

#+begin_src julia
function loss(problem_wrapper::AbstractEpidemicProblem, p)
    # NOTE: Extract the `problem` from `problem_wrapper`.
    problem = remake(problem_wrapper.problem, p=p)
    sol = solve(problem, saveat=1)
    # NOTE: Now this is completely general!
    sol_for_observed = infected(problem_wrapper, sol)[2:end]
    return sum(abs2.(sol_for_observed - data.in_bed))
end
#+end_src

#+RESULTS:
: loss (generic function with 1 method)

Now we can call the _same =loss= for both_ =SIR= and =SEIR=

#+begin_src julia 
loss(SIRProblem(N), [2.0, 0.6])
#+end_src

#+RESULTS:
: 50257.839781348805

#+begin_src julia 
loss(SEIRProblem(N), [2.0, 0.6, 0.8])
#+end_src

#+RESULTS:
: 287325.105532706

* GPU programming
#+begin_src julia 
using CUDA
#+end_src

#+RESULTS:

I'll use =CUDA= here, but there is also support for other GPU backends

For more, see https://juliagpu.org/

#+REVEAL: split

Because some of you might not have a GPU, we'll use

#+begin_src julia 
CUDA.has_cuda()
#+end_src

#+RESULTS:
: true

to avoid executing the GPU code in that case

#+REVEAL: split

#+begin_src julia 
if CUDA.has_cuda()
    CUDA.versioninfo()
end
#+end_src

#+RESULTS:
#+begin_example
CUDA runtime 11.8, artifact installation
CUDA driver 11.4
NVIDIA driver 470.199.2

CUDA libraries: 
- CUBLAS: 11.11.3
- CURAND: 10.3.0
- CUFFT: 10.9.0
- CUSOLVER: 11.4.1
- CUSPARSE: 11.7.5
- CUPTI: 18.0.0
- NVML: 11.0.0+470.199.2

Julia packages: 
- CUDA: 4.4.1
- CUDA_Driver_jll: 0.5.0+1
- CUDA_Runtime_jll: 0.6.0+0

Toolchain:
- Julia: 1.9.3
- LLVM: 14.0.6
- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0, 7.1, 7.2, 7.3, 7.4
- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80, sm_86

1 device:
  0: NVIDIA GeForce GTX 1650 with Max-Q Design (sm_75, 3.815 GiB / 3.822 GiB available)
#+end_example

#+REVEAL: split

#+begin_src julia
# Array on CPU
xs = rand(2)
#+end_src

#+RESULTS:
: 2-element Vector{Float64}:
:  0.7911106632038112
:  0.7542712130619208

#+begin_src julia
if CUDA.has_cuda()
    # Array on GPU
    xs_cuda = cu(xs)
end
#+end_src

#+RESULTS:
: 2-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:
:  0.79111063
:  0.7542712

And that's it!

#+REVEAL: split

#+begin_src julia
if CUDA.has_cuda()
    2 * xs_cuda
end
#+end_src

#+RESULTS:
: 2-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:
:  1.5822213
:  1.5085424

#+begin_src julia
if CUDA.has_cuda()
    xs_cuda .+ xs_cuda
end
#+end_src

#+RESULTS:
: 2-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:
:  1.5822213
:  1.5085424

#+REVEAL: split

#+begin_src julia
if CUDA.has_cuda()
    X_cuda = xs_cuda * xs_cuda' + 1f-2 * I
    cholesky(X_cuda)
end
#+end_src

#+RESULTS:
: Cholesky{Float32, CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}}
: U factor:
: 2√ó2 UpperTriangular{Float32, CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}}:
:  0.797406  0.748317
:   ‚ãÖ        0.137649

#+REVEAL: split

#+HTML: <div class="WARNING">
*Important:* Turing.jl is _not_ completely GPU compatible
#+HTML: </div>

You can execute all the GPU code you want /inside/ the model

But you can't use GPU for the entire computation, /yet/

Though some samplers are already GPU compatible

* Turing.jl
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Bayesian-inference
:CUSTOM_ID: 2023-01-29-16-57-28-Bayesian-inference
:END:

#+begin_src julia
using Turing
#+end_src

#+RESULTS:

and so we are finally here

#+RESULTS:

** A simple demo

#+begin_src julia :async yes
# 1. Define the model
@model function simple_demo(x, y)
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    x ~ Normal(m, sqrt(s))
    y ~ Normal(m, sqrt(s))
end
# 2. Instantiate the model, giving it some data.
model = simple_demo(1.5, 2.0)
# 3. Sample.
chain = sample(model, NUTS(), 1000);
#+end_src

#+RESULTS:
: [36m[1m‚îå [22m[39m[36m[1mInfo: [22m[39mFound initial step size
: [36m[1m‚îî [22m[39m  œµ = 0.8
: [32mSampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Time: 0:00:00[39m
:

#+REVEAL: split

#+begin_src julia 
chain
#+end_src

#+RESULTS:
#+begin_example
Chains MCMC chain (1000√ó14√ó1 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 1
Samples per chain = 1000
Wall duration     = 9.12 seconds
Compute duration  = 9.12 seconds
parameters        = s, m
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m ess_bulk [0m [1m ess_tail [0m [1m    rhat [0m [1m e[0m ‚ãØ
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m  [0m ‚ãØ

           s    1.9267    1.3645    0.0630   485.6727   577.6553    1.0054     ‚ãØ
           m    1.2265    0.7849    0.0309   674.6064   552.4304    1.0099     ‚ãØ
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m   25.0% [0m [1m   50.0% [0m [1m   75.0% [0m [1m   97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m

           s    0.5507    1.0696    1.5418    2.3495    5.1686
           m   -0.2176    0.7099    1.1755    1.6958    2.7835
#+end_example

#+REVEAL: split

#+begin_src julia 
plot(chain)
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/1e1f607dc21364143f2f8b67328de490ce367807.svg]]

#+REVEAL: split


#+begin_src julia :display text/plain :eval no :tangle no
# 1. Define the model
@model function simple_demo(x, y)
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    x ~ Normal(m, sqrt(s))
    y ~ Normal(m, sqrt(s))
end
# 2. Instantiate the model, giving it some data.
model = simple_demo(1.5, 2.0)
# 3. Sample.
chain = sample(model, NUTS(), 1000);
#+end_src

#+begin_quote
Okay, what is going on here?
#+end_quote

Let's break it down

#+REVEAL: split

To define a model in Turing.jl, we use the =@model= macro

#+begin_src julia 
@model function simple_demo(x, y)
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    x ~ Normal(m, sqrt(s))
    y ~ Normal(m, sqrt(s))
end
#+end_src

#+RESULTS:
: simple_demo (generic function with 2 methods)

which, as we can see, results in a few =simple_demo= methods

#+ATTR_REVEAL: :frag appear
- One method is for evaluation of the model
- The rest (one, here) are for constructing the =Model=

#+HTML: <div class="fragment (appear)">

#+begin_src julia 
model = simple_demo(1.5, 2.0)
#+end_src

#+RESULTS:
: Model(
:   args = (:x, :y)
:   defaults = ()
:   context = DynamicPPL.DefaultContext()
: )

#+HTML: </div>

#+REVEAL: split

In fact, we can call the =model=

#+begin_src julia 
model()
#+end_src

#+RESULTS:
: 2.0

It returns =2.0= because the last line was

#+begin_src julia :eval no :tangle no
y ~ Normal(m, sqrt(s))
#+end_src

where =y= is conditioned to be =2.0=

#+REVEAL: split

We can add an explicit =return= statement if we want

#+begin_src julia 
@model function simple_demo(x, y)
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    x ~ Normal(m, sqrt(s))
    y ~ Normal(m, sqrt(s))

    # This is just standard Julia, so we can put anything in here.
    return (; s, m, x, y, hello=42)
end
#+end_src

#+RESULTS:
: simple_demo (generic function with 2 methods)

#+begin_src julia :results scalar
model = simple_demo(1.5, 2.0)
model()
#+end_src

#+RESULTS:
: (s = 0.9042382869221636, m = -0.4180599286347479, x = 1.5, y = 2.0, hello = 42)

When we call the =model=, =s= and =m= are sampled from the prior

#+REVEAL: split

This can be very useful for debugging, e.g.

#+begin_src julia 
@model function demo_buggy()
    x ~ truncated(Normal(), -10, 0)
    y ~ Normal(0, x)
end
model_buggy = demo_buggy()
model_buggy()
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
DomainError with -1.5909802088598057:
Normal: the condition œÉ >= zero(œÉ) is not satisfied.

Stacktrace:
  [1] #371
    @ ~/.julia/packages/Distributions/Ufrz2/src/univariate/continuous/normal.jl:37 [inlined]
  [2] check_args
    @ ~/.julia/packages/Distributions/Ufrz2/src/utils.jl:89 [inlined]
  [3] #Normal#370
    @ ~/.julia/packages/Distributions/Ufrz2/src/univariate/continuous/normal.jl:37 [inlined]
  [4] Normal
    @ ~/.julia/packages/Distributions/Ufrz2/src/univariate/continuous/normal.jl:36 [inlined]
  [5] #Normal#373
    @ ~/.julia/packages/Distributions/Ufrz2/src/univariate/continuous/normal.jl:42 [inlined]
  [6] Normal(Œº::Int64, œÉ::Float64)
    @ Distributions ~/.julia/packages/Distributions/Ufrz2/src/univariate/continuous/normal.jl:42
  [7] macro expansion
    @ ~/.julia/packages/DynamicPPL/m0PXI/src/compiler.jl:555 [inlined]
  [8] demo_buggy(__model__::DynamicPPL.Model{typeof(demo_buggy), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext}, __varinfo__::DynamicPPL.ThreadSafeVarInfo{DynamicPPL.UntypedVarInfo{DynamicPPL.Metadata{Dict{AbstractPPL.VarName, Int64}, Vector{Distribution}, Vector{AbstractPPL.VarName}, Vector{Real}, Vector{Set{DynamicPPL.Selector}}}, Float64}, Vector{Base.RefValue{Float64}}}, __context__::DynamicPPL.SamplingContext{DynamicPPL.SampleFromPrior, DynamicPPL.DefaultContext, TaskLocalRNG})
    @ Main ./In[144]:3
  [9] _evaluate!!
    @ ~/.julia/packages/DynamicPPL/m0PXI/src/model.jl:963 [inlined]
 [10] evaluate_threadsafe!!
    @ ~/.julia/packages/DynamicPPL/m0PXI/src/model.jl:952 [inlined]
 [11] evaluate!!
    @ ~/.julia/packages/DynamicPPL/m0PXI/src/model.jl:887 [inlined]
 [12] evaluate!! (repeats 2 times)
    @ ~/.julia/packages/DynamicPPL/m0PXI/src/model.jl:900 [inlined]
 [13] evaluate!!
    @ ~/.julia/packages/DynamicPPL/m0PXI/src/model.jl:908 [inlined]
 [14] (::DynamicPPL.Model{typeof(demo_buggy), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext})()
    @ DynamicPPL ~/.julia/packages/DynamicPPL/m0PXI/src/model.jl:860
 [15] top-level scope
    @ In[144]:6
#+end_example
:END:

#+REVEAL: split

Let's insert some good old-fashioned print-statements

#+begin_src julia 
@model function demo_buggy()
    x ~ truncated(Normal(), -10, 0)
    println("x=$x")
    y ~ Normal(0, x)
    println("y=$y")
end
model_buggy = demo_buggy()
model_buggy()
#+end_src

#+RESULTS:
:RESULTS:
: x=-0.39059114202501893
# [goto error]
#+begin_example
DomainError with -0.39059114202501893:
Normal: the condition œÉ >= zero(œÉ) is not satisfied.

Stacktrace:
  [1] #371
    @ ~/.julia/packages/Distributions/Ufrz2/src/univariate/continuous/normal.jl:37 [inlined]
  [2] check_args
    @ ~/.julia/packages/Distributions/Ufrz2/src/utils.jl:89 [inlined]
  [3] #Normal#370
    @ ~/.julia/packages/Distributions/Ufrz2/src/univariate/continuous/normal.jl:37 [inlined]
  [4] Normal
    @ ~/.julia/packages/Distributions/Ufrz2/src/univariate/continuous/normal.jl:36 [inlined]
  [5] #Normal#373
    @ ~/.julia/packages/Distributions/Ufrz2/src/univariate/continuous/normal.jl:42 [inlined]
  [6] Normal(Œº::Int64, œÉ::Float64)
    @ Distributions ~/.julia/packages/Distributions/Ufrz2/src/univariate/continuous/normal.jl:42
  [7] demo_buggy(__model__::DynamicPPL.Model{typeof(demo_buggy), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext}, __varinfo__::DynamicPPL.ThreadSafeVarInfo{DynamicPPL.UntypedVarInfo{DynamicPPL.Metadata{Dict{AbstractPPL.VarName, Int64}, Vector{Distribution}, Vector{AbstractPPL.VarName}, Vector{Real}, Vector{Set{DynamicPPL.Selector}}}, Float64}, Vector{Base.RefValue{Float64}}}, __context__::DynamicPPL.SamplingContext{DynamicPPL.SampleFromPrior, DynamicPPL.DefaultContext, TaskLocalRNG})
    @ Main ./In[145]:4
  [8] _evaluate!!
    @ ~/.julia/packages/DynamicPPL/m0PXI/src/model.jl:963 [inlined]
  [9] evaluate_threadsafe!!(model::DynamicPPL.Model{typeof(demo_buggy), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext}, varinfo::DynamicPPL.UntypedVarInfo{DynamicPPL.Metadata{Dict{AbstractPPL.VarName, Int64}, Vector{Distribution}, Vector{AbstractPPL.VarName}, Vector{Real}, Vector{Set{DynamicPPL.Selector}}}, Float64}, context::DynamicPPL.SamplingContext{DynamicPPL.SampleFromPrior, DynamicPPL.DefaultContext, TaskLocalRNG})
    @ DynamicPPL ~/.julia/packages/DynamicPPL/m0PXI/src/model.jl:952
 [10] evaluate!!(model::DynamicPPL.Model{typeof(demo_buggy), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext}, varinfo::DynamicPPL.UntypedVarInfo{DynamicPPL.Metadata{Dict{AbstractPPL.VarName, Int64}, Vector{Distribution}, Vector{AbstractPPL.VarName}, Vector{Real}, Vector{Set{DynamicPPL.Selector}}}, Float64}, context::DynamicPPL.SamplingContext{DynamicPPL.SampleFromPrior, DynamicPPL.DefaultContext, TaskLocalRNG})
    @ DynamicPPL ~/.julia/packages/DynamicPPL/m0PXI/src/model.jl:887
 [11] evaluate!! (repeats 2 times)
    @ ~/.julia/packages/DynamicPPL/m0PXI/src/model.jl:900 [inlined]
 [12] evaluate!!
    @ ~/.julia/packages/DynamicPPL/m0PXI/src/model.jl:908 [inlined]
 [13] (::DynamicPPL.Model{typeof(demo_buggy), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext})()
    @ DynamicPPL ~/.julia/packages/DynamicPPL/m0PXI/src/model.jl:860
 [14] top-level scope
    @ In[145]:8
#+end_example
:END:

#+REVEAL: split

=x= is negative ‚ü∂ let's fix that

#+begin_src julia 
@model function demo_buggy()
    x ~ truncated(Normal(), 0, 10)
    println("x=$x")
    y ~ Normal(0, x)
    println("y=$y")
end
model_buggy = demo_buggy()
model_buggy()
#+end_src

#+RESULTS:
: x=0.7899722982041668
: y=0.4623970224585408

It works!

#+REVEAL: split

But let's get back to our =simple_demo= example

#+begin_src julia :eval no :tangle no
@model function simple_demo(x, y)
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    x ~ Normal(m, sqrt(s))
    y ~ Normal(m, sqrt(s))
end
#+end_src


#+HTML: <div class="fragment (appear)"
We've seen a =function= before, so that part isn't new

#+begin_src julia :eval no :tangle no
@model function simple_demo(x)
    ...
end
#+end_src

Roughly, =@model= "transforms" the function =simple_demo= in a "certain way"
#+HTML: </div>

#+HTML: <div class="x-small-text">

#+HTML: <div class="fragment (appear)">

If you /really/ want to have a look, you can execute the following code block

#+begin_src julia :exports code :eval no :tangle no
@macroexpand @model function demo()
    x ~ Normal()
    return nothing
end
#+end_src

#+RESULTS:
#+begin_example
quote
    function demo(__model__::DynamicPPL.Model, __varinfo__::DynamicPPL.AbstractVarInfo, __context__::AbstractPPL.AbstractContext; )
        #= In[29]:1 =#
        begin
            #= In[29]:1 =#
            #= In[29]:2 =#
            begin
                var"##dist#437" = Normal()
                var"##vn#434" = (DynamicPPL.resolve_varnames)((AbstractPPL.VarName){:x}(), var"##dist#437")
                var"##isassumption#435" = begin
                        if (DynamicPPL.contextual_isassumption)(__context__, var"##vn#434")
                            if !((DynamicPPL.inargnames)(var"##vn#434", __model__)) || (DynamicPPL.inmissings)(var"##vn#434", __model__)
                                true
                            else
                                x === missing
                            end
                        else
                            false
                        end
                    end
                if (DynamicPPL.contextual_isfixed)(__context__, var"##vn#434")
                    x = (DynamicPPL.getfixed_nested)(__context__, var"##vn#434")
                elseif var"##isassumption#435"
                    begin
                        (var"##value#438", __varinfo__) = (DynamicPPL.tilde_assume!!)(__context__, (DynamicPPL.unwrap_right_vn)((DynamicPPL.check_tilde_rhs)(var"##dist#437"), var"##vn#434")..., __varinfo__)
                        x = var"##value#438"
                        var"##value#438"
                    end
                else
                    if !((DynamicPPL.inargnames)(var"##vn#434", __model__))
                        x = (DynamicPPL.getconditioned_nested)(__context__, var"##vn#434")
                    end
                    (var"##value#436", __varinfo__) = (DynamicPPL.tilde_observe!!)(__context__, (DynamicPPL.check_tilde_rhs)(var"##dist#437"), x, var"##vn#434", __varinfo__)
                    var"##value#436"
                end
            end
            #= In[29]:3 =#
            begin
                #= /home/tor/.julia/packages/DynamicPPL/YThRW/src/compiler.jl:555 =#
                var"##retval#439" = nothing
                #= /home/tor/.julia/packages/DynamicPPL/YThRW/src/compiler.jl:556 =#
                return (var"##retval#439", __varinfo__)
            end
        end
    end
    begin
        $(Expr(:meta, :doc))
        function demo(; )
            #= In[29]:1 =#
            return (DynamicPPL.Model)(demo, NamedTuple{()}(()); )
        end
    end
end
#+end_example

to see the actual code being generated

#+HTML: </div>

#+HTML: </div>

#+REVEAL: split

Then we have the "tilde-statements"

#+begin_src julia :eval no :tangle no
s ~ InverseGamma(2, 3)
m ~ Normal(0, sqrt(s))

x ~ Normal(m, sqrt(s))
y ~ Normal(m, sqrt(s))
#+end_src

#+HTML: <div class="fragment (appear)">
_Important:_ only lines of the form =LEFT ~ RIGHT= are touched by =@model=
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
‚üπ Everything that is _not_ of the form =LEFT ~ RIGHT= is _not_ touched by =@model=

#+begin_quote
If it's valid Julia code, it's valid inside a =@model= block
#+end_quote

#+HTML: </div>

#+REVEAL: split

But in our simple demo model, =s= and =m= are treated differently than =x= and =y=

=s= and =m= are considered as random variables to be inferred

=x= and =y= are considered as data / conditioned

#+HTML: <div class="fragment (appear)">

Basically, =L ~ R= is considered a /conditioned/ variable if either
1. =L= is present in the arguments of the function defining the model, or
2. =L= is /conditioned/ using =model | (L9 = ..., )= or similar.
3. =L= is a /literal/, e.g. =1.5 ~ Normal()=.

_Otherwise_, =L= is considered a random variable

#+HTML: </div>

#+REVEAL: split

The following are all equivalent

#+begin_src julia :results none
# (1): using the arguments of the function
@model function simple_demo_v1(x, y)
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    x ~ Normal(m, sqrt(s))
    y ~ Normal(m, sqrt(s))
end
model_v1 = simple_demo_v1(1.5, 2.0)
#+end_src

#+begin_src julia :results none
# (2): using the `|` operator / `condition`
@model function simple_demo_v2()
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    x ~ Normal(m, sqrt(s))
    y ~ Normal(m, sqrt(s))
end
model_v2 = simple_demo_v2() | (x = 1.5, y = 2.0)
#+end_src

#+begin_src julia :results none
# (3): when `L` is a literal
@model function simple_demo_v3()
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    1.5 ~ Normal(m, sqrt(s))
    2.0 ~ Normal(m, sqrt(s))
end
model_v3 = simple_demo_v3()
#+end_src

#+REVEAL: split

#+begin_src julia :async yes
with_logger(NullLogger()) do  # just surpressing the log output for presentation
    chain_v1 = sample(model_v1, NUTS(), 1000; progress=false)
    chain_v2 = sample(model_v2, NUTS(), 1000; progress=false)
    chain_v3 = sample(model_v3, NUTS(), 1000; progress=false)
    plot(chainscat(chain_v1, chain_v2, chain_v3))
end
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/ce69f397ca10a9e2661b065d19ad1468ed77de9a.svg]]

#+REVEAL: split

One thing that Turing.jl cannot handle is the following

#+begin_src julia 
@model function simple_demo_v1_failure(x, y)
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    x ~ Normal(m, sqrt(s))
    # Used to be: y ~ Normal(m, sqrt(s))
    z = y
    z ~ Normal(m, sqrt(s))
end

model_v1_failure = simple_demo_v1_failure(1.5, 2.0)
model_v1_failure() # `y` is treated as a random variable!!!
#+end_src

#+RESULTS:
: -1.6045141567134922

Turing.jl performs no analysis of the code ‚üπ don't know that =y= is constant

#+REVEAL: split

=decondition= can be used to "undo" the conditioning of a variable

#+HTML: <div class="fragment (appear)"
For _argument-based conditioning_, we need to replace it with =missing=

#+begin_src julia 
model_v1_decondition = simple_demo_v1(1.5, missing)
model_v1_decondition()  # `y` is now a random variable
#+end_src

#+RESULTS:
: 0.011085659616252741

#+HTML: </div>

#+HTML: <div class="fragment (appear)"
For _|-based conditioning_, we can just call =decondition=

#+begin_src julia
model_v2_decondition = DynamicPPL.decondition(model_v2, @varname(y))
model_v2_decondition()  # `y` is now a random variable!
#+end_src

#+RESULTS:
: 0.3154944722541396
#+HTML: </div>

#+HTML: <div class="fragment (appear)"
For _literal-based conditioning_, =y= is hard-coded, so deconditioning is not possible
#+HTML: </div>

#+REVEAL: split

Overall, |-based conditioning is preferred, i.e.

#+begin_src julia :eval no :tangle no
@model function simple_demo_v2()
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    x ~ Normal(m, sqrt(s))
    y ~ Normal(m, sqrt(s))
end
model_v2 = simple_demo_v2() | (x = 1.5, y = 2.0)
#+end_src

But you will also encounter the other two approaches in the wild

** Other actions on a =Model=

#+REVEAL: split

*Computing probabilities*

#+HTML: <div class="fragment (appear)">
#+begin_src julia 
logprior(model, (s = 1, m = 1))
#+end_src

#+RESULTS:
: -2.221713955868453
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
#+begin_src julia
loglikelihood(model, (s = 1, m = 1))
#+end_src

#+RESULTS:
: -2.4628770664093453
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
#+begin_src julia
logjoint(model, (s = 1, m = 1))
#+end_src

#+RESULTS:
: -4.6845910222777984
#+HTML: </div>

#+REVEAL: split

*Conditioning and fixing*

#+HTML: <div class="fragment (appear)">
#+begin_src julia :results scalar
# Condition a variable to be a value
model_with_condition = Turing.condition(model, s=1.0)  # equivalent to `|` operator
model_with_condition()
#+end_src

#+RESULTS:
: (s = 1.0, m = 0.31819749773509765, x = 1.5, y = 2.0, hello = 42)
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
#+begin_src julia :results scalar
# Fix a variable to a value
model_with_fixed = Turing.fix(model, s=1.0)
model_with_fixed()
#+end_src

#+RESULTS:
: (s = 1.0, m = -1.662009914303139, x = 1.5, y = 2.0, hello = 42)
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
Difference between conditioning and fixing

#+begin_src julia :results scalar
logjoint(model_with_condition, (m=1,)), logjoint(model_with_fixed, (m=1,))
#+end_src

#+RESULTS:
: (-4.6845910222777984, -3.881815599614018)

A =fixed= variable is _not_ included in the log-probability
#+HTML: </div>

#+REVEAL: split

And can query the model about these things

#+begin_src julia :results scalar
DynamicPPL.observations(model_with_condition)
#+end_src

#+RESULTS:
: (s = 1.0,)

#+begin_src julia :results scalar
DynamicPPL.fixed(model_with_fixed)
#+end_src

#+RESULTS:
: (s = 1.0,)

[[https://turinglang.org/library/DynamicPPL/stable/api/][And much more...]]

** Back to our working example: S(E)IR model

#+REVEAL: split

We'll use the following model
\begin{equation*}
\begin{split}
  \beta &\sim \mathcal{N}_{ + }(2, 1) \\
  \gamma &\sim \mathcal{N}_{ + }(0.4, 0.5) \\
  \phi^{-1} &\sim \mathrm{Exponential}(1/5) \\
   y_i &\sim \mathrm{NegativeBinomial2}\big(F(u_0, t_i;\ \beta, \gamma), \phi \big)
\end{split}
\end{equation*}
where 
- $\big( y_i \big)_{i = 1}^{14}$ are the observations, 
- $F$ is the integrated system, and
- $\phi$ is the over-dispersion parameter.

#+REVEAL: split

#+begin_src julia
plot(
    plot(truncated(Normal(2, 1); lower=0), label=nothing, title="Œ≤"),
    plot(truncated(Normal(0.4, 0.5); lower=0), label=nothing, title="Œ≥"),
    plot(Exponential(1/5), label=nothing, title="œï‚Åª¬π"),
    layout=(3, 1)
)
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/80c76ad5d3151ec0f3e044a075b2a5b88956d14b.svg]]

#+REVEAL: split

=NegativeBinomial(r, p)= represents the number of trials to achieve $r$ successes, where each trial has a probability $p$ of success

=NegativeBinomial2(Œº, œï)= is parameterized by mean $Œº$ and /dispersion/ $\phi$

#+begin_src julia
# `NegativeBinomial` already exists, so let's just make an alternative constructor instead.
function NegativeBinomial2(Œº, œï)
    p = 1/(1 + Œº/œï)
    r = œï
    return NegativeBinomial(r, p)
end
#+end_src

#+RESULTS:
: NegativeBinomial2 (generic function with 1 method)

#+begin_src julia
# Let's just make sure we didn't do something stupid.
Œº = 2; œï = 3;
dist = NegativeBinomial2(Œº, œï)
# Source: https://mc-stan.org/docs/2_20/functions-reference/nbalt.html
mean(dist) ‚âà Œº && var(dist) ‚âà Œº + Œº^2 / œï
#+end_src

#+RESULTS:
: true

#+REVEAL: split

And here's the full model

#+begin_src julia
@model function sir_model(
    num_days;                                  # Number of days to model
    tspan = (0.0, float(num_days)),            # Timespan to model
    u0 = [N - 1, 1, 0.0],                      # Initial state
    p0 = [2.0, 0.6],                           # Placeholder parameters
    problem = ODEProblem(SIR!, u0, tspan, p0)  # Create problem once so we can `remake`.
)
    Œ≤ ~ truncated(Normal(2, 1); lower=0)
    Œ≥ ~ truncated(Normal(0.4, 0.5); lower=0)
    œï‚Åª¬π ~ Exponential(1/5)
    œï = inv(œï‚Åª¬π)

    problem_new = remake(problem, p=[Œ≤, Œ≥])  # Replace parameters `p`.
    sol = solve(problem_new, saveat=1)       # Solve!

    sol_for_observed = sol[2, 2:num_days + 1]  # Timesteps we have observations for.
    in_bed = Vector{Int}(undef, num_days)
    for i = 1:length(sol_for_observed)
        # Add a small constant to `sol_for_observed` to make things more stable.
        in_bed[i] ~ NegativeBinomial2(sol_for_observed[i] + 1e-5, œï)
    end

    # Some quantities we might be interested in.
    return (R0 = Œ≤ / Œ≥, recovery_time = 1 / Œ≥, infected = sol_for_observed)
end
#+end_src

#+RESULTS:
: sir_model (generic function with 2 methods)

It's break-down time

#+REVEAL: split

#+begin_src julia :eval no :tangle no
function sir_model(
    num_days;                                  # Number of days to model
    tspan = (0.0, float(num_days)),            # Timespan to model
    u0 = [N - 1, 1, 0.0],                      # Initial state
    p0 = [2.0, 0.6],                           # Placeholder parameters
    problem = ODEProblem(SIR!, u0, tspan, p0)  # Create problem once so we can `remake`.
)
    ...
end
#+end_src

#+REVEAL: split

#+begin_src julia :eval no :tangle no
Œ≤ ~ truncated(Normal(2, 1); lower=0)
Œ≥ ~ truncated(Normal(0.4, 0.5); lower=0)
œï‚Åª¬π ~ Exponential(1/5)
œï = inv(œï‚Åª¬π)
#+end_src

defines our prior

=truncated= is just a way of restricting the domain of the distribution you pass it

#+REVEAL: split

#+begin_src julia :eval no :tangle no
problem_new = remake(problem, p=[Œ≤, Œ≥])  # Replace parameters `p`.
sol = solve(problem_new, saveat=1)       # Solve!
#+end_src

We then remake the problem, now with the parameters =[Œ≤, Œ≥]= sampled above

=saveat = 1= gets us the solution at the timesteps =[0, 1, 2, ..., 14]=

#+REVEAL: split

Then we extract the timesteps we have observations for

#+begin_src julia :eval no :tangle no
sol_for_observed = sol[2, 2:num_days + 1]  # Timesteps we have observations for.
#+end_src

and define what's going to be a likelihood (once we add observations)

#+begin_src julia :eval no :tangle no
in_bed = Vector{Int}(undef, num_days)
for i = 1:length(sol_for_observed)
    # Add a small constant to `sol_for_observed` to make things more stable.
    in_bed[i] ~ NegativeBinomial2(sol_for_observed[i] + 1e-5, œï)
end
#+end_src

#+REVEAL: split

Finally we return some values that might be of interest

#+begin_src julia :eval no :tangle no
# Some quantities we might be interested in.
return (R0 = Œ≤ / Œ≥, recovery_time = 1 / Œ≥, infected = sol_for_observed)
#+end_src

This is useful for a post-sampling diagnostics, debugging, etc.

#+REVEAL: split

#+begin_src julia
model = sir_model(length(data.in_bed))
#+end_src

#+RESULTS:
: Model(
:   args = (:num_days,)
:   defaults = (:tspan, :u0, :p0, :problem)
:   context = DynamicPPL.DefaultContext()
: )

The model is just another function, so we can call it to check that it works

#+HTML: <div class="fragment (appear)">

#+begin_src julia
model().infected
#+end_src

#+RESULTS:
#+begin_example
14-element Vector{Float64}:
   7.245479773214603
  46.940917072935044
 175.06842724268554
 218.33349924642326
 135.9699709916868
  67.18345574595645
  30.7778031212463
  13.708529913965242
   6.037403608626295
   2.646112937992608
   1.1574372086041416
   0.5058188715354062
   0.22097215001835135
   0.09654724443857109
#+end_example

Hey, it does!

#+HTML: </div>

** Is the prior reasonable?
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Is-the-prior-reasonable
:CUSTOM_ID: 2023-01-29-16-57-28-Is-the-prior-reasonable
:END:

Before we do any inference, we should check if the prior is reasonable

From domain knowledge we know that (for influenza at least)
#+ATTR_REVEAL: :frag (appear)
- $R_0$ is typically between 1 and 2
- =recovery_time= ($1 / \gamma$) is usually ~1 week

#+HTML: <div class="fragment (appear)">

We want to make sure that your prior belief reflects this knowledge while still being flexible enough to accommodate the observations

#+HTML: </div>

#+REVEAL: split

To check this we'll just simulate some draws from our prior model, i.e. the model /without/ conditioning on =in_bed=

There are two ways to sample form the prior

#+HTML: <div class="fragment (appear)">

#+begin_src julia
# 1. By just calling the `model`, which returns a `NamedTuple` containing the quantities of interest
print(model())
#+end_src

#+RESULTS:
: (R0 = 3.421935966345707, recovery_time = 1.0862660411734395, infected = [9.113559456088096, 70.63033985892632, 239.93016676827358, 232.25038629413262, 130.1989796907194, 62.916178149981135, 29.018913772921493, 13.152593290124099, 5.918461197786946, 2.6551129148637975, 1.1895164910507834, 0.532615267682943, 0.238423148128917, 0.10672170534563777])

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

Or by just calling =sample= using =Prior=

#+begin_src julia :async yes
# Sample from prior.
chain_prior = sample(model, Prior(), 10_000);
#+end_src

#+RESULTS:
: [32mSampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Time: 0:00:00[39m
:

#+HTML: </div>

#+REVEAL: split

Let's have a look at the prior predictive

#+begin_src julia :async yes
p = plot(legend=false, size=(600, 300))
plot_trajectories!(p, group(chain_prior, :in_bed); n = 1000)
hline!([N], color="red")
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/f110b08ba2b09629fd9d3c498a45a8ab21e0915a.svg]]

#+ATTR_REVEAL: :frag (appear)
For certain values we get number of infected /larger/ than the actual population

#+ATTR_REVEAL: :frag (appear)
But this is includes the randomness from =NegativeBinomial2= likelihood

#+ATTR_REVEAL: :frag (appear)
Maybe more useful to inspect the (I)nfected state from the ODE solution?

#+REVEAL: split

We can also look at the =generated_quantities=, i.e. the values from the =return= statement in our model

#+HTML: <div class="fragment (appear)">
Our =return= looked like this

#+begin_src julia :eval no :tangle no
# Some quantities we might be interested in.
return (R0 = Œ≤ / Œ≥, recovery_time = 1 / Œ≥, infected = sol_for_observed)
#+end_src

#+HTML: </div>

#+HTML: <div class="fragment (appear)">
and so =generated_quantities= (conditioned on =chain_prior=) gives us

#+begin_src julia
quantities_prior = generated_quantities(model, chain_prior)
print(quantities_prior[1])
#+end_src

#+RESULTS:
: (R0 = 2.204935947161777, recovery_time = 1.876174703727979, infected = [1.8956442506599653, 3.580784916960343, 6.719169194301077, 12.45349236430635, 22.56926177862657, 39.32277667182724, 64.20929976637757, 95.1804136876868, 124.39989367504738, 141.63232495725526, 141.78069289388353, 127.90160667286129, 106.85701946772551, 84.60953676844156])

#+HTML: </div>

#+REVEAL: split

We can convert it into a =Chains= using a utility function of mine

#+begin_src julia
# Convert to `Chains`.
chain_quantities_prior = to_chains(quantities_prior);

# Plot.
p = plot(legend=false, size=(600, 300))
plot_trajectories!(p, group(chain_quantities_prior, :infected); n = 1000)
hline!([N], color="red")
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/a1b75cd5d378febc7029c1ed63db2831e09ab9ee.svg]]

#+HTML: <div class="x-small-text">

*NOTE:* =to_chains= is not part of "official" Turing.jl because the =return= can contain /whatever/ you want, and so it's not always possible to convert into a =Chains=

#+HTML: </div>

#+REVEAL: split

And the quantiles for the trajectories

#+begin_src julia
p = plot(legend=false, size=(600, 300))
plot_trajectory_quantiles!(p, group(chain_quantities_prior, :infected))
hline!(p, [N], color="red")
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/3f7598c6f0df5f376d9f89161504465787bacf25.svg]]

#+REVEAL: split


#+begin_src julia :display text/plain
DataFrame(quantile(chain_quantities_prior[:, [:R0, :recovery_time], :]))
#+end_src

#+RESULTS:
: [1m2√ó6 DataFrame[0m
: [1m Row [0m‚îÇ[1m parameters    [0m[1m 2.5%     [0m[1m 25.0%   [0m[1m 50.0%   [0m[1m 75.0%   [0m[1m 97.5%   [0m
:      ‚îÇ[90m Symbol        [0m[90m Float64  [0m[90m Float64 [0m[90m Float64 [0m[90m Float64 [0m[90m Float64 [0m
: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
:    1 ‚îÇ R0             0.534892  2.09785  3.71417  7.36482  59.3506
:    2 ‚îÇ recovery_time  0.709748  1.20969  1.88343  3.54966  27.4522

Compare to our prior knowledge of $R_0 \in [1, 2]$ and $(1/\gamma) \approx 1$ for influenza

Do we really need probability mass on $R_0 \ge 10$?

** TASK Can we improve the current prior?
:PROPERTIES:
:ID:       2023-01-29-16-57-28-What-s-wrong-with-the-current-prior
:CUSTOM_ID: 2023-01-29-16-57-28-What-s-wrong-with-the-current-prior
:END:

#+HTML: <div class="side-by-side">

#+HTML: <div style="margin: auto;">

The SIR model

\begin{equation*}
\begin{split}
  \frac{\mathrm{d} S}{\mathrm{d} t} &= - \beta S \frac{I}{N} \\
  \frac{\mathrm{d} I}{\mathrm{d} t} &= \beta S \frac{I}{N} - \gamma I \\
  \frac{\mathrm{d} R}{\mathrm{d} t} &= \gamma I
\end{split}
\end{equation*}

#+HTML: </div>

#+HTML: <div>

And here's the current priors

#+HTML: <div class="x-small-text">

#+begin_src julia 
plot(
    plot(truncated(Normal(2, 1); lower=0), label=nothing, title="Œ≤"),
    plot(truncated(Normal(0.4, 0.5); lower=0), label=nothing, title="Œ≥"),
    plot(Exponential(1/5), label=nothing, title="œï‚Åª¬π"),
    layout=(3, 1)
)
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/80c76ad5d3151ec0f3e044a075b2a5b88956d14b.svg]]

#+HTML: </div>

#+HTML: </div>

#+HTML: </div>

** SOLUTION Recovery time shouldn't be several years
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Recovery-time-shouldn-t-be-several-years
:CUSTOM_ID: 2023-01-29-16-57-28-Recovery-time-shouldn-t-be-several-years
:END:

We mentioned that =recovery_time=, which is expressed as $1 / \gamma$, is ~1 week

We're clearly putting high probability on regions near 0, i.e. /long/ recovery times

#+begin_src julia
plot(truncated(Normal(0.4, 0.5); lower=0), label=nothing, title="Œ≥", size=(500, 300))
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/ed3058dc552f72dbe5151feb938d103b75251f63.svg]]

_Should probably be putting less probability mass near 0_

** SOLUTION What if ${\color{red} \beta} > N$?
:PROPERTIES:
:ID:       2023-01-29-16-57-28-What-if-color-red-beta-N
:CUSTOM_ID: 2023-01-29-16-57-28-What-if-color-red-beta-N
:END:
Then for $t = 0$ we have
\begin{equation*}
\frac{\mathrm{d} S}{\mathrm{d} t} \bigg|_{t = 0} = - {\color{red} \beta} S \frac{I}{N} > - N (N - 1) \frac{1}{N} = - (N - 1)
\end{equation*}

i.e. we /immediately/ infect everyone on the very first time-step

Also doesn't seem very realistic

#+REVEAL: split

/But/ under our current prior does this matter?

#+begin_src julia
# ‚Ñô(Œ≤ > N) = 1 - ‚Ñô(Œ≤ ‚â§ N)
1 - cdf(truncated(Normal(2, 1); lower=0), N)
#+end_src

#+RESULTS:
: 0.0

Better yet

#+begin_src julia
quantile(truncated(Normal(2, 1); lower=0), 0.95)
#+end_src

#+RESULTS:
: 3.6559843567138275

i.e. 95% of the probability mass falls below ~3.65

‚üπ _Current prior for $\beta$ seems fine (‚úì)_

#+REVEAL: split

Before we change the prior, let's also make it a bit easier to change the prior using =@submodel=

#+HTML: <div class="fragment (appear)">

=@submodel= allows you call models within models, e.g.

#+begin_src julia
@model function ModelA()
    x_hidden_from_B ~ Normal()
    x = x_hidden_from_B + 100
    return x
end

@model function ModelB()
    @submodel x = ModelA()
    y ~ Normal(x, 1)

    return (; x, y)
end
#+end_src

#+RESULTS:
: ModelB (generic function with 2 methods)

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

#+begin_src julia
# So if we call `B` we only see `x` and `y`
println(ModelB()())
#+end_src

#+RESULTS:
: (x = 101.64055653138922, y = 100.7647417887675)

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

#+begin_src julia
# While if we sample from `B` we get the latent variables
println(rand(ModelB()))
#+end_src

#+RESULTS:
: (x_hidden_from_B = -0.1589429806607626, y = 98.49097210754098)

#+HTML: </div>

#+REVEAL: split

To avoid clashes of variable-names, we can specify a =prefix=

#+begin_src julia
@model ModelA() = (x ~ Normal(); return x + 100)

@model function ModelB()
    # Given it a prefix to use for the variables in `A`.
    @submodel prefix=:inner x_inner = ModelA()
    x ~ Normal(x_inner, 1)

    return (; x_inner, x)
end
#+end_src

#+RESULTS:
: ModelB (generic function with 2 methods)

#+begin_src julia
print(rand(ModelB()))
#+end_src

#+RESULTS:
: (var"inner.x" = 1.152744983322086, x = 101.51855299222316)

#+REVEAL: split

=@submodel= is useful as it allows you to:
1. Easy to swap out certain parts of your model.
2. Can re-use models across projects and packages.

When working on larger projects, this really shines

#+REVEAL: split

Equipped with =@submodel= we can replace

#+begin_src julia :eval no :tangle no
Œ≤ ~ truncated(Normal(2, 1); lower=0)
Œ≥ ~ truncated(Normal(0.4, 0.5); lower=0)
#+end_src

with

#+begin_src julia :eval no :tangle no
@submodel p = prior(problem_wrapper)
#+end_src

#+HTML: <div class="fragment (appear)">

where =prior= can be something like

#+begin_src julia
@model function prior_original(problem_wrapper::SIRProblem)
    Œ≤ ~ truncated(Normal(2, 1); lower=0)
    Œ≥ ~ truncated(Normal(0.4, 0.5); lower=0)

    return [Œ≤, Œ≥]
end

@model function prior_improved(problem_wrapper::SIRProblem)
    # NOTE: Should probably also lower mean for `Œ≤` since
    # more probability mass on small `Œ≥` ‚üπ `R0 =  Œ≤ / Œ≥` grows.
    Œ≤ ~ truncated(Normal(1, 1); lower=0)
    # NOTE: New prior for `Œ≥`.
    Œ≥ ~ Beta(2, 5)

    return [Œ≤, Œ≥]
end
#+end_src

#+RESULTS:
: prior_improved (generic function with 2 methods)

#+HTML: </div>

#+REVEAL: split

#+begin_src julia
@model function epidemic_model(
    problem_wrapper::AbstractEpidemicProblem,
    prior  # NOTE: now we just pass the prior as an argument
)
    # NOTE: And use `@submodel` to embed the `prior` in our model.
    @submodel p = prior(problem_wrapper)

    œï‚Åª¬π ~ Exponential(1/5)
    œï = inv(œï‚Åª¬π)

    problem_new = remake(problem_wrapper.problem, p=p)  # Replace parameters `p`.
    sol = solve(problem_new, saveat=1)                  # Solve!

    # Extract the `infected`.
    sol_for_observed = infected(problem_wrapper, sol)[2:end]

    # NOTE: `product_distribution` is faster for larger dimensional problems,
    # and it does not require explicit allocation of the vector.
    in_bed ~ product_distribution(NegativeBinomial2.(sol_for_observed .+ 1e-5, œï))

    Œ≤, Œ≥ = p[1:2]
    return (R0 = Œ≤ / Œ≥, recovery_time = 1 / Œ≥, infected = sol_for_observed)
end
#+end_src

#+RESULTS:
: epidemic_model (generic function with 2 methods)

#+REVEAL: split

#+HTML: <div class="x-small-text">

Another neat trick is to return early if integration fail

#+HTML: </div>

#+begin_src julia
@model function epidemic_model(
    problem_wrapper::AbstractEpidemicProblem,
    prior  # now we just pass the prior as an argument
)
    # And use `@submodel` to embed the `prior` in our model.
    @submodel p = prior(problem_wrapper)

    œï‚Åª¬π ~ Exponential(1/5)
    œï = inv(œï‚Åª¬π)

    problem_new = remake(problem_wrapper.problem, p=p)  # Replace parameters `p`.
    sol = solve(problem_new, saveat=1)                  # Solve!

    # NOTE: Return early if integration failed.
    if !Part2.issuccess(sol)
        Turing.@addlogprob! -Inf  # NOTE: Causes automatic rejection.
        return nothing
    end

    # Extract the `infected`.
    sol_for_observed = infected(problem_wrapper, sol)[2:end]

    # `product_distribution` is faster for larger dimensional problems,
    # and it does not require explicit allocation of the vector.
    in_bed ~ product_distribution(NegativeBinomial2.(sol_for_observed .+ 1e-5, œï))

    Œ≤, Œ≥ = p[1:2]
    return (R0 = Œ≤ / Œ≥, recovery_time = 1 / Œ≥, infected = sol_for_observed)
end
#+end_src

#+RESULTS:
: epidemic_model (generic function with 2 methods)

#+REVEAL: split

Equipped with this we can now easily construct /two/ models using different priors

#+begin_src julia
sir = SIRProblem(N);
model_original = epidemic_model(sir, prior_original);
model_improved = epidemic_model(sir, prior_improved);
#+end_src

#+RESULTS:

but using the same underlying =epidemic_model=

#+begin_src julia
chain_prior_original = sample(model_original, Prior(), 10_000; progress=false);
chain_prior_improved = sample(model_improved, Prior(), 10_000; progress=false);
#+end_src

#+RESULTS:

Let's compare the resulting priors over some of the quantities of interest

#+REVEAL: split

Let's compare the =generated_quantities=, e.g. $R_0$

#+HTML: <div class="small-text">

#+begin_src julia
chain_quantities_original = to_chains(
    generated_quantities(
        model_original,
        chain_prior_original
    );
);

chain_quantities_improved = to_chains(
    generated_quantities(
        model_improved,
        chain_prior_improved
    );
);
#+end_src

#+RESULTS:

#+begin_src julia
p = plot(; legend=false, size=(500, 200))
plot_trajectories!(p, group(chain_quantities_original, :infected); n = 100, trajectory_color="red")
plot_trajectories!(p, group(chain_quantities_improved, :infected); n = 100, trajectory_color="blue")
hline!([N], color="red", linestyle=:dash)
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/2e1579a71c4072534cabe41a233cd283543874e6.svg]]

#+HTML: </div>

#+REVEAL: split

#+HTML: <div class="small-text">

#+begin_src julia
plt1 = plot(legend=false)
plot_trajectory_quantiles!(plt1, group(chain_quantities_original, :infected))
hline!(plt1, [N], color="red", linestyle=:dash)

plt2 = plot(legend=false)
plot_trajectory_quantiles!(plt2, group(chain_quantities_improved, :infected))
hline!(plt2, [N], color="red", linestyle=:dash)

plot(plt1, plt2, layout=(2, 1))
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/cffea7a80541e6f61fd182de85cb18bc8520b0b4.svg]]

#+HTML: </div>

This makes sense: if half of the population is immediately infected ‚üπ number of infected tapers wrt. time as they recover

#+REVEAL: split

For =model_improved= we then have

#+begin_src julia :display text/plain
DataFrame(quantile(chain_quantities_improved[:, [:R0, :recovery_time], :]))
#+end_src

#+RESULTS:
: [1m2√ó6 DataFrame[0m
: [1m Row [0m‚îÇ[1m parameters    [0m[1m 2.5%     [0m[1m 25.0%   [0m[1m 50.0%   [0m[1m 75.0%   [0m[1m 97.5%   [0m
:      ‚îÇ[90m Symbol        [0m[90m Float64  [0m[90m Float64 [0m[90m Float64 [0m[90m Float64 [0m[90m Float64 [0m
: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
:    1 ‚îÇ R0             0.311812  2.28184  4.43501  8.35226  34.1911
:    2 ‚îÇ recovery_time  1.56854   2.54757  3.75472  6.21961  22.6241

Compare to =model_original=

#+begin_src julia :display text/plain
DataFrame(quantile(chain_quantities_original[:, [:R0, :recovery_time], :]))
#+end_src

#+RESULTS:
: [1m2√ó6 DataFrame[0m
: [1m Row [0m‚îÇ[1m parameters    [0m[1m 2.5%     [0m[1m 25.0%   [0m[1m 50.0%   [0m[1m 75.0%   [0m[1m 97.5%   [0m
:      ‚îÇ[90m Symbol        [0m[90m Float64  [0m[90m Float64 [0m[90m Float64 [0m[90m Float64 [0m[90m Float64 [0m
: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
:    1 ‚îÇ R0             0.528003  2.10801  3.77232  7.34902  73.5875
:    2 ‚îÇ recovery_time  0.707391  1.22814  1.89126  3.54464  33.5939

** TASK Make =epidemic_model= work for =SEIRProblem=
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Make-epidemic-model-work-for-SEIRProblem
:CUSTOM_ID: 2023-01-29-16-57-28-Make-epidemic-model-work-for-SEIRProblem
:END:
1. [ ] Implement a prior which also includes $\sigma$ and execute
   =epidemic_model= with it
2. [ ] Can we make a better prior for $\sigma$? Do we even need one?

#+begin_src julia :eval no
@model function prior_original(problem_wrapper::SEIRProblem)
    # TODO: Implement
end
#+end_src

#+begin_src julia :exports (by-backend (reveal "none") (t "code"))
# Some space so you don't cheat.



















# Are you sure?
#+end_src

#+RESULTS:


** SOLUTION
:PROPERTIES:
:ID:       2023-01-29-16-57-28-
:CUSTOM_ID: 2023-01-29-16-57-28-
:END:

#+begin_src julia
@model function prior_original(problem_wrapper::SEIRProblem)
    Œ≤ ~ truncated(Normal(2, 1); lower=0)
    Œ≥ ~ truncated(Normal(0.4, 0.5); lower=0)
    œÉ ~ truncated(Normal(0.8, 0.5); lower=0)

    return [Œ≤, Œ≥, œÉ]
end
#+end_src

#+RESULTS:
: prior_original (generic function with 4 methods)

#+begin_src julia
model_seir = epidemic_model(SEIRProblem(N), prior_original)
print(model_seir())
#+end_src

#+RESULTS:
: (R0 = 1.1047986187921068, recovery_time = 0.9906676968248809, infected = [0.5602452665531149, 0.5136476859462638, 0.5266355538208665, 0.5493052805718489, 0.5741884446928026, 0.6001760717397501, 0.6271154531635528, 0.6549950994966428, 0.683772907797617, 0.7135447417415389, 0.744111701180274, 0.775785089897001, 0.8083022220361633, 0.8416712774662599])

** WARNING Consult with domain experts
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Consult-with-domain-experts
:CUSTOM_ID: 2023-01-29-16-57-28-Consult-with-domain-experts
:END:
This guy should _not_ be the one setting your priors!

#+ATTR_HTML: :height 400px
#+ATTR_ORG: :width 600
[[file:assets/attachments/2023-01-18_14-49-24_471337_3317365246956_1262712540_o.jpg]]

Get an actual scientist to do that...

** Condition
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Condition
:CUSTOM_ID: 2023-01-29-16-57-28-Condition
:END:
Now let's actually involve the data

#+HTML: <div class="fragment (appear)">

We can condition a =Model= as so

#+begin_src julia
# Condition on the observations.
model = epidemic_model(SIRProblem(N), prior_improved)
model_conditioned = model | (in_bed = data.in_bed,)
#+end_src

#+RESULTS:
: Model(
:   args = (:problem_wrapper, :prior)
:   defaults = ()
:   context = ConditionContext((in_bed = [3, 8, 26, 76, 225, 298, 258, 233, 189, 128, 68, 29, 14, 4],), DynamicPPL.DefaultContext())
: )

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

You know what time it is: /inference time/!

#+HTML: </div>


** Metropolis-Hastings (MH)
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Metropolis-Hastings--MH
:CUSTOM_ID: 2023-01-29-16-57-28-Metropolis-Hastings--MH
:END:

#+begin_src julia
chain_mh = sample(model_conditioned, MH(), MCMCThreads(), 10_000, 4; discard_initial=5_000);
#+end_src

#+RESULTS:

Rhat is /okay-ish/ but not great, and ESS is pretty low innit?

#+REVEAL: split

#+begin_src julia
plot(chain_mh; size=(800, 500))
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/7763ba1e7b6bb223fde4d453cab14cbcf723b75a.svg]]

Eeehh doesn't look the greatest

#+REVEAL: split

Difficult to trust these results, but let's check if it at least did /something/ useful

#+begin_src julia
# We're using the unconditioned model!
predictions_mh = predict(model, chain_mh)
#+end_src

#+RESULTS:
#+begin_example
Chains MCMC chain (10000√ó14√ó4 Array{Float64, 3}):

Iterations        = 1:1:10000
Number of chains  = 4
Samples per chain = 10000
parameters        = in_bed[1], in_bed[2], in_bed[3], in_bed[4], in_bed[5], in_bed[6], in_bed[7], in_bed[8], in_bed[9], in_bed[10], in_bed[11], in_bed[12], in_bed[13], in_bed[14]
internals         = 

Summary Statistics
 [1m parameters [0m [1m     mean [0m [1m     std [0m [1m    mcse [0m [1m   ess_bulk [0m [1m   ess_tail [0m [1m    rha[0m ‚ãØ
 [90m     Symbol [0m [90m  Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m    Float64 [0m [90m    Float64 [0m [90m Float6[0m ‚ãØ

   in_bed[1]     3.3496    2.2548    0.0143   28844.0197   24663.7952    1.000 ‚ãØ
   in_bed[2]    11.0049    5.6044    0.1124    2485.6291    4471.3269    1.001 ‚ãØ
   in_bed[3]    34.8531   16.6902    0.5174     993.2220    2136.9653    1.004 ‚ãØ
   in_bed[4]    95.1006   43.6147    1.5454     739.0583    1949.4236    1.005 ‚ãØ
   in_bed[5]   190.1128   82.0691    2.5249     951.7581    2492.6706    1.003 ‚ãØ
   in_bed[6]   250.7661   99.5104    1.9748    2134.3829    2956.1255    1.001 ‚ãØ
   in_bed[7]   237.7858   92.0707    1.1639    5027.4189    3758.1239    1.002 ‚ãØ
   in_bed[8]   186.2204   73.3891    1.1480    3683.3975    2930.1316    1.003 ‚ãØ
   in_bed[9]   131.9364   52.5514    1.1589    1980.6418    2853.4127    1.003 ‚ãØ
  in_bed[10]    89.4755   36.8751    0.9178    1551.2900    2489.5252    1.004 ‚ãØ
  in_bed[11]    59.3121   25.7064    0.7421    1164.8430    2155.1517    1.005 ‚ãØ
  in_bed[12]    38.8393   17.4595    0.5330    1041.9480    2363.9809    1.007 ‚ãØ
  in_bed[13]    25.2249   12.1044    0.4070     897.4136    1677.4979    1.009 ‚ãØ
  in_bed[14]    16.2789    8.3131    0.2775     896.9420    1922.5588    1.009 ‚ãØ
[36m                                                               2 columns omitted[0m

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m    25.0% [0m [1m    50.0% [0m [1m    75.0% [0m [1m    97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m

   in_bed[1]    0.0000     2.0000     3.0000     5.0000     9.0000
   in_bed[2]    2.0000     7.0000    10.0000    14.0000    24.0000
   in_bed[3]   11.0000    24.0000    32.0000    43.0000    73.0000
   in_bed[4]   31.0000    66.0000    88.0000   116.0000   199.0000
   in_bed[5]   66.0000   135.0000   179.0000   230.0000   386.0000
   in_bed[6]   92.0000   183.0000   240.0000   303.0000   480.0000
   in_bed[7]   88.0000   176.0000   228.0000   287.0000   448.0250
   in_bed[8]   69.0000   137.0000   178.0000   225.0000   354.0000
   in_bed[9]   47.0000    96.0000   126.0000   160.0000   254.0000
  in_bed[10]   31.0000    65.0000    85.0000   109.0000   176.0000
  in_bed[11]   19.0000    42.0000    56.0000    73.0000   119.0000
  in_bed[12]   12.0000    27.0000    37.0000    48.0000    80.0000
  in_bed[13]    7.0000    17.0000    24.0000    31.0000    54.0000
  in_bed[14]    4.0000    10.0000    15.0000    21.0000    36.0000
#+end_example

#+REVEAL: split

#+begin_src julia
plot_trajectories!(plot(legend=false, size=(600, 300)), predictions_mh; data=data)
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/53d5bcefafe0e9c29e4ee0f560bc2648d8b43118.svg]]

#+begin_src julia
plot_trajectory_quantiles!(plot(legend=false, size=(600, 300)), predictions_mh; data=data)
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/0d84524cc352ba3471bf66156dda0a8cfd1e66b9.svg]]

Okay, it's not /completely/ useless, but my trust-issues are still present.

Metropolis-Hastings have disappointed me one too many times before.

** So instead, let's go =NUTS=
:PROPERTIES:
:ID:       2023-01-29-16-57-28-So-instead-let-s-go-NUTS
:CUSTOM_ID: 2023-01-29-16-57-28-So-instead-let-s-go-NUTS
:END:
That's right, we're reaching for the *No U-Turn sampler (NUTS)*

*** 
:PROPERTIES:
:reveal_background_iframe: file:///home/tor/Projects/public/mcmc-demo/app.html?closeControls=true&algorithm=HamiltonianMH&target=standard&seed=1&autoplay=true&histBins=100
:ID:       2023-01-29-16-57-28-
:CUSTOM_ID: 2023-01-29-16-57-28-
:END:

#+ATTR_REVEAL: :frag (appear)
[[https://chi-feng.github.io/mcmc-demo/app.html][https://chi-feng.github.io/mcmc-demo/app.html]]

** 
:PROPERTIES:
:ID:       2023-01-29-16-57-28-
:CUSTOM_ID: 2023-01-29-16-57-28-
:END:

#+begin_quote
Wooaah there! =NUTS= requires gradient information!

How are you going to get that through that =solve=?
#+end_quote

Good question, voice in my head

#+ATTR_REVEAL: :frag (appear)
I'm obviously not going to it myself

** Automatic differentiation (AD) in Julia
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Automatic-differentiation--AD--in-Julia
:CUSTOM_ID: 2023-01-29-16-57-28-Automatic-differentiation--AD--in-Julia
:END:
- [[https://github.com/JuliaDiff/ForwardDiff.jl][ForwardDiff.jl]]: forward-mode AD /(default in Turing.jl)/
- [[https://github.com/JuliaDiff/ReverseDiff.jl][ReverseDiff.jl]]: tape-based reverse-mode AD
- [[https://github.com/FluxML/Zygote.jl][Zygote.jl]]: source-to-source reverse-mode AD
- And more...

#+HTML: <div class="fragment (appear)">

Up-and-coming

- [[https://github.com/EnzymeAD/Enzyme.jl][Enzyme.jl]]: Julia bindings for [[https://github.com/EnzymeAD/Enzyme.jl][Enzyme]] which ADs LLVM (low-level)
- [[https://github.com/JuliaDiff/Diffractor.jl][Diffractor.jl]]: experimental mixed-mode AD meant to replace Zygote.jl

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

Of importance
- [[https://github.com/JuliaDiff/ChainRulesCore.jl][ChainRulesCore.jl]]: light-weight package for defining rules, compatible with many of the above

#+HTML: </div>

#+REVEAL: split

*Important*

#+begin_quote
When you write code, you don't have to make a choice which one you want to use!
#+end_quote

All the (stable) ones, will (mostly) work

/But/ how you write code will affect performance characteristics

Takes a bit of know-how + a bit of digging to go properly "vroom!"

** Differentiating through =solve=
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Differentiating-through-solve
:CUSTOM_ID: 2023-01-29-16-57-28-Differentiating-through-solve
:END:
With that being said, differentiating through numerical =solve= is not necessarily trivial to do efficiently

There are numerous ways of approaching this problem

#+ATTR_HTML: :width 400px
#+ATTR_ORG: :width 400
[[file:assets/attachments/2023-01-22_12-30-07_Screenshot_20230122_122936.png]]

[[https://arxiv.org/abs/1812.01892][https://arxiv.org/abs/1812.01892]] is /great/ resource

#+HTML: <div class="fragment (appear)">

But this is why we have [[https://github.com/SciML/SciMLSensitivity.jl][=SciMLSensitivity.jl=]]

[[https://docs.sciml.ai/SciMLSensitivity/stable/manual/differential_equation_sensitivities/#Choosing-a-Sensitivity-Algorithm][SciMLSensitivity.jl docs]] also provides a great overview of different approaches

#+HTML: </div>

#+REVEAL: split

#+begin_src julia
using SciMLSensitivity
#+end_src

#+RESULTS:

It offers

1. /Discrete sensitivity analysis/ or the /"Direct" method/: just use
   =ForwardDiff.Dual= in the =solve=.
2. /Continuous local sensitivity analysis (CSA)/: extends the original
   system such that the =solve= gives you both the solution and the the
   gradient simultaenously.
3. /Adjoint methods/: construct a backwards system whose solution gives
   us the gradient.

Just do =solve(problem, solver, sensealg = ...)=

** Back to being =NUTS=
   :PROPERTIES:
   :CUSTOM_ID: back-to-being-nuts
   :ID:       2023-01-29-16-57-28-Back-to-being-NUTS
   :END:

#+begin_src julia
chain = sample(model_conditioned, NUTS(0.8), MCMCThreads(), 1000, 4);
#+end_src

#+RESULTS:
#+begin_example
[36m[1m‚îå [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m‚îî [22m[39m  œµ = 0.025
[36m[1m‚îå [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m‚îî [22m[39m  œµ = 0.0125
[36m[1m‚îå [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m‚îî [22m[39m  œµ = 0.05
[36m[1m‚îå [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m‚îî [22m[39m  œµ = 0.8
[33m[1m‚îå [22m[39m[33m[1mWarning: [22m[39mInstability detected. Aborting
[33m[1m‚îî [22m[39m[90m@ SciMLBase ~/.julia/packages/SciMLBase/szsYq/src/integrator_interface.jl:606[39m
#+end_example

#+REVEAL: split

#+begin_src julia
chain
#+end_src

#+RESULTS:
#+begin_example
Chains MCMC chain (1000√ó15√ó4 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 4
Samples per chain = 1000
Wall duration     = 27.18 seconds
Compute duration  = 108.0 seconds
parameters        = Œ≤, Œ≥, œï‚Åª¬π
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m  ess_bulk [0m [1m  ess_tail [0m [1m    rhat [0m [1m[0m ‚ãØ
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m   Float64 [0m [90m   Float64 [0m [90m Float64 [0m [90m[0m ‚ãØ

           Œ≤    1.7305    0.0542    0.0010   2794.5077   2357.1357    1.0008   ‚ãØ
           Œ≥    0.5297    0.0439    0.0009   2349.9049   2086.4939    1.0010   ‚ãØ
         œï‚Åª¬π    0.1379    0.0749    0.0016   1948.2556   1937.4531    0.9999   ‚ãØ
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m   25.0% [0m [1m   50.0% [0m [1m   75.0% [0m [1m   97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m

           Œ≤    1.6305    1.6948    1.7282    1.7644    1.8434
           Œ≥    0.4407    0.5026    0.5288    0.5570    0.6182
         œï‚Åª¬π    0.0447    0.0862    0.1212    0.1709    0.3336
#+end_example

Muuuch better! Both ESS and Rhat is looking good

#+REVEAL: split

#+begin_src julia
plot(chain; size=(800, 500))
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/ba6a57881c744560992e6aeb0ff47cfb9aaf9238.svg]]

#+REVEAL: split

#+begin_src julia
# Predict using the results from NUTS.
predictions = predict(model, chain)
#+end_src

#+RESULTS:
#+begin_example
Chains MCMC chain (1000√ó14√ó4 Array{Float64, 3}):

Iterations        = 1:1:1000
Number of chains  = 4
Samples per chain = 1000
parameters        = in_bed[1], in_bed[2], in_bed[3], in_bed[4], in_bed[5], in_bed[6], in_bed[7], in_bed[8], in_bed[9], in_bed[10], in_bed[11], in_bed[12], in_bed[13], in_bed[14]
internals         = 

Summary Statistics
 [1m parameters [0m [1m     mean [0m [1m      std [0m [1m    mcse [0m [1m  ess_bulk [0m [1m  ess_tail [0m [1m    rhat[0m ‚ãØ
 [90m     Symbol [0m [90m  Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m   Float64 [0m [90m   Float64 [0m [90m Float64[0m ‚ãØ

   in_bed[1]     3.3200     2.2872    0.0367   3869.0113   3757.5599    1.0004 ‚ãØ
   in_bed[2]    10.8800     5.4558    0.0884   3760.4499   3840.4952    1.0010 ‚ãØ
   in_bed[3]    34.3910    16.2547    0.2673   3950.1408   3297.1476    0.9999 ‚ãØ
   in_bed[4]    92.7687    44.5892    0.7711   3360.5024   3226.8261    1.0002 ‚ãØ
   in_bed[5]   185.6832    79.4009    1.3986   3298.6495   3442.5850    1.0013 ‚ãØ
   in_bed[6]   247.6912   100.4472    1.6388   3716.0238   3738.6149    1.0002 ‚ãØ
   in_bed[7]   235.3295    91.1449    1.5092   3657.6049   3748.3664    1.0010 ‚ãØ
   in_bed[8]   182.7982    70.6465    1.1247   3939.1717   3967.9279    1.0002 ‚ãØ
   in_bed[9]   131.7473    52.3558    0.8474   3857.4942   3845.1358    1.0004 ‚ãØ
  in_bed[10]    89.3732    36.8920    0.6141   3756.4616   3366.1403    1.0016 ‚ãØ
  in_bed[11]    59.0973    25.0144    0.4067   3855.0060   3928.0521    0.9997 ‚ãØ
  in_bed[12]    38.8780    16.8736    0.2787   3674.5558   3612.8610    1.0000 ‚ãØ
  in_bed[13]    25.1333    11.9396    0.1968   3686.2674   3575.0453    1.0015 ‚ãØ
  in_bed[14]    16.1432     8.0709    0.1342   3629.5783   3442.4297    0.9997 ‚ãØ
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m    25.0% [0m [1m    50.0% [0m [1m    75.0% [0m [1m    97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m

   in_bed[1]    0.0000     2.0000     3.0000     5.0000     9.0000
   in_bed[2]    3.0000     7.0000    10.0000    14.0000    24.0000
   in_bed[3]   10.0000    23.0000    32.0000    43.0000    73.0000
   in_bed[4]   30.9750    63.0000    85.0000   112.0000   201.0000
   in_bed[5]   67.0000   131.0000   174.0000   225.0000   374.0000
   in_bed[6]   90.0000   179.0000   235.0000   299.0000   477.0000
   in_bed[7]   88.0000   174.0000   226.0000   282.0000   441.0000
   in_bed[8]   68.9750   135.0000   175.0000   221.0000   341.0250
   in_bed[9]   52.0000    96.0000   125.0000   159.0000   255.0000
  in_bed[10]   31.0000    65.0000    84.0000   109.0000   175.0000
  in_bed[11]   21.0000    42.0000    56.0000    72.0000   119.0250
  in_bed[12]   13.0000    27.0000    37.0000    48.0000    81.0000
  in_bed[13]    7.0000    17.0000    23.0000    31.0000    52.0000
  in_bed[14]    4.0000    10.0000    15.0000    21.0000    35.0000
#+end_example

#+REVEAL: split

#+begin_src julia
plot_trajectories!(plot(legend=false, size=(600, 300)), predictions; n = 1000, data=data)
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/870b8fafd890396444043fd96c7d303fb7722627.svg]]

#+REVEAL: split

#+begin_src julia
plot_trajectory_quantiles!(plot(legend=false, size=(600, 300)), predictions; data=data)
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/145ba1245ad3196f9bb3001254df5b3cf451e778.svg]]

** Simulation-based calibration (SBC) [[https://arxiv.org/abs/1804.06788][Talts et. al. (2018)]]
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Simulation-based-calibration--SBC--https-arxiv-dot-org-abs-1804-dot-06788-Talts-et-dot-al-dot--2018
:CUSTOM_ID: 2023-01-29-16-57-28-Simulation-based-calibration--SBC--https-arxiv-dot-org-abs-1804-dot-06788-Talts-et-dot-al-dot--2018
:END:
1. Sample from prior $\theta_1, \dots, \theta_n \sim p(\theta)$.
2. Sample datasets $\mathcal{D}_i \sim p(\cdot \mid \theta_i)$ for $i = 1, \dots, n$.
3. Obtain (approximate) $p(\theta \mid \mathcal{D}_i)$ for $i = 1, \dots, n$.

For large enough (n), the "combination" of the posteriors should recover the prior!

"Combination" here usually means computing some statistic and comparing against what it should be

#+ATTR_HTML: :width 800px
#+ATTR_ORG: :width 400
[[file:.more-julia/attachments/2023-01-22_12-09-24_Screenshot_20230122_120848.png]]

#+REVEAL: split

That's very expensive ‚Üí in practice we just do this once or twice

#+begin_src julia :results scalar
# Sample from the conditioned model so we don't get the `in_bed` variables too
using Random  # Just making sure the numbers of somewhat interesting
rng = MersenneTwister(43);
test_values = rand(rng, NamedTuple, model_conditioned)
#+end_src

#+RESULTS:
: (Œ≤ = 1.2254566808077714, Œ≥ = 0.27594266205681933, œï‚Åª¬π = 0.13984179162984164)

Now we condition on those values and run once to generate data

#+begin_src julia
model_test = model | test_values
#+end_src

#+RESULTS:
: Model(
:   args = (:problem_wrapper, :prior)
:   defaults = ()
:   context = ConditionContext((Œ≤ = 1.2254566808077714, Œ≥ = 0.27594266205681933, œï‚Åª¬π = 0.13984179162984164), DynamicPPL.DefaultContext())
: )

#+begin_src julia
in_best_test = rand(rng, model_test).in_bed;
#+end_src

#+RESULTS:

#+REVEAL: split

Next, inference!

#+begin_src julia
model_test_conditioned = model | (in_bed = in_best_test,)
#+end_src

#+RESULTS:
: Model(
:   args = (:problem_wrapper, :prior)
:   defaults = ()
:   context = ConditionContext((in_bed = [1, 9, 11, 45, 159, 136, 270, 123, 463, 376, 231, 148, 99, 162],), DynamicPPL.DefaultContext())
: )

#+begin_src julia
# Let's just do a single chain here.
chain_test = sample(model_test_conditioned, NUTS(0.8), 1000);
#+end_src

#+RESULTS:
: [36m[1m‚îå [22m[39m[36m[1mInfo: [22m[39mFound initial step size
: [36m[1m‚îî [22m[39m  œµ = 0.025
: [32mSampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Time: 0:00:00[39m
:

#+REVEAL: split

Did we recover the parameters?

#+HTML: <div class="small-text">

#+begin_src julia
ps = []
for sym in [:Œ≤, :Œ≥, :œï‚Åª¬π]
    p = density(chain_test[:, [sym], :])
    vline!([test_values[sym]])
    push!(ps, p)
end
plot(ps..., layout=(3, 1), size=(600, 400))
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/be1c12fbb76e6f039cc149b75842d8ee461b8f89.svg]]

#+HTML: </div>

Yay!

** Samplers in Turing.jl
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Samplers-in-Turing-dot-jl
:CUSTOM_ID: 2023-01-29-16-57-28-Samplers-in-Turing-dot-jl
:END:
- Metropolis-Hastings, MALA, emcee ([[https://github.com/TuringLang/AdvancedMH.jl][AdvancedMH.jl]])
- Hamiltonian Monte Carlo, NUTS ([[https://github.com/TuringLang/AdvancedMH.jl][AdvancedHMC.jl]])
- SMC ([[https://github.com/TuringLang/AdvancedPS.jl][AdvancedPS.jl]])
- Elliptical Slice Sampling ([[https://github.com/TuringLang/EllipticalSliceSampling.jl][EllipticalSliceSampling.jl]])
- Nested sampling ([[https://github.com/TuringLang/NestedSamplers.jl][NestedSamplers.jl]])
- (Experimental) Tempered sampling ([[https://github.com/Julia-Tempering/Pigeons.jl][Pigeons.jl]] and [[https://github.com/TuringLang/MCMCTempering.jl][MCMCTempering.jl]])

#+REVEAL: split

You can also combine some of these in Turing.jl

#+HTML: <div class="small-text">

#+begin_src julia
using LinearAlgebra: I

@model function linear_regression(X)
    num_params = size(X, 1)
    Œ≤ ~ MvNormal(ones(num_params))
    œÉ¬≤ ~ InverseGamma(2, 3)
    y ~ MvNormal(vec(Œ≤' * X), œÉ¬≤ * I)
end

# Generate some dummy data.
X = randn(2, 1_000); lin_reg = linear_regression(X); true_vals = rand(lin_reg)

# Condition.
lin_reg_conditioned = lin_reg | (y = true_vals.y,);
#+end_src

#+RESULTS:

We can then do =Gibbs= but sampling $Œ≤$ using =ESS= and $\sigma^2$ using =HMC=

#+begin_src julia
chain_ess_hmc = sample(lin_reg_conditioned, Gibbs(ESS(:Œ≤), HMC(1e-3, 16, :œÉ¬≤)), 1_000);
#+end_src

#+RESULTS:
: [32mSampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Time: 0:00:00[39m
:

#+HTML: </div>

#+REVEAL: split

#+HTML: <div class="small-text">

#+begin_src julia
chain_ess_hmc
#+end_src

#+RESULTS:
#+begin_example
Chains MCMC chain (1000√ó4√ó1 Array{Float64, 3}):

Iterations        = 1:1:1000
Number of chains  = 1
Samples per chain = 1000
Wall duration     = 13.28 seconds
Compute duration  = 13.28 seconds
parameters        = Œ≤[1], Œ≤[2], œÉ¬≤
internals         = lp

Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m ess_bulk [0m [1m ess_tail [0m [1m    rhat [0m [1m e[0m ‚ãØ
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m  [0m ‚ãØ

        Œ≤[1]    0.2368    0.0377    0.0021   303.6277   312.7163    1.0020     ‚ãØ
        Œ≤[2]   -0.4866    0.0367    0.0017   368.4395   359.5562    0.9992     ‚ãØ
          œÉ¬≤    0.9832    0.0579    0.0168    12.1798    77.1990    1.1376     ‚ãØ
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m   25.0% [0m [1m   50.0% [0m [1m   75.0% [0m [1m   97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m

        Œ≤[1]    0.1636    0.2152    0.2359    0.2575    0.2987
        Œ≤[2]   -0.5474   -0.5090   -0.4879   -0.4627   -0.4245
          œÉ¬≤    0.8816    0.9409    0.9804    1.0215    1.0941
#+end_example

Could potentially lead to improvements

*NOTE:* Usually /very/ difficult to choose sampler parameters in this case

#+HTML: </div>

#+REVEAL: split

Means one can also mix discrete and continuous

#+HTML: <div class="small-text">

#+begin_src julia 
@model function mixture(n)
    cluster ~ filldist(Categorical([0.25, 0.75]), n)
    Œº ~ MvNormal([-10.0, 10.0], I)
    x ~ product_distribution(Normal.(Œº[cluster], 1))
end

model_mixture = mixture(10)
fake_values_mixture = rand(model_mixture)
model_mixture_conditioned = model_mixture | (x = fake_values_mixture.x, )
chain_discrete = sample(
    model_mixture_conditioned, Gibbs(PG(10, :cluster), HMC(1e-3, 16, :Œº)), MCMCThreads(), 1_000, 4
)
#+end_src

#+RESULTS:
#+begin_example
Chains MCMC chain (1000√ó13√ó4 Array{Float64, 3}):

Iterations        = 1:1:1000
Number of chains  = 4
Samples per chain = 1000
Wall duration     = 36.46 seconds
Compute duration  = 144.91 seconds
parameters        = cluster[1], cluster[2], cluster[3], cluster[4], cluster[5], cluster[6], cluster[7], cluster[8], cluster[9], cluster[10], Œº[1], Œº[2]
internals         = lp

Summary Statistics
 [1m  parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m  ess_bulk [0m [1m ess_tail [0m [1m    rhat [0m [1m[0m ‚ãØ
 [90m      Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m   Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m[0m ‚ãØ

   cluster[1]    2.0000    0.0000       NaN         NaN        NaN       NaN   ‚ãØ
   cluster[2]    1.0163    0.1265    0.0137     85.5708    85.5708    1.0281   ‚ãØ
   cluster[3]    1.0285    0.1664    0.0222     56.1096    56.1096    1.0695   ‚ãØ
   cluster[4]    1.0085    0.0918    0.0076    147.7564   147.7564    1.0148   ‚ãØ
   cluster[5]    2.0000    0.0000       NaN         NaN        NaN       NaN   ‚ãØ
   cluster[6]    2.0000    0.0000       NaN         NaN        NaN       NaN   ‚ãØ
   cluster[7]    1.9990    0.0316    0.0007   2002.5842        NaN    1.0007   ‚ãØ
   cluster[8]    2.0000    0.0000       NaN         NaN        NaN       NaN   ‚ãØ
   cluster[9]    2.0000    0.0000       NaN         NaN        NaN       NaN   ‚ãØ
  cluster[10]    1.9937    0.0788    0.0056    197.6402        NaN    1.0250   ‚ãØ
         Œº[1]   -9.3065    0.3103    0.1036     10.0162    38.6601    2.0981   ‚ãØ
         Œº[2]    9.7775    0.8181    0.2839      8.5007    14.8574    3.7783   ‚ãØ
[36m                                                                1 column omitted[0m

Quantiles
 [1m  parameters [0m [1m    2.5% [0m [1m   25.0% [0m [1m   50.0% [0m [1m   75.0% [0m [1m   97.5% [0m
 [90m      Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m

   cluster[1]    2.0000    2.0000    2.0000    2.0000    2.0000
   cluster[2]    1.0000    1.0000    1.0000    1.0000    1.0000
   cluster[3]    1.0000    1.0000    1.0000    1.0000    2.0000
   cluster[4]    1.0000    1.0000    1.0000    1.0000    1.0000
   cluster[5]    2.0000    2.0000    2.0000    2.0000    2.0000
   cluster[6]    2.0000    2.0000    2.0000    2.0000    2.0000
   cluster[7]    2.0000    2.0000    2.0000    2.0000    2.0000
   cluster[8]    2.0000    2.0000    2.0000    2.0000    2.0000
   cluster[9]    2.0000    2.0000    2.0000    2.0000    2.0000
  cluster[10]    2.0000    2.0000    2.0000    2.0000    2.0000
         Œº[1]   -9.9258   -9.5774   -9.2221   -9.0471   -8.8829
         Œº[2]    7.9865    9.2653    9.7118   10.4310   11.0571
#+end_example

#+HTML: </div>

#+REVEAL: split

#+HTML: <div class="x-small-text">

#+begin_src julia 
ps = []
for (i, realizations) in enumerate(eachcol(Array(group(chain_discrete, :cluster))))
    p = density(
        realizations,
        legend=false,
        ticks=false,
        border=:none
    )
    vline!(p, [fake_values_mixture.cluster[i]])
    push!(ps, p)
end
plot(ps..., layout=(length(ps) √∑ 2, 2), size=(600, 40 * length(ps)))
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/d49b052ee893e7410f4c67e9ca34a5e5faee8850.svg]]

#+HTML: </div>

Again, this is difficult to get to work properly on non-trivial examples

_But_ it is possible

** Other utilities for Turing.jl
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Other-utilities-for-Turing-dot-jl
:CUSTOM_ID: 2023-01-29-16-57-28-Other-utilities-for-Turing-dot-jl
:END:
- [[https://github.com/TuringLang/TuringGLM.jl][TuringGLM.jl]]: GLMs using the formula-syntax from R but using Turing.jl under the hood
- [[https://github.com/TuringLang/TuringBenchmarking.jl][TuringBenchmarking.jl]]: useful for benchmarking Turing.jl models
- [[https://github.com/TuringLang/TuringCallbacks.jl][TuringCallbacks.jl]]: on-the-fly visualizations using =tensorboard=

*** TuringGLM.jl

#+begin_src julia 
using TuringGLM
#+end_src

#+REVEAL: split

We'll use the KidIQ dataset for a quick example

#+begin_src julia
register(DataDep(
    "kidiq",
    "Survey of adult American women and their respective children from 2007",
    "https://raw.githubusercontent.com/TuringLang/TuringGLM.jl/bbc9129fc2d1ff7a1026fe2189b6580303d5c9f5/data/kidiq.csv",
))
#+end_src

#+RESULTS:
: DataDep("kidiq", "https://raw.githubusercontent.com/TuringLang/TuringGLM.jl/bbc9129fc2d1ff7a1026fe2189b6580303d5c9f5/data/kidiq.csv", nothing, DataDeps.fetch_default, identity, "Survey of adult American women and their respective children from 2007")

#+begin_src julia :display text/plain
fname = joinpath(datadep"kidiq", "kidiq.csv")
kidiq = DataFrame(CSV.File(fname))
#+end_src

#+RESULTS:
#+begin_example
[1m434√ó4 DataFrame[0m
[1m Row [0m‚îÇ[1m kid_score [0m[1m mom_hs [0m[1m mom_iq   [0m[1m mom_age [0m
     ‚îÇ[90m Int64     [0m[90m Int64  [0m[90m Float64  [0m[90m Int64   [0m
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   1 ‚îÇ        65       1  121.118        27
   2 ‚îÇ        98       1   89.3619       25
   3 ‚îÇ        85       1  115.443        27
   4 ‚îÇ        83       1   99.4496       25
   5 ‚îÇ       115       1   92.7457       27
   6 ‚îÇ        98       0  107.902        18
   7 ‚îÇ        69       1  138.893        20
   8 ‚îÇ       106       1  125.145        23
   9 ‚îÇ       102       1   81.6195       24
  10 ‚îÇ        95       1   95.0731       19
  11 ‚îÇ        91       1   88.577        23
  ‚ãÆ  ‚îÇ     ‚ãÆ        ‚ãÆ        ‚ãÆ         ‚ãÆ
 425 ‚îÇ        42       1   78.2446       27
 426 ‚îÇ       102       1  127.676        29
 427 ‚îÇ       104       1  124.515        23
 428 ‚îÇ        59       0   80.464        21
 429 ‚îÇ        93       0   74.8607       25
 430 ‚îÇ        94       0   84.8774       21
 431 ‚îÇ        76       1   92.9904       23
 432 ‚îÇ        50       0   94.8597       24
 433 ‚îÇ        88       1   96.8566       21
 434 ‚îÇ        70       1   91.2533       25
[36m                            413 rows omitted[0m
#+end_example

#+REVEAL: split

Now we can create the formula

#+begin_src julia :display text/plain
fm = @formula(kid_score ~ mom_hs * mom_iq)
#+end_src

#+RESULTS:
: FormulaTerm
: Response:
:   kid_score(unknown)
: Predictors:
:   mom_hs(unknown)
:   mom_iq(unknown)
:   mom_hs(unknown) & mom_iq(unknown)

which can then easily be converted into a Turing.jl-model

#+begin_src julia :display text/plain
model = turing_model(fm, kidiq);
#+end_src

#+RESULTS:

And then we can use our standard Turing.jl workflow:

#+begin_src julia :display text/plain :async yes
chns = sample(model, NUTS(), 1000)
#+end_src

#+RESULTS:
:RESULTS:
: [36m[1m‚îå [22m[39m[36m[1mInfo: [22m[39mFound initial step size
: [36m[1m‚îî [22m[39m  œµ = 0.0015625
: [32mSampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Time: 0:00:02[39m
: 
#+begin_example
Chains MCMC chain (1000√ó17√ó1 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 1
Samples per chain = 1000
Wall duration     = 17.79 seconds
Compute duration  = 17.79 seconds
parameters        = Œ±, Œ≤[1], Œ≤[2], Œ≤[3], œÉ
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m ess_bulk [0m [1m ess_tail [0m [1m    rhat [0m [1m e[0m ‚ãØ
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m  [0m ‚ãØ

           Œ±   31.3384    6.4780    0.3341   373.7576   549.8589    0.9991     ‚ãØ
        Œ≤[1]    0.5098    2.3878    0.1392   506.3722   195.9589    1.0006     ‚ãØ
        Œ≤[2]    0.5107    0.0714    0.0037   374.7500   463.9428    0.9991     ‚ãØ
        Œ≤[3]    0.0498    0.0333    0.0018   415.5495   215.4668    1.0050     ‚ãØ
           œÉ   18.2205    0.6039    0.0229   705.3081   654.7311    1.0041     ‚ãØ
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m   25.0% [0m [1m   50.0% [0m [1m   75.0% [0m [1m   97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m

           Œ±   19.1674   27.0275   30.8933   36.0974   43.9800
        Œ≤[1]   -2.8269   -0.5578    0.2158    1.0697    7.1574
        Œ≤[2]    0.3667    0.4572    0.5153    0.5602    0.6489
        Œ≤[3]   -0.0235    0.0319    0.0522    0.0718    0.1083
           œÉ   17.1135   17.8058   18.1929   18.6014   19.4824
#+end_example
:END:



*** TuringCallbacks.jl

#+begin_src julia 
using TuringCallbacks
#+end_src

#+RESULTS:

#+begin_src julia 
model = simple_demo(1.5, 2.0);
#+end_src

#+RESULTS:

#+begin_src julia :async yes
logdir = mktempdir()
#+end_src

#+RESULTS:
: "/tmp/jl_uVazSd"

#+begin_src julia :async yes
callback = TensorBoardCallback(joinpath(logdir, "logs"); include_hyperparams=true)
chain = sample(model, NUTS(0.8), 1000; callback=callback);
#+end_src

#+RESULTS:
: [36m[1m‚îå [22m[39m[36m[1mInfo: [22m[39mFound initial step size
: [36m[1m‚îî [22m[39m  œµ = 0.40625

#+REVEAL: split

If you have =tensorboard= installed, you can then run

#+begin_src sh :eval no
python3 -m tensorboard.main --logdir /tmp/jl_uVazSd
#+end_src

#+DOWNLOADED: file:///home/tor/Downloads/tensorboard_demo_histograms_screen.png @ 2023-01-25 20:50:11
#+ATTR_HTML: :width 800px
#+ATTR_ORG: :width 600
[[file:assets/attachments/2023-01-25_20-50-11_tensorboard_demo_histograms_screen.png]]

#+REVEAL: split

Can inspect hyperparameters, e.g. target acceptance rate for =NUTS=

#+DOWNLOADED: file:///tmp/Spectacle.EtZoaa/Screenshot_20230919_213557.png @ 2023-09-19 21:36:15
#+attr_org: :width 600px
[[file:.more-julia/attachments/2023-09-19_21-36-15_Screenshot_20230919_213557.png]]


** Downsides of using Turing.jl
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Downsides-of-using-Turing-dot-jl
:CUSTOM_ID: 2023-01-29-16-57-28-Downsides-of-using-Turing-dot-jl
:END:

#+ATTR_REVEAL: :frag (appear)
- ${\color{red} \times}$ No depedency-extraction of the model
  - ‚üπ can't do things like automatic marginalization
  - /But/ it's not impossible; just a matter of development effort
  - ${\color{green} \checkmark}$ And we have JuliaBUGS now!
- ${\color{red} \times}$ NUTS performance is at the mercy of AD in Julia
- ${\color{green} \checkmark}$ You _can_ put anything in a model
- ${\color{red} \times}$ Whether you _should_ put anything in a model is a another matter


* Setting up a project environment

Now we're going to do some live-coding!

** What we're doing

1. Create project.
   - What does that mean?
2. Add dependencies.
   - Project.toml and Manifest.toml
   - Dependencies we need
3. Get our S(?)IR implementation into the project.
4. Run some experiments.
   - Use Pluto.jl notebook to get started
   - Move into a script if we have time

* Case study

It's time to do a case study!

But on which dataset?

*You're own!*

#+REVEAL: split

But if you don't have one, here are some alternatives:
1. Lotka-Volterra model for snowhoe hares and Canadian lynxes
   - A classic example of predator-prey dynamics
2. Cockroaches in residential buildings throughout New York
   - Become your landlord's favorite tenant by minimizing cockroach complaints in residential buildings while keeping costs low
3. Synthetic time-series model
   - A syncthetic time-series with periodic behavior
4. S(?)IR modeling of influenza
   - You already have the data; go knock yourself out!
5. Pick one from RDatasets.jl

Go to the next slides for more details

** 1. Lotka-Volterra Predator-Prey model

#+HTML: <div class="small-text">

#+HTML: <div class="side-by-side">

#+HTML: <div>
The species of interest in this case study are:
- Snowshoe hares, an hervivorous cousin of rabbits
- Canadian lynxes, a feline predator whose diet consists largely of snowshoe hares
#+HTML: </div>

#+HTML: <div>
Use Lotka-Volterra equations to model the population dynamics
\begin{equation*}
\begin{aligned}
\frac{dH}{dt} &= \alpha H - \beta H L \\
\frac{dL}{dt} &= \delta H L - \gamma L
\end{aligned}
\end{equation*}

#+HTML: </div>

#+HTML: </div>

Use Turing.jl to infer the parameters
- $\alpha$: growth rate of the prey population
- $\beta$: rate of shrinkage of the prey population
- $\delta$: rate of growth of the predator population
- $\gamma$: rate of shrinkage of the predator population

[[https://mc-stan.org/users/documentation/case-studies/lotka-volterra-predator-prey.html][Source (but don't look!)]]

#+RESULTS:

#+HTML: </div>

#+REVEAL: split

#+HTML: <div class="small-text">

#+begin_src julia :exports none
using DataFrames, CSV, DataDeps
#+end_src

#+begin_src julia :display text/plain :exports code
register(DataDep(
    "hares-and-lynxes",
    "Numerical data for the number of pelts collected by the Hudson‚Äôs Bay Company in the years 1900-1920.",
    "https://raw.githubusercontent.com/stan-dev/example-models/master/knitr/lotka-volterra/hudson-bay-lynx-hare.csv",
))
#+end_src

#+RESULTS:
: DataDep("hares-and-lynxes", "https://raw.githubusercontent.com/stan-dev/example-models/master/knitr/lotka-volterra/hudson-bay-lynx-hare.csv", nothing, DataDeps.fetch_default, identity, "Numerical data for the number of pelts collected by the Hudson‚Äôs Bay Company in the years 1900-1920.")


And then we can load it

#+begin_src julia :display text/plain
df = DataFrame(
    CSV.File(
        joinpath(datadep"hares-and-lynxes", "hudson-bay-lynx-hare.csv"),
        skipto=4,
        header=3
    )
)
#+end_src

#+RESULTS:
#+begin_example
[1m21√ó3 DataFrame[0m
[1m Row [0m‚îÇ[1m Year  [0m[1m  Lynx   [0m[1m  Hare   [0m
     ‚îÇ[90m Int64 [0m[90m Float64 [0m[90m Float64 [0m
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   1 ‚îÇ  1900      4.0     30.0
   2 ‚îÇ  1901      6.1     47.2
   3 ‚îÇ  1902      9.8     70.2
   4 ‚îÇ  1903     35.2     77.4
   5 ‚îÇ  1904     59.4     36.3
   6 ‚îÇ  1905     41.7     20.6
   7 ‚îÇ  1906     19.0     18.1
   8 ‚îÇ  1907     13.0     21.4
   9 ‚îÇ  1908      8.3     22.0
  10 ‚îÇ  1909      9.1     25.4
  11 ‚îÇ  1910      7.4     27.1
  12 ‚îÇ  1911      8.0     40.3
  13 ‚îÇ  1912     12.3     57.0
  14 ‚îÇ  1913     19.5     76.6
  15 ‚îÇ  1914     45.7     52.3
  16 ‚îÇ  1915     51.1     19.5
  17 ‚îÇ  1916     29.7     11.2
  18 ‚îÇ  1917     15.8      7.6
  19 ‚îÇ  1918      9.7     14.6
  20 ‚îÇ  1919     10.1     16.2
  21 ‚îÇ  1920      8.6     24.7
#+end_example

#+HTML: </div>

** 2. Cockroaches in New York

#+HTML: <div class="x-small-text">

#+begin_quote
Imagine that you are a statistician or data scientist working as an independent contractor. One of your clients is a company that owns many residential buildings throughout New York City. The property manager explains that they are concerned about the number of cockroach complaints that they receive from their buildings. Previously the company has offered monthly visits from a pest inspector as a solution to this problem. While this is the default solution of many property managers in NYC, the tenants are rarely home when the inspector visits, and so the manager reasons that this is a relatively expensive solution that is currently not very effective.

One alternative to this problem is to deploy long term bait stations. In this alternative, child and pet safe bait stations are installed throughout the apartment building. Cockroaches obtain quick acting poison from these stations and distribute it throughout the colony. The manufacturer of these bait stations provides some indication of the space-to-bait efficacy, but the manager suspects that this guidance was not calculated with NYC roaches in mind. NYC roaches, the manager rationalizes, have more hustle than traditional roaches; and NYC buildings are built differently than other common residential buildings in the US. This is particularly important as the unit cost for each bait station per year is quite high.
#+end_quote

[[https://github.com/jgabry/stancon2018helsinki_intro][Source #1]] and [[https://github.com/jgabry/stancon2018helsinki_intro][Source #2]]  _(but don't look!)_

#+HTML: </div>

#+REVEAL: split

#+HTML: <div class="x-small-text">

The manager wishes to employ your services to help them to find the optimal number of roach bait stations they should place in each of their buildings in order to minimize the number of cockroach complaints while also keeping expenditure on pest control affordable.

A subset of the company's buildings have been randomly selected for an experiment:
- At the beginning of each month, a pest inspector randomly places a number of bait stations throughout the building, without knowledge of the current cockroach levels in the building
- At the end of the month, the manager records the total number of cockroach complaints in that building.
- The manager would like to determine the optimal number of traps (=traps=) that balances the lost revenue (=R=) such that complaints (=complaints=) generate with the all-in cost of maintaining the traps (=TC=).

Formally, we are interested in finding
\begin{equation*}
\arg \max_{\mathrm{traps} \in \mathbb{N}} \mathbb{E}_{\mathrm{complaints}} \big[ R \big( \mathrm{complaints}(\mathrm{traps}) \big) - \mathrm{TC}(\mathrm{traps}) \big]
\end{equation*}

The property manager would also, if possible, like to learn how these results generalize to buildings they haven't treated so they can understand the potential costs of pest control at buildings they are acquiring as well as for the rest of their building portfolio.

As the property manager has complete control over the number of traps set, the random variable contributing to this expectation is the number of complaints given the number of traps. We will model the number of complaints as a function of the number of traps.

#+HTML: </div>

#+REVEAL: split

#+begin_src julia :display text/plain
DataFrame(CSV.File(joinpath("data", "pest_data.csv")))
#+end_src

#+RESULTS:
#+begin_example
[1m120√ó14 DataFrame[0m
[1m Row [0m‚îÇ[1m mus       [0m[1m building_id [0m[1m wk_ind [0m[1m date       [0m[1m traps   [0m[1m floors  [0m[1m sq_footag[0m ‚ãØ
     ‚îÇ[90m Float64   [0m[90m Int64       [0m[90m Int64  [0m[90m Date       [0m[90m Float64 [0m[90m Float64 [0m[90m Float64  [0m ‚ãØ
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   1 ‚îÇ 0.369134            37       1  2017-01-15      8.0      8.0            ‚ãØ
   2 ‚îÇ 0.359355            37       2  2017-02-14      8.0      8.0
   3 ‚îÇ 0.281783            37       3  2017-03-16      9.0      8.0
   4 ‚îÇ 0.129254            37       4  2017-04-15     10.0      8.0
   5 ‚îÇ 0.452041            37       5  2017-05-15     11.0      8.0            ‚ãØ
   6 ‚îÇ 0.44213             37       6  2017-06-14     11.0      8.0
   7 ‚îÇ 0.990865            37       7  2017-07-14     10.0      8.0
   8 ‚îÇ 0.785977            37       8  2017-08-13     10.0      8.0
   9 ‚îÇ 0.691797            37       9  2017-09-12      9.0      8.0            ‚ãØ
  10 ‚îÇ 0.480696            37      10  2017-10-12      9.0      8.0
  11 ‚îÇ 0.562431            37      11  2017-11-11      8.0      8.0
  ‚ãÆ  ‚îÇ     ‚ãÆ           ‚ãÆ         ‚ãÆ         ‚ãÆ          ‚ãÆ        ‚ãÆ             ‚ãÆ ‚ã±
 111 ‚îÇ 0.542095            98       3  2017-03-16      7.0     13.0
 112 ‚îÇ 0.866334            98       4  2017-04-15      6.0     13.0            ‚ãØ
 113 ‚îÇ 1.40571             98       5  2017-05-15      6.0     13.0
 114 ‚îÇ 1.65598             98       6  2017-06-14      5.0     13.0
 115 ‚îÇ 2.2483              98       7  2017-07-14      4.0     13.0
 116 ‚îÇ 2.30359             98       8  2017-08-13      3.0     13.0            ‚ãØ
 117 ‚îÇ 2.253               98       9  2017-09-12      2.0     13.0
 118 ‚îÇ 2.0419              98      10  2017-10-12      2.0     13.0
 119 ‚îÇ 1.90705             98      11  2017-11-11      2.0     13.0
 120 ‚îÇ 2.10317             98      12  2017-12-11      1.0     13.0            ‚ãØ
[36m                                                   8 columns and 99 rows omitted[0m
#+end_example

** 3. Synthetic time-series
Or you can have a go at this synthetic time-series example

#+HTML: <div class="side-by-side small-text">

#+HTML: <div>

#+begin_src julia :exports both :display text/plain
DataFrame(CSV.File(
    joinpath("data", "time-series.csv")
))
#+end_src

#+RESULTS:
#+begin_example
[1m67√ó2 DataFrame[0m
[1m Row [0m‚îÇ[1m t         [0m[1m y         [0m
     ‚îÇ[90m Float64   [0m[90m Float64   [0m
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   1 ‚îÇ 0.0        -19.3009
   2 ‚îÇ 0.0151515  -18.2195
   3 ‚îÇ 0.030303   -17.931
   4 ‚îÇ 0.0454545  -18.5562
   5 ‚îÇ 0.0606061  -19.2006
   6 ‚îÇ 0.0757576  -18.7376
   7 ‚îÇ 0.0909091  -16.4586
   8 ‚îÇ 0.106061   -15.0723
   9 ‚îÇ 0.121212   -12.6583
  10 ‚îÇ 0.136364   -11.1347
  11 ‚îÇ 0.151515   -10.9626
  ‚ãÆ  ‚îÇ     ‚ãÆ          ‚ãÆ
  58 ‚îÇ 0.863636    -6.70737
  59 ‚îÇ 0.878788    -6.59501
  60 ‚îÇ 0.893939    -7.91087
  61 ‚îÇ 0.909091    -8.78053
  62 ‚îÇ 0.924242    -9.81755
  63 ‚îÇ 0.939394    -9.06206
  64 ‚îÇ 0.954545    -7.48517
  65 ‚îÇ 0.969697    -4.72118
  66 ‚îÇ 0.984848    -1.85908
  67 ‚îÇ 1.0          0.0
[36m             46 rows omitted[0m
#+end_example

#+HTML: </div>

#+HTML: <div class="center">

[[./assets/attachments/synthetic-timeseries-data.png]]

#+HTML: </div>

#+HTML: </div>

*** Data generation                                                :noexport:
:PROPERTIES:
:header-args:julia: :session timeseries :kernel julia-4-threads-1.9 :async yes :eval no :tangle no :exports none
:END:

#+begin_src julia 
]activate --temp
#+end_src

#+RESULTS:
: [32m[1m  Activating[22m[39m new project at `/tmp/jl_SaDhi8`

#+begin_src julia :results silent
]add Turing FillArrays LinearAlgebra StatsPlots Random Statistics DataFrames CSV
#+end_src

#+begin_src julia :eval no :tangle no
using Turing, FillArrays, StatsPlots, LinearAlgebra, Random, Statistics, DataFrames, CSV
#+end_src

#+begin_src julia :eval no :tangle no
Random.seed!(12345)

true_sin_freq = 2
true_sin_amp = 5
true_cos_freq = 7
true_cos_amp = 2.5
tmax = 10
Œ≤_true = 2
Œ±_true = -1
tt = 0:0.05:tmax
f‚ÇÅ(t) = Œ±_true + Œ≤_true * t
f‚ÇÇ(t) = true_sin_amp * sinpi(2 * t * true_sin_freq / tmax)
f‚ÇÉ(t) = true_cos_amp * cospi(2 * t * true_cos_freq / tmax)
f(t) = f‚ÇÅ(t) + f‚ÇÇ(t) + f‚ÇÉ(t)

plot(f, tt; label="f(t)", title="Observed time series", legend=:topleft, linewidth=3)
plot!(
    [f‚ÇÅ, f‚ÇÇ, f‚ÇÉ],
    tt;
    label=["f‚ÇÅ(t)" "f‚ÇÇ(t)" "f‚ÇÉ(t)"],
    style=[:dot :dash :dashdot],
    linewidth=1,
)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/0b26b4ffdbb5a6d6692d6439bbcca36ca293e4a5.svg]]


#+begin_src julia :exports output :eval no :tangle no
œÉ_true = 0.35
t = collect(tt[begin:3:end])
t_min, t_max = extrema(t)
x = (t .- t_min) ./ (t_max - t_min)
yf = f.(t) .+ œÉ_true .* randn(size(t))

p = scatter(x, yf; title="Standardised data", legend=false)
savefig("assets/attachments/synthetic-timeseries-data.png")
p
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/901b0ff0c5838163d8c7b76abe4158c02d07125b.svg]]

#+begin_src julia :display text/plain :eval no :tangle no
CSV.write(joinpath("data", "time-series.csv"), DataFrame(t = x, y = yf))
#+end_src

#+RESULTS:
: "data/time-series.csv"

** 4. Influenza at British boarding school (same as before)
An outbreak of influenza A (H1N1) in 1978 at a British boarding school

- 763 male students -> 512 of which became ill
- Reported that one infected boy started the epidemic
- Observations are number of boys in bed over 14 days

Data are freely available in the R package =outbreaks=, maintained as part of the [[http://www.repidemicsconsortium.org/][R Epidemics Consortium]]

#+REVEAL: split

#+begin_src julia :display text/plain
DataFrame(CSV.File(joinpath("data", "influenza_england_1978_school.csv")))
#+end_src

#+RESULTS:
#+begin_example
[1m14√ó4 DataFrame[0m
[1m Row [0m‚îÇ[1m Column1 [0m[1m date       [0m[1m in_bed [0m[1m convalescent [0m
     ‚îÇ[90m Int64   [0m[90m Date       [0m[90m Int64  [0m[90m Int64        [0m
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   1 ‚îÇ       1  1978-01-22       3             0
   2 ‚îÇ       2  1978-01-23       8             0
   3 ‚îÇ       3  1978-01-24      26             0
   4 ‚îÇ       4  1978-01-25      76             0
   5 ‚îÇ       5  1978-01-26     225             9
   6 ‚îÇ       6  1978-01-27     298            17
   7 ‚îÇ       7  1978-01-28     258           105
   8 ‚îÇ       8  1978-01-29     233           162
   9 ‚îÇ       9  1978-01-30     189           176
  10 ‚îÇ      10  1978-01-31     128           166
  11 ‚îÇ      11  1978-02-01      68           150
  12 ‚îÇ      12  1978-02-02      29            85
  13 ‚îÇ      13  1978-02-03      14            47
  14 ‚îÇ      14  1978-02-04       4            20
#+end_example

** 5. Anything from =RDatasets.jl=

Or you can just do =]add RDatasets= and knock yourself out

https://github.com/JuliaStats/RDatasets.jl

* Julia: The Good, the Bad, and the Ugly
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Julia-The-Good-the-Bad-and-the-Ugly
:CUSTOM_ID: 2023-01-29-16-57-28-Julia-The-Good-the-Bad-and-the-Ugly
:END:

An honest take from a little 27-year old Norwegian boy

*** The Good
:PROPERTIES:
:ID:       2023-01-29-16-57-28-The-Good
:CUSTOM_ID: 2023-01-29-16-57-28-The-Good
:END:
- Speed
- Composability (thank you multiple dispatch)
- No need to tie yourself to an underlying computational framework
- Interactive
- Transparency
- Very easy to call into other languages

*** Speed
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Speed
:CUSTOM_ID: 2023-01-29-16-57-28-Speed
:END:

I think you got this already...

*** Composability
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Composability
:CUSTOM_ID: 2023-01-29-16-57-28-Composability
:END:

We've seen some of that

Defining =infected(problem_wrapper, u)= allowed us to abstract away how to extract the compartment of interest

*** Transparency
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Transparency
:CUSTOM_ID: 2023-01-29-16-57-28-Transparency
:END:

For starters, almost all the code you'll end up using is pure Julia

Hence, you can always look at the code

You can find the implementation by using =@which=

#+begin_src julia 
# Without arguments
@which sum
#+end_src

#+RESULTS:
: Base

#+begin_src julia :display text/plain
# With arguments
@which sum([1.0])
#+end_src

#+RESULTS:
: sum([90ma[39m::[1mAbstractArray[22m; dims, kw...)
: [90m     @[39m [90mBase[39m [90m[4mreducedim.jl:994[24m[39m

#+REVEAL: split

And yeah, you can even look into the macros

#+HTML: <div class="small-text">

#+begin_src julia 
@macroexpand @model f() = x ~ Normal()
#+end_src

#+RESULTS:
#+begin_example
quote
    function f(__model__::DynamicPPL.Model, __varinfo__::DynamicPPL.AbstractVarInfo, __context__::AbstractPPL.AbstractContext; )
        [90m#= In[245]:1 =#[39m
        begin
            var"##dist#7280" = Normal()
            var"##vn#7277" = (DynamicPPL.resolve_varnames)((AbstractPPL.VarName){:x}(), var"##dist#7280")
            var"##isassumption#7278" = begin
                    if (DynamicPPL.contextual_isassumption)(__context__, var"##vn#7277")
                        if !((DynamicPPL.inargnames)(var"##vn#7277", __model__)) || (DynamicPPL.inmissings)(var"##vn#7277", __model__)
                            true
                        else
                            x === missing
                        end
                    else
                        false
                    end
                end
            begin
                [90m#= /home/tor/.julia/packages/DynamicPPL/m0PXI/src/compiler.jl:555 =#[39m
                var"##retval#7282" = if (DynamicPPL.contextual_isfixed)(__context__, var"##vn#7277")
                        x = (DynamicPPL.getfixed_nested)(__context__, var"##vn#7277")
                    elseif var"##isassumption#7278"
                        begin
                            (var"##value#7281", __varinfo__) = (DynamicPPL.tilde_assume!!)(__context__, (DynamicPPL.unwrap_right_vn)((DynamicPPL.check_tilde_rhs)(var"##dist#7280"), var"##vn#7277")..., __varinfo__)
                            x = var"##value#7281"
                            var"##value#7281"
                        end
                    else
                        if !((DynamicPPL.inargnames)(var"##vn#7277", __model__))
                            x = (DynamicPPL.getconditioned_nested)(__context__, var"##vn#7277")
                        end
                        (var"##value#7279", __varinfo__) = (DynamicPPL.tilde_observe!!)(__context__, (DynamicPPL.check_tilde_rhs)(var"##dist#7280"), x, var"##vn#7277", __varinfo__)
                        var"##value#7279"
                    end
                [90m#= /home/tor/.julia/packages/DynamicPPL/m0PXI/src/compiler.jl:556 =#[39m
                return (var"##retval#7282", __varinfo__)
            end
        end
    end
    begin
        $(Expr(:meta, :doc))
        function f(; )
            [90m#= In[245]:1 =#[39m
            return (DynamicPPL.Model)(f, NamedTuple{()}(()); )
        end
    end
end
#+end_example

#+HTML: </div>

#+REVEAL: split

I told you didn't want to see that.

Can make it /a bit/ cleaner by removing linenums:

#+HTML: <div class="x-small-text">

#+begin_src julia 
@macroexpand(@model f() = x ~ Normal()) |> Base.remove_linenums!
#+end_src

#+RESULTS:
#+begin_example
quote
    function f(__model__::DynamicPPL.Model, __varinfo__::DynamicPPL.AbstractVarInfo, __context__::AbstractPPL.AbstractContext; )
        begin
            var"##dist#7286" = Normal()
            var"##vn#7283" = (DynamicPPL.resolve_varnames)((AbstractPPL.VarName){:x}(), var"##dist#7286")
            var"##isassumption#7284" = begin
                    if (DynamicPPL.contextual_isassumption)(__context__, var"##vn#7283")
                        if !((DynamicPPL.inargnames)(var"##vn#7283", __model__)) || (DynamicPPL.inmissings)(var"##vn#7283", __model__)
                            true
                        else
                            x === missing
                        end
                    else
                        false
                    end
                end
            begin
                var"##retval#7288" = if (DynamicPPL.contextual_isfixed)(__context__, var"##vn#7283")
                        x = (DynamicPPL.getfixed_nested)(__context__, var"##vn#7283")
                    elseif var"##isassumption#7284"
                        begin
                            (var"##value#7287", __varinfo__) = (DynamicPPL.tilde_assume!!)(__context__, (DynamicPPL.unwrap_right_vn)((DynamicPPL.check_tilde_rhs)(var"##dist#7286"), var"##vn#7283")..., __varinfo__)
                            x = var"##value#7287"
                            var"##value#7287"
                        end
                    else
                        if !((DynamicPPL.inargnames)(var"##vn#7283", __model__))
                            x = (DynamicPPL.getconditioned_nested)(__context__, var"##vn#7283")
                        end
                        (var"##value#7285", __varinfo__) = (DynamicPPL.tilde_observe!!)(__context__, (DynamicPPL.check_tilde_rhs)(var"##dist#7286"), x, var"##vn#7283", __varinfo__)
                        var"##value#7285"
                    end
                return (var"##retval#7288", __varinfo__)
            end
        end
    end
    begin
        $(Expr(:meta, :doc))
        function f(; )
            return (DynamicPPL.Model)(f, NamedTuple{()}(()); )
        end
    end
end
#+end_example

#+HTML: </div>

#+REVEAL: split

#+begin_src julia
f(x) = 2x
#+end_src

#+RESULTS:
: f (generic function with 1 method)

You can inspect the type-inferred and lowered code

#+begin_src julia
@code_typed f(1)
#+end_src

#+RESULTS:
: CodeInfo(
: [90m1 ‚îÄ[39m %1 = Base.mul_int(2, x)[36m::Int64[39m
: [90m‚îî‚îÄ‚îÄ[39m      return %1
: ) => Int64

#+REVEAL: split

You can inspect the LLVM code

#+begin_src julia
@code_llvm f(1)
#+end_src

#+RESULTS:
: [90m;  @ In[247]:1 within `f`[39m
: [95mdefine[39m [36mi64[39m [93m@julia_f_40807[39m[33m([39m[36mi64[39m [95msignext[39m [0m%0[33m)[39m [0m#0 [33m{[39m
: [91mtop:[39m
: [90m; ‚îå @ int.jl:88 within `*`[39m
:    [0m%1 [0m= [96m[1mshl[22m[39m [36mi64[39m [0m%0[0m, [33m1[39m
: [90m; ‚îî[39m
:   [96m[1mret[22m[39m [36mi64[39m [0m%1
: [33m}[39m

#+REVEAL: split

And even the resulting machine code

#+begin_src julia
@code_native f(1)
#+end_src

#+RESULTS:
#+begin_example
	[0m.text
	[0m.file	[0m"f"
	[0m.globl	[0mjulia_f_40844                   [90m# -- Begin function julia_f_40844[39m
	[0m.p2align	[33m4[39m[0m, [33m0x90[39m
	[0m.type	[0mjulia_f_40844[0m,[0m@function
[91mjulia_f_40844:[39m                          [90m# @julia_f_40844[39m
[90m; ‚îå @ In[247]:1 within `f`[39m
	[0m.cfi_startproc
[90m# %bb.0:                                # %top[39m
	[96m[1mpushq[22m[39m	[0m%rbp
	[0m.cfi_def_cfa_offset [33m16[39m
	[0m.cfi_offset [0m%rbp[0m, [33m-16[39m
	[96m[1mmovq[22m[39m	[0m%rsp[0m, [0m%rbp
	[0m.cfi_def_cfa_register [0m%rbp
[90m; ‚îÇ‚îå @ int.jl:88 within `*`[39m
	[96m[1mleaq[22m[39m	[33m([39m[0m%rdi[0m,[0m%rdi[33m)[39m[0m, [0m%rax
[90m; ‚îÇ‚îî[39m
	[96m[1mpopq[22m[39m	[0m%rbp
	[0m.cfi_def_cfa [0m%rsp[0m, [33m8[39m
	[96m[1mretq[22m[39m
[91m.Lfunc_end0:[39m
	[0m.size	[0mjulia_f_40844[0m, [0m.Lfunc_end0-julia_f_40844
	[0m.cfi_endproc
[90m; ‚îî[39m
                                        [90m# -- End function[39m
	[0m.section	[0m".note.GNU-stack"[0m,[0m""[0m,[0m@progbits
#+end_example

It really just depends on which level of "I hate my life" you're currently at

*** Calling into other languages
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Calling-into-other-languages
:CUSTOM_ID: 2023-01-29-16-57-28-Calling-into-other-languages
:END:
- [[https://docs.julialang.org/en/v1/manual/calling-c-and-fortran-code/][C and Fortran comes built-in stdlib]]
- [[https://juliainterop.github.io/RCall.jl/stable/][RCall.jl]]: call into =R=
- [[https://github.com/JuliaPy/PyCall.jl][PyCall.jl]]: call into =python=
- Etc.

When working with =Array=, etc. memory is usually shared ‚üπ fairly low overhead

*** C and Fortran
:PROPERTIES:
:ID:       2023-01-29-16-57-28-C-and-Fortran
:CUSTOM_ID: 2023-01-29-16-57-28-C-and-Fortran
:END:
#+begin_src julia 
# Define the Julia function
function mycompare(a, b)::Cint
    println("mycompare($a, $b)")  # NOTE: Let's look at the comparisons made.
    return (a < b) ? -1 : ((a > b) ? +1 : 0)
end

# Get the corresponding C function pointer.
mycompare_c = @cfunction(mycompare, Cint, (Ref{Cdouble}, Ref{Cdouble}))

# Array to sort.
arr = [1.3, -2.7, 4.4, 3.1];

# Call in-place quicksort.
ccall(:qsort, Cvoid, (Ptr{Cdouble}, Csize_t, Csize_t, Ptr{Cvoid}),
      arr, length(arr), sizeof(eltype(arr)), mycompare_c)
#+end_src

#+RESULTS:
: mycompare(1.3, -2.7)
: mycompare(4.4, 3.1)
: mycompare(-2.7, 3.1)
: mycompare(1.3, 3.1)

#+begin_src julia 
# All sorted!
arr
#+end_src

#+RESULTS:
: 4-element Vector{Float64}:
:  -2.7
:   1.3
:   3.1
:   4.4

[[https://docs.julialang.org/en/v1/manual/calling-c-and-fortran-code/#Creating-C-Compatible-Julia-Function-Pointers][Example is from Julia docs]]

*** The Bad
:PROPERTIES:
:ID:       2023-01-29-16-57-28-The-Bad
:CUSTOM_ID: 2023-01-29-16-57-28-The-Bad
:END:
Sometimes
- your code might just slow down without a seemingly good reason,
- someone did bad, and Julia can't tell which method to call, or
- someone forces the Julia compiler to compile insane amounts of code

*** "Why is my code suddenly slow?"
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Why-is-my-code-suddenly-slow
:CUSTOM_ID: 2023-01-29-16-57-28-Why-is-my-code-suddenly-slow
:END:

One word: *type-instability*

Sometimes the Julia compiler can't quite infer what types fully

#+HTML: <div class="fragment (appear)">

*Result:* python-like performance (for those particular function calls)

#+begin_src julia 
# NOTE: this is NOT `const`, and so it could become some other type
# at any given point without `my_func` knowing about it!
global_variable = 1
my_func_unstable(x) = global_variable * x
#+end_src

#+RESULTS:
: my_func_unstable (generic function with 1 method)

#+begin_src julia
using BenchmarkTools
@btime my_func_unstable(2.0);
#+end_src

#+RESULTS:
:   19.901 ns (2 allocations: 32 bytes)

#+HTML: </div>

#+REVEAL: split

Luckily there are tools for inspecting this

#+begin_src julia 
@code_warntype my_func_unstable(2.0)
#+end_src

#+RESULTS:
#+begin_example
MethodInstance for my_func_unstable(::Float64)
  from my_func_unstable([90mx[39m)[90m @[39m [90mMain[39m [90m[4mIn[253]:4[24m[39m
Arguments
  #self#[36m::Core.Const(my_func_unstable)[39m
  x[36m::Float64[39m
Body[91m[1m::Any[22m[39m
[90m1 ‚îÄ[39m %1 = (Main.global_variable * x)[91m[1m::Any[22m[39m
[90m‚îî‚îÄ‚îÄ[39m      return %1
#+end_example

See that =Any= there? _'tis a big no-no!_

#+REVEAL: split

Once discovered, it can be fixed

#+begin_src julia 
const constant_global_variable = 1
my_func_fixed(x) = constant_global_variable * x
@code_warntype my_func_fixed(2.0)
#+end_src

#+RESULTS:
: MethodInstance for my_func_fixed(::Float64)
:   from my_func_fixed([90mx[39m)[90m @[39m [90mMain[39m [90m[4mIn[257]:2[24m[39m
: Arguments
:   #self#[36m::Core.Const(my_func_fixed)[39m
:   x[36m::Float64[39m
: Body[36m::Float64[39m
: [90m1 ‚îÄ[39m %1 = (Main.constant_global_variable * x)[36m::Float64[39m
: [90m‚îî‚îÄ‚îÄ[39m      return %1
: 

So long Python performance!

#+begin_src julia 
@btime my_func_fixed(2.0);
#+end_src

#+RESULTS:
:   1.238 ns (0 allocations: 0 bytes)


#+REVEAL: split

/But/ this is not always so easy to discover (though this is generally rare)

#+begin_src julia 
# HACK: Here we explicitly tell Julia what type `my_func_unstable`
# returns. This is _very_ rarely a good idea because it just hides
# the underlying problem from `@code_warntype`!
my_func_forced(x) = my_func_unstable(x)::typeof(x)
@code_warntype my_func_forced(2.0)
#+end_src

#+RESULTS:
#+begin_example
MethodInstance for my_func_forced(::Float64)
  from my_func_forced([90mx[39m)[90m @[39m [90mMain[39m [90m[4mIn[259]:4[24m[39m
Arguments
  #self#[36m::Core.Const(my_func_forced)[39m
  x[36m::Float64[39m
Body[36m::Float64[39m
[90m1 ‚îÄ[39m %1 = Main.my_func_unstable(x)[91m[1m::Any[22m[39m
[90m‚îÇ  [39m %2 = Main.typeof(x)[36m::Core.Const(Float64)[39m
[90m‚îÇ  [39m %3 = Core.typeassert(%1, %2)[36m::Float64[39m
[90m‚îî‚îÄ‚îÄ[39m      return %3
#+end_example

We can still see the =Any= in there, but on a first glance it looks like =my_func_forced= is type-stable

There are more natural cases where this might occur, e.g. unfortunate closures deep in your callstack

#+REVEAL: split

To discovery these there are a couple of more advanced tools:
- [[https://github.com/JuliaDebug/Cthulhu.jl][Cthulhu.jl]]: Allows you to step through your code like a debugger and perform =@code_warntype=
- [[https://github.com/aviatesk/JET.jl][JET.jl]]: Experimental package which attempts to automate the process

And even simpler: profile using [[https://github.com/timholy/ProfileView.jl][ProfileView.jl]] and look for code-paths that /should/ be fast but take up a lot of the runtime

#+REVEAL: split

#+begin_src julia :eval no
using ProfileView
#+end_src

#+begin_src julia :eval no
@profview foreach(_ -> my_func_unstable(2.0), 1_000_000)
#+end_src

#+DOWNLOADED: file:///tmp/Spectacle.wcviMK/Screenshot_20230125_011603.png @ 2023-01-25 01:16:13
#+ATTR_HTML: :height 350px
#+ATTR_ORG: :width 600
[[file:assets/attachments/2023-01-25_01-16-13_Screenshot_20230125_011603.png]]

Note that there's no sign of multiplication here

But most of the runtime is the =./reflection.jl= at the top there

That's Julia looking up the type at runtime

*** Method ambiguity
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Method-ambiguity
:CUSTOM_ID: 2023-01-29-16-57-28-Method-ambiguity
:END:
#+begin_src julia 
ambiguous_function(x, y::Int) = y
ambiguous_function(x::Int, y) = x

# NOTE: Here we have `ambiguous_function(x::Int, y::Int)`
# Which one should we hit?!
ambiguous_function(1, 2)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
MethodError: ambiguous_function(::Int64, ::Int64) is ambiguous.

Candidates:
  ambiguous_function([90mx[39m, [90my[39m::[1mInt64[22m)
[90m    @[39m [90mMain[39m [90m[4mIn[261]:1[24m[39m
  ambiguous_function([90mx[39m::[1mInt64[22m, [90my[39m)
[90m    @[39m [90mMain[39m [90m[4mIn[261]:2[24m[39m

Possible fix, define
  ambiguous_function(::Int64, ::Int64)


Stacktrace:
 [1] top-level scope
   @ In[261]:6
#+end_example
:END:

But here Julia warns us, and so we can fix this by just doing as it says: define =ambiguous_function(::Int64, ::Int64)=

#+begin_src julia 
ambiguous_function(::Int64, ::Int64) = "neato"
ambiguous_function(1, 2)
#+end_src

#+RESULTS:
: "neato"

*** Long compilation times
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Long-compilation-times
:CUSTOM_ID: 2023-01-29-16-57-28-Long-compilation-times
:END:
In Julia, for better or worse, we can generate code

*Problem:* it can be /lots/ of code of we really want to

*Result:* first execution can be /slow/

#+HTML: <div class="fragment (appear)">

*Time to first plot (TTFP)* is Julia's worst enemy

But things are always improving

#+DOWNLOADED: file:///tmp/Spectacle.wcviMK/Screenshot_20230125_012853.png @ 2023-01-25 01:29:05
[[file:assets/attachments/2023-01-25_01-29-05_Screenshot_20230125_012853.png]]

#+HTML: </div>

*** Another example: mis-use of =@generated=
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Another-example-mis-use-of-generated
:CUSTOM_ID: 2023-01-29-16-57-28-Another-example-mis-use-of-generated
:END:

#+begin_src julia 
# NOTE: `@generated` only has access to static information, e.g. types of arguments.
# Here I'm using the special type `Val` to make a number `N` static.
@generated function unrolled_addition(::Val{N}) where {N}
    expr = Expr(:block)
    push!(expr.args, :(x = 0))
    for i = 1:N
        push!(expr.args, :(x += $(3.14 * i)))
    end

    return expr
end
#+end_src

#+RESULTS:
: unrolled_addition (generic function with 1 method)

When I call this with some =Val(N)=, Julia will execute this /at compile-time/!

#+begin_src julia 
# NOTE: At runtime, it then just returns the result immediately
@code_typed unrolled_addition(Val(10))
#+end_src

#+RESULTS:
: CodeInfo(
: [90m1 ‚îÄ[39m     return 172.70000000000002
: ) => Float64

But if I just change the value =10= to =11=, it's a /completely/ different type!

#+REVEAL: split

So Julia has to compile =unrolled_addition= from scratch

#+begin_src julia 
@time @eval unrolled_addition(Val(11));
#+end_src

#+RESULTS:
:   0.008437 seconds (10.35 k allocations: 620.520 KiB)

Or a bit crazier

#+begin_src julia 
@time @eval unrolled_addition(Val(10_001));
#+end_src

#+RESULTS:
:   0.269772 seconds (782.52 k allocations: 37.372 MiB, 9.83% gc time)

Here it took ~0.4s, of which 99.95% was compilation time

I think you get the idea

#+REVEAL: split

But boy is it fast to run!

#+begin_src julia 
@btime unrolled_addition(Val(10_001));
#+end_src

#+RESULTS:
:   1.080 ns (0 allocations: 0 bytes)

#+begin_src julia 
function not_unrolled_addition(N)
    x = 0
    for i = 1:N
        x += 3.14 * i
    end

    return x
end
#+end_src

#+RESULTS:
: not_unrolled_addition (generic function with 1 method)

#+begin_src julia 
@btime not_unrolled_addition(10_001);
#+end_src

#+RESULTS:
:   8.522 Œºs (0 allocations: 0 bytes)

#+REVEAL: split

*Funny side-note:* at first I did the following

#+begin_src julia 
@generated function unrolled_addition_old(::Val{N}) where {N}
    expr = Expr(:block)
    push!(expr.args, :(x = 0))
    for i = 1:N
        push!(expr.args, :(x += $i))  # NOTE: No 3.14!
    end
    return expr
end
function not_unrolled_addition_old(N)
    x = 0
    for i = 1:N
        x += i  # NOTE: No 3.14!
    end
    return x
end
#+end_src

#+RESULTS:
: not_unrolled_addition_old (generic function with 1 method)

#+begin_src julia 
@btime unrolled_addition_old(Val(10_001));
@btime not_unrolled_addition_old(10_001);
#+end_src

#+RESULTS:
:   1.080 ns (0 allocations: 0 bytes)
:   2.227 ns (0 allocations: 0 bytes)

LLVM probably recognized the pattern of =not_unrolled_addition_old= and unrolls it for us

Let's check!

#+REVEAL: split

#+begin_src julia 
# NOTE: The one LLVM failed to unroll
@code_llvm not_unrolled_addition(10_001)
#+end_src

#+RESULTS:
#+begin_example
[90m;  @ In[268]:1 within `not_unrolled_addition`[39m
[95mdefine[39m [33m{[39m [33m{[39m[33m}[39m[0m*[0m, [36mi8[39m [33m}[39m [93m@julia_not_unrolled_addition_41667[39m[33m([39m[33m[[39m[33m8[39m [0mx [36mi8[39m[33m][39m[0m* [95mnoalias[39m [95mnocapture[39m [95mnoundef[39m [95mnonnull[39m [95malign[39m [33m8[39m [95mdereferenceable[39m[33m([39m[33m8[39m[33m)[39m [0m%0[0m, [36mi64[39m [95msignext[39m [0m%1[33m)[39m [0m#0 [33m{[39m
[91mtop:[39m
[90m;  @ In[268]:3 within `not_unrolled_addition`[39m
[90m; ‚îå @ range.jl:5 within `Colon`[39m
[90m; ‚îÇ‚îå @ range.jl:397 within `UnitRange`[39m
[90m; ‚îÇ‚îÇ‚îå @ range.jl:404 within `unitrange_last`[39m
     [0m%.inv [0m= [96m[1micmp[22m[39m [96m[1msgt[22m[39m [36mi64[39m [0m%1[0m, [33m0[39m
     [0m%. [0m= [96m[1mselect[22m[39m [36mi1[39m [0m%.inv[0m, [36mi64[39m [0m%1[0m, [36mi64[39m [33m0[39m
[90m; ‚îî‚îî‚îî[39m
  [96m[1mbr[22m[39m [36mi1[39m [0m%.inv[0m, [36mlabel[39m [91m%L17.preheader[39m[0m, [36mlabel[39m [91m%union_move16[39m

[91mL17.preheader:[39m                                    [90m; preds = %top[39m
[90m;  @ In[268]:5 within `not_unrolled_addition`[39m
[90m; ‚îå @ range.jl:891 within `iterate`[39m
[90m; ‚îÇ‚îå @ promotion.jl:499 within `==`[39m
    [0m%.not30 [0m= [96m[1micmp[22m[39m [96m[1meq[22m[39m [36mi64[39m [0m%.[0m, [33m1[39m
[90m; ‚îî‚îî[39m
  [96m[1mbr[22m[39m [36mi1[39m [0m%.not30[0m, [36mlabel[39m [91m%union_move[39m[0m, [36mlabel[39m [91m%L48[39m

[91mL48:[39m                                              [90m; preds = %L48, %L17.preheader[39m
  [0m%value_phi1032 [0m= [96m[1mphi[22m[39m [36mdouble[39m [33m[[39m [0m%value_phi10[0m, [91m%L48[39m [33m][39m[0m, [33m[[39m [33m3.140000e+00[39m[0m, [91m%L17.preheader[39m [33m][39m
  [0m%value_phi431 [0m= [96m[1mphi[22m[39m [36mi64[39m [33m[[39m [0m%2[0m, [91m%L48[39m [33m][39m[0m, [33m[[39m [33m1[39m[0m, [91m%L17.preheader[39m [33m][39m
[90m; ‚îå @ range.jl:891 within `iterate`[39m
   [0m%2 [0m= [96m[1madd[22m[39m [36mi64[39m [0m%value_phi431[0m, [33m1[39m
[90m; ‚îî[39m
[90m;  @ In[268]:4 within `not_unrolled_addition`[39m
[90m; ‚îå @ promotion.jl:411 within `*`[39m
[90m; ‚îÇ‚îå @ promotion.jl:381 within `promote`[39m
[90m; ‚îÇ‚îÇ‚îå @ promotion.jl:358 within `_promote`[39m
[90m; ‚îÇ‚îÇ‚îÇ‚îå @ number.jl:7 within `convert`[39m
[90m; ‚îÇ‚îÇ‚îÇ‚îÇ‚îå @ float.jl:159 within `Float64`[39m
       [0m%3 [0m= [96m[1msitofp[22m[39m [36mi64[39m [0m%2 [95mto[39m [36mdouble[39m
[90m; ‚îÇ‚îî‚îî‚îî‚îî[39m
[90m; ‚îÇ @ promotion.jl:411 within `*` @ float.jl:410[39m
   [0m%4 [0m= [96m[1mfmul[22m[39m [36mdouble[39m [0m%3[0m, [33m3.140000e+00[39m
[90m; ‚îî[39m
[90m;  @ In[268] within `not_unrolled_addition`[39m
  [0m%value_phi10 [0m= [96m[1mfadd[22m[39m [36mdouble[39m [0m%value_phi1032[0m, [0m%4
[90m;  @ In[268]:5 within `not_unrolled_addition`[39m
[90m; ‚îå @ range.jl:891 within `iterate`[39m
[90m; ‚îÇ‚îå @ promotion.jl:499 within `==`[39m
    [0m%.not [0m= [96m[1micmp[22m[39m [96m[1meq[22m[39m [36mi64[39m [0m%2[0m, [0m%.
[90m; ‚îî‚îî[39m
  [96m[1mbr[22m[39m [36mi1[39m [0m%.not[0m, [36mlabel[39m [91m%L17.union_move_crit_edge[39m[0m, [36mlabel[39m [91m%L48[39m

[91mpost_union_move:[39m                                  [90m; preds = %union_move16, %union_move[39m
  [0m%tindex_phi1429 [0m= [96m[1mphi[22m[39m [36mi8[39m [33m[[39m [33m2[39m[0m, [91m%union_move16[39m [33m][39m[0m, [33m[[39m [33m1[39m[0m, [91m%union_move[39m [33m][39m
[90m;  @ In[268]:7 within `not_unrolled_addition`[39m
  [0m%5 [0m= [96m[1minsertvalue[22m[39m [33m{[39m [33m{[39m[33m}[39m[0m*[0m, [36mi8[39m [33m}[39m [33m{[39m [33m{[39m[33m}[39m[0m* [95mnull[39m[0m, [36mi8[39m [95mundef[39m [33m}[39m[0m, [36mi8[39m [0m%tindex_phi1429[0m, [33m1[39m
  [96m[1mret[22m[39m [33m{[39m [33m{[39m[33m}[39m[0m*[0m, [36mi8[39m [33m}[39m [0m%5

[91mL17.union_move_crit_edge:[39m                         [90m; preds = %L48[39m
[90m;  @ In[268]:5 within `not_unrolled_addition`[39m
  [0m%phi.cast [0m= [96m[1mbitcast[22m[39m [36mdouble[39m [0m%value_phi10 [95mto[39m [36mi64[39m
  [96m[1mbr[22m[39m [36mlabel[39m [91m%union_move[39m

[91munion_move:[39m                                       [90m; preds = %L17.union_move_crit_edge, %L17.preheader[39m
  [0m%value_phi10.lcssa [0m= [96m[1mphi[22m[39m [36mi64[39m [33m[[39m [0m%phi.cast[0m, [91m%L17.union_move_crit_edge[39m [33m][39m[0m, [33m[[39m [33m4614253070214989087[39m[0m, [91m%L17.preheader[39m [33m][39m
[90m;  @ In[268]:7 within `not_unrolled_addition`[39m
  [0m%6 [0m= [96m[1mbitcast[22m[39m [33m[[39m[33m8[39m [0mx [36mi8[39m[33m][39m[0m* [0m%0 [95mto[39m [36mi64[39m[0m*
  [96m[1mstore[22m[39m [36mi64[39m [0m%value_phi10.lcssa[0m, [36mi64[39m[0m* [0m%6[0m, [95malign[39m [33m8[39m
  [96m[1mbr[22m[39m [36mlabel[39m [91m%post_union_move[39m

[91munion_move16:[39m                                     [90m; preds = %top[39m
  [0m%7 [0m= [96m[1mbitcast[22m[39m [33m[[39m[33m8[39m [0mx [36mi8[39m[33m][39m[0m* [0m%0 [95mto[39m [36mi64[39m[0m*
  [96m[1mstore[22m[39m [36mi64[39m [33m0[39m[0m, [36mi64[39m[0m* [0m%7[0m, [95malign[39m [33m8[39m
  [96m[1mbr[22m[39m [36mlabel[39m [91m%post_union_move[39m
[33m}[39m
#+end_example

#+REVEAL: split

#+begin_src julia 
# NOTE: The one LLVM seems to have unrolled.
@code_llvm not_unrolled_addition_old(10_001)
#+end_src

#+RESULTS:
#+begin_example
[90m;  @ In[270]:9 within `not_unrolled_addition_old`[39m
[95mdefine[39m [36mi64[39m [93m@julia_not_unrolled_addition_old_41669[39m[33m([39m[36mi64[39m [95msignext[39m [0m%0[33m)[39m [0m#0 [33m{[39m
[91mtop:[39m
[90m;  @ In[270]:11 within `not_unrolled_addition_old`[39m
[90m; ‚îå @ range.jl:5 within `Colon`[39m
[90m; ‚îÇ‚îå @ range.jl:397 within `UnitRange`[39m
[90m; ‚îÇ‚îÇ‚îå @ range.jl:404 within `unitrange_last`[39m
     [0m%.inv [0m= [96m[1micmp[22m[39m [96m[1msgt[22m[39m [36mi64[39m [0m%0[0m, [33m0[39m
     [0m%. [0m= [96m[1mselect[22m[39m [36mi1[39m [0m%.inv[0m, [36mi64[39m [0m%0[0m, [36mi64[39m [33m0[39m
[90m; ‚îî‚îî‚îî[39m
  [96m[1mbr[22m[39m [36mi1[39m [0m%.inv[0m, [36mlabel[39m [91m%L17.preheader[39m[0m, [36mlabel[39m [91m%L32[39m

[91mL17.preheader:[39m                                    [90m; preds = %top[39m
[90m;  @ In[270]:13 within `not_unrolled_addition_old`[39m
  [0m%1 [0m= [96m[1mshl[22m[39m [95mnuw[39m [36mi64[39m [0m%.[0m, [33m1[39m
  [0m%2 [0m= [96m[1madd[22m[39m [95mnsw[39m [36mi64[39m [0m%.[0m, [33m-1[39m
  [0m%3 [0m= [96m[1mzext[22m[39m [36mi64[39m [0m%2 [95mto[39m [36mi65[39m
  [0m%4 [0m= [96m[1madd[22m[39m [95mnsw[39m [36mi64[39m [0m%.[0m, [33m-2[39m
  [0m%5 [0m= [96m[1mzext[22m[39m [36mi64[39m [0m%4 [95mto[39m [36mi65[39m
  [0m%6 [0m= [96m[1mmul[22m[39m [36mi65[39m [0m%3[0m, [0m%5
  [0m%7 [0m= [96m[1mlshr[22m[39m [36mi65[39m [0m%6[0m, [33m1[39m
  [0m%8 [0m= [96m[1mtrunc[22m[39m [36mi65[39m [0m%7 [95mto[39m [36mi64[39m
  [0m%9 [0m= [96m[1madd[22m[39m [36mi64[39m [0m%1[0m, [0m%8
  [0m%10 [0m= [96m[1madd[22m[39m [36mi64[39m [0m%9[0m, [33m-1[39m
[90m;  @ In[270]:14 within `not_unrolled_addition_old`[39m
  [96m[1mbr[22m[39m [36mlabel[39m [91m%L32[39m

[91mL32:[39m                                              [90m; preds = %L17.preheader, %top[39m
  [0m%value_phi10 [0m= [96m[1mphi[22m[39m [36mi64[39m [33m[[39m [33m0[39m[0m, [91m%top[39m [33m][39m[0m, [33m[[39m [0m%10[0m, [91m%L17.preheader[39m [33m][39m
  [96m[1mret[22m[39m [36mi64[39m [0m%value_phi10
[33m}[39m
#+end_example

*** The Ugly
:PROPERTIES:
:ID:       2023-01-29-16-57-28-The-Ugly
:CUSTOM_ID: 2023-01-29-16-57-28-The-Ugly
:END:

#+REVEAL: split

_*Reverse-mode automatic differentiation*_

ForwardDiff.jl is a pure joy, but slows down as dimensionality grows

Then one should reach for ReverseDiff.jl or Zygote.jl

#+HTML: <div class="fragment (appear)">
Most of the time it works really well, but sometimes you hit a real sharp edge

And sharp edges cut; they cut /deep/

Like _"16X slower when the function is implemented more efficiently"-deep_

#+DOWNLOADED: file:///tmp/Spectacle.wcviMK/Screenshot_20230125_010111.png @ 2023-01-25 01:01:31
[[file:assets/attachments/2023-01-25_01-01-31_Screenshot_20230125_010111.png]]

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

If you want to see a man in pain, you can find the full issue [[https://github.com/TuringLang/Turing.jl/issues/1934][here]]

On the flip-side, once addressed (a type-instability), it's [[https://github.com/TuringLang/DistributionsAD.jl/pull/231][3X faster than before]]

#+HTML: </div>

*** Overall
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Overall
:CUSTOM_ID: 2023-01-29-16-57-28-Overall
:END:

Julia is pretty darn awesome

Easy to get going, and you can always make it faster by just optimizing _Julia_ code

No need to drop down to C++

#+REVEAL: split
Buuuut it can't beat Python at deep learning

#+REVEAL: split
Otherwise, it's worth a try

Godspeed to you

* Fin
