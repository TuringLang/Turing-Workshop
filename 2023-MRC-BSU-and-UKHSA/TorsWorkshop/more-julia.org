#+SETUPFILE: ~/org-blog/setup.org
#+OPTIONS: tex:t toc:nil date:nil
#+PROPERTY: header-args:R :session :exports both :eval no
#+PROPERTY: header-args:julia :session mrc-biostats-2023-more-julia :tangle more-julia.jl :exports both :kernel julia-4-threads-1.9 :async no :file (f-join "assets" "outputs" "more-julia" (sha1 (plist-get (cadr (org-element-at-point)) :value)))
#+EXCLUDE_TAGS: noexport
#+TODO: TODO(t) TASK(q) WARNING(w) | DONE(d) SOLUTION(s)

#+REVEAL_ROOT: assets/reveal.js-4.1.0/
#+REVEAL_MATHJAX_URL: assets/MathJax-2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML
#+REVEAL_TITLE_SLIDE: <div><div style="margin: -200px auto; opacity: 0.2;"><p><object data="assets/images/turing-logo-wide.svg"></object></p></div><h1>Bayesian inference and other things</h1><h2>with the TuringLang ecosystem</h2><p><a href="https://github.com/TuringLang">https://github.com/TuringLang</a></p><p><a href="https://github.com/TuringLang/Turing-Workshop/tree/main/2023-Geilo-Winter-School/Part-2-Turing-and-other-things">The workshop is found here</a></p></div>
#+REVEAL_EXTRA_CSS: assets/css/custom.css
#+REVEAL_THEME: white
#+REVEAL_PLUGINS: (markdown zoom)
#+HTML_HEAD: <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

#+AUTHOR: Tor Erlend Fjelde
#+TITLE: More Julia

* Notes                                                            :noexport:

- [ ] How much do I need to cover of LinearAlgebra?
  - Xianda covers
    - A bit of IO
    - A
- [ ] Problem: estimate pi
- [ ] distriubtions.jl
  - Show a table of distrubiotns
  - Show a table of API
- [ ] turing
  - use product dist
  - debugging
    - broken simple model
      - Explicitly running the model
  - quick slide demonstrating TuringCallbacks.jl + TensorBoard
- [X] Pidgeon and MCMCTempering
- [X] TuringGLM example
- [ ] Starting a project from scratch
  - Maybe use the rats example from https://github.com/kskyten/BayesWorkshop2021/tree/generated/notebooks/Turing
- [ ] Case studies
  - Rats example
    - Simple to get started
    - Can progressively make more and more complicated models
    - Project will have relatively simple models, and so will focus more on analysis of results and comparisons of models.
  - Lotka-Volterra
    - https://mc-stan.org/users/documentation/case-studies/lotka-volterra-predator-prey.html
    - https://turinglang.org/v0.29/tutorials/10-bayesian-differential-equations/
    - Involves ODEs
    - Not many different models
  - ImperialReport13
    - The model itself is quite complex
  - S(?)IR model for influenza dataset
    - We've already gone through this partially
    - Maybe they want to try to implement something else?
- [ ] Implement Adaptive MH from [cite:@journal.pcbi.1011088]
  - Adaptive MH and PMMH methods are more explicitly talked about here: [cite:@kxs052]
- [ ] =EpimapSimple=
  - Show off the submodel stuff.
- [ ] Add some more stuff to TuringCallbacks.jl section
* Before we begin
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Before-we-begin
:CUSTOM_ID: 2023-01-29-16-57-28-Before-we-begin
:END:

Make sure you're in the correct directory

#+begin_src julia
pwd()
#+end_src

#+RESULTS:
: "/drive-2/Projects/public/Turing-Workshop/2023-MRC-BSU-and-UKHSA/TorsWorkshop"

Then run something like (depending on which OS you are on)

#+begin_src sh :eval no
julia --project
#+end_src

or if you're already in a REPL, do

#+begin_src julia :tangle no
]activate .
#+end_src

#+RESULTS:
:   Activating project at `/drive-2/Projects/public/Turing-Workshop/2023-MRC-BSU-and-UKHSA/TorsWorkshop`

to activate the project

#+REVEAL: split

And just to check that you're in the correct one

#+begin_src julia :tangle no
]status
#+end_src

#+RESULTS:
#+begin_example
Project TorsWorkshop v0.1.0
Status `/drive-2/Projects/public/Turing-Workshop/2023-MRC-BSU-and-UKHSA/TorsWorkshop/Project.toml`
  [336ed68f] CSV v0.10.11
  [a93c6f00] DataFrames v1.6.1
  [0c46a032] DifferentialEquations v7.9.1
  [31c24e10] Distributions v0.25.100
  [7073ff75] IJulia v1.24.2
  [7f7a1694] Optimization v3.17.0
  [36348300] OptimizationOptimJL v0.1.9
  [91a5bcdd] Plots v1.39.0
  [f3b207a7] StatsPlots v0.15.6
  [fce5fe82] Turing v0.29.1
  [ade2ca70] Dates
#+end_example

Download and install dependencies

#+begin_src julia :tangle no
]instantiate
#+end_src

#+RESULTS:

#+REVEAL: split

And finally, do

#+begin_src julia 
using TorsWorkshop
#+end_src

#+RESULTS:


to get some functionality I've implemented for the occasion


* Base & Standard library
Julia is mainly a programing language for scientific computing

⟹ Julia comes with tons of useful functionality built-in
** =Base=


[[https://docs.julialang.org/en/v1/base/base/][=Base=]] is the only module which is /always/ imported

It contains the most fundamental functionality of the language, e.g.

#+begin_src julia
@which map
#+end_src

#+RESULTS:
: Base

#+REVEAL: split

Relevant modules you'll find in =Base=

- [[https://docs.julialang.org/en/v1/base/file/][Filesystem]]
- [[https://docs.julialang.org/en/v1/base/io-network/][I/O and Network]]
- [[https://docs.julialang.org/en/v1/base/iterators/][Iterators]]
- [[https://docs.julialang.org/en/v1/base/multi-threading/][Threads]]

*** Filesystem
#+begin_src julia 
Base.Filesystem
#+end_src

#+RESULTS:
: Base.Filesystem

#+begin_src julia
pwd()
#+end_src

#+RESULTS:
: "/drive-2/Projects/public/Turing-Workshop/2023-MRC-BSU-and-UKHSA/TorsWorkshop"

#+begin_src julia
@which pwd
#+end_src

#+RESULTS:
: Base.Filesystem

https://docs.julialang.org/en/v1/base/file/

*** Multi-threading
One example of a 

#+begin_src julia 
Threads
#+end_src

#+RESULTS:
: Base.Threads

#+begin_src julia
Threads.nthreads()
#+end_src

#+RESULTS:
: 4

Or we can call =using Threads= so so we don't have to write =Threads.= all the time

#+begin_src julia 
using Base.Threads
#+end_src

#+RESULTS:

#+begin_src julia
nthreads()
#+end_src

#+RESULTS:
: 4

#+REVEAL: split

Making use of the threads is trivial

#+begin_src julia
Threads.@threads for i in 1:10
    println("Thread $(Threads.threadid()): $i")
end
#+end_src

#+RESULTS:
: Thread 1: 1
: Thread 4: 7
: Thread 3: 9
: Thread 3: 2
: Thread 4: 8
: Thread 2: 4
: Thread 2: 5
: Thread 3: 3
: Thread 2: 10
: Thread 2: 6

https://docs.julialang.org/en/v1/base/multi-threading/

** Standard library
- [[https://docs.julialang.org/en/v1/stdlib/Pkg/][Pkg]]
- [[https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/][LinearAlgebra]]
- [[https://docs.julialang.org/en/v1/stdlib/SparseArrays/][SparseArrays]]
- [[https://docs.julialang.org/en/v1/stdlib/Statistics/][Statistics]]
- [[https://docs.julialang.org/en/v1/stdlib/Random/][Random]]
- [[https://docs.julialang.org/en/v1/stdlib/Distributed/][Distributed]]
- [[https://docs.julialang.org/en/v1/stdlib/Logging/][Logging]]
- [[https://docs.julialang.org/en/v1/stdlib/Dates/][Dates]]
- [[https://docs.julialang.org/en/v1/stdlib/Serialization/][Serialization]]
- [[https://docs.julialang.org/en/v1/stdlib/Downloads/][Downloads]]
- [[https://docs.julialang.org/en/v1/stdlib/Test/][Unit testing]]

*** =Dates=

#+begin_src julia 
using Dates
before = Dates.now()
#+end_src

#+RESULTS:
: 2023-09-15T10:53:09.147

#+begin_src julia
Dates.now() - before
#+end_src

#+RESULTS:
: 354 milliseconds

#+begin_src julia
dump(before)
#+end_src

#+RESULTS:
: DateTime
:   instant: Dates.UTInstant{Millisecond}
:     periods: Millisecond
:       value: Int64 63830458389147

https://docs.julialang.org/en/v1/stdlib/Dates/

*** =Random=

#+begin_src julia 
using Random
#+end_src

#+RESULTS:

We can set the "global" seed

#+begin_src julia 
Random.seed!(1234)
#+end_src

#+RESULTS:
: TaskLocalRNG()

#+begin_src julia
rand()
#+end_src

#+RESULTS:
: 0.32597672886359486

Or provide the RNG explicitly

#+begin_src julia
# Xoshiro is what Julia uses by default
rng = Random.Xoshiro(1234)
rand(rng) # <= same as before
#+end_src

#+RESULTS:
: 0.32597672886359486

Most functions using RNGs follow this pattern of optionally accepting an RNG as the first argument

#+REVEAL: split

To sample multiple values, we just specify how many we want

#+begin_src julia
rand(3)
#+end_src

#+RESULTS:
: 3-element Vector{Float64}:
:  0.5490511363155669
:  0.21858665481883066
:  0.8942454282009883

#+begin_src julia
rand(3, 3)
#+end_src

#+RESULTS:
: 3×3 Matrix{Float64}:
:  0.520355  0.967143  0.951162
:  0.639562  0.205168  0.0739957
:  0.839622  0.527184  0.571586

And we can also specify the type of the output

#+begin_src julia
rand(Float32)
#+end_src

#+RESULTS:
: 0.07271612f0

#+REVEAL: split

And of course other standard sampling functions are available
#+begin_src julia 
randn()
#+end_src

#+RESULTS:
: 1.724189934074888

#+begin_src julia
randexp()
#+end_src

#+RESULTS:
: 0.04221258127478853

#+begin_src julia
# Sample uniformly from a vector
rand([1, 2, 3])
#+end_src

#+RESULTS:
: 3

And more: https://docs.julialang.org/en/v1/stdlib/Random/

*** =LinearAlgebra=

#+begin_src julia 
A = [1 2 3; 4 1 6; 7 8 1]
#+end_src

#+RESULTS:
: 3×3 Matrix{Int64}:
:  1  2  3
:  4  1  6
:  7  8  1

#+begin_src julia
using LinearAlgebra

norm(A)
#+end_src

#+RESULTS:
: 13.45362404707371

#+begin_src julia
@which norm
#+end_src

#+RESULTS:
: LinearAlgebra

Other functions are =det=, =dot=, =cholesky=, and much, much more.

https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/

*** =SparseArrays=

#+begin_src julia 
using SparseArrays
A_sparse = sparse([1, 1, 2, 3], [1, 3, 2, 3], [0, 1, 2, 0])
#+end_src

#+RESULTS:
: 3×3 SparseMatrixCSC{Int64, Int64} with 4 stored entries:
:  0  ⋅  1
:  ⋅  2  ⋅
:  ⋅  ⋅  0

#+begin_src julia
dropzeros(A_sparse)
#+end_src

#+RESULTS:
: 3×3 SparseMatrixCSC{Int64, Int64} with 2 stored entries:
:  ⋅  ⋅  1
:  ⋅  2  ⋅
:  ⋅  ⋅  ⋅

And standard array methods are applicable

#+begin_src julia
# `A` is the dense array from earlier
A * A_sparse
#+end_src

#+RESULTS:
: 3×3 Matrix{Int64}:
:  0   4  1
:  0   2  4
:  0  16  7

https://docs.julialang.org/en/v1/stdlib/SparseArrays/

*** =Statistics=

#+begin_src julia 
using Statistics
#+end_src

#+RESULTS:

#+begin_src julia :results scalar
mean(A), std(A)
#+end_src

#+RESULTS:
: (3.6666666666666665, 2.7386127875258306)

https://docs.julialang.org/en/v1/stdlib/Statistics/

*** =Distributed=

Functionality for parallel computation across workers (either local or remote)

#+begin_src julia 
using Distributed
nprocs()
#+end_src

#+RESULTS:
: 1

#+begin_src julia
# Spawn a local worker
addprocs(1)
#+end_src

#+RESULTS:
: 1-element Vector{Int64}:
:  2

#+begin_src julia
# Spawn a remote worker
addprocs(["tor@beastly"], tunnel=true, dir="/tmp/")
#+end_src

#+RESULTS:
: 1-element Vector{Int64}:
:  3

#+begin_src julia
nprocs()
#+end_src

#+RESULTS:
: 3

#+REVEAL: split

#+begin_src julia
# Define something on all workers
@everywhere function hostname_and_number(i)
    # Execute shell command on worker to get hostname.
    # NOTE: Using `...` syntax for shell commands.
    # This creates a `Cmd`, which is run once we call `read on it.
    hostname = read(`hostname`, String)
    # Return a tuple of worker ID, hostname and the number.
    return (myid(), i, chomp(hostname))
end
#+end_src

#+RESULTS:

#+begin_src julia
# Run the function on all workers
pmap(hostname_and_number, 1:12)
#+end_src

#+RESULTS:
#+begin_example
12-element Vector{Tuple{Int64, Int64, SubString{String}}}:
 (2, 1, "tor-Prestige-15-A10SC")
 (3, 2, "beastly")
 (2, 3, "tor-Prestige-15-A10SC")
 (2, 4, "tor-Prestige-15-A10SC")
 (2, 5, "tor-Prestige-15-A10SC")
 (2, 6, "tor-Prestige-15-A10SC")
 (2, 7, "tor-Prestige-15-A10SC")
 (2, 8, "tor-Prestige-15-A10SC")
 (2, 9, "tor-Prestige-15-A10SC")
 (2, 10, "tor-Prestige-15-A10SC")
 (2, 11, "tor-Prestige-15-A10SC")
 (2, 12, "tor-Prestige-15-A10SC")
#+end_example

https://docs.julialang.org/en/v1/stdlib/Distributed/

*** =Logging=

#+begin_src julia 
A = ones(Int, 4, 4)
v = ones(100)
@info "Some variables"  A  s=sum(v)
#+end_src

#+RESULTS:
: ┌ Info: Some variables
: │   A =
: │    4×4 Matrix{Int64}:
: │     1  1  1  1
: │     1  1  1  1
: │     1  1  1  1
: │     1  1  1  1
: └   s = 100.0

https://docs.julialang.org/en/v1/stdlib/Logging/

*** =Pkg=
** TASK Estimate $\pi$

[[./assets/outputs/more-julia/pi.gif]]

*Extra:* Parallelize it.


#+begin_src julia :exports (by-backend (reveal "none") (t "code"))
# Some space so you don't cheat.



















# Are you sure?
#+end_src


** SOLUTION Estimate $\pi$

Well, Julia has irrational numbers built-in

#+begin_src julia
π
#+end_src

#+RESULTS:
: π = 3.1415926535897...

So you could just do

#+begin_src julia
Float64(π)
#+end_src

#+RESULTS:
: 3.141592653589793

But that's not fair.

#+begin_src julia 
num_within = 0; num_total = 1_000_000

for i in 1:num_total
    x = 2 .* rand(2) .- 1
    if norm(x) < 1
        num_within += 1
    end
end

# Area of a circle = πr^2 = π * (1/2)^2 = π/4
4 * num_within / num_total
#+end_src

#+RESULTS:
: 3.142068

#+begin_src julia :exports none 
ts = -π:0.01:π
p = plot(
    cos.(ts), sin.(ts);
    label="",
    size=(400, 400),
    xlim=(-1.1, 1.1),
    ylim=(-1.1, 1.1),
    ticks=:none,
    border=:none
)

anim = @animate for i=1:100
    x, y = 2 .* rand(2) .- 1
    c = x^2 + y^2 < 1 ? :green : :red
    scatter!(p, [x], [y], color=c, label="", markersize=2, markerstrokewidth=0.1)
    p
end every 1
gif(anim, outputdir("pi.gif"));
#+end_src

#+RESULTS:
: [ Info: Saved animation to /drive-2/Projects/public/Turing-Workshop/2023-MRC-BSU-and-UKHSA/TorsWorkshop/assets/outputs/more-julia/pi.gif

[[./assets/outputs/more-julia/pi.gif]]

* Scientific computing ecosystem
- [[https://dataframes.juliadata.org/stable/][DataFrames.jl]], etc.
- [[https://docs.juliaplots.org/stable/][Plots.jl]], etc.
- [[https://juliastats.org/Distributions.jl/stable/][Distributions.jl]], etc.
- [[https://docs.sciml.ai/Optimization/stable/][Optimization.jl]] and all it contains
- [[https://docs.sciml.ai/DiffEqDocs/stable/][DifferentialEquations.jl]]
- Deep learning, e.g. [[https://fluxml.ai/Flux.jl/stable/][Flux.jl]]
- Automatic Differentiation
- BenchmarkTools.jl
* Running example

We'll work with an outbreak of influenza A (H1N1) in 1978 at a British boarding school

- 763 male students -> 512 of which became ill
- Reported that one infected boy started the epidemic
- Observations are number of boys in bed over 14 days

Data are freely available in the R package =outbreaks=, maintained as part of the [[http://www.repidemicsconsortium.org/][R Epidemics Consortium]]

* Distributions.jl

#+begin_src julia
using Distributions
#+end_src

#+RESULTS:

In Julia, the go-to for working with distributions is [[https://juliastats.org/Distributions.jl/stable/][=Distributions.jl=]]

This package provides a large number of distributions (see the docs for a full overview)

Used throughout the Julia community, e.g. =Turing= uses this

#+REVEAL: split

#+begin_src julia 
dist = Normal()
#+end_src

#+RESULTS:
: Normal{Float64}(μ=0.0, σ=1.0)

#+begin_src julia :results scalar
mean(dist), var(dist)
#+end_src

#+RESULTS:
: (0.0, 1.0)

Remeber the =Random.rand= function from earlier? This now also accepts a =Distribution=

#+begin_src julia 
x = rand(dist)
#+end_src

#+RESULTS:
: -0.9250567049017008

#+REVEAL: split

#+begin_src julia 
logpdf(dist, x)
#+end_src

#+RESULTS:
: -1.346803486846469

#+begin_src julia
cdf(dist, 0.5)
#+end_src

#+RESULTS:
: 0.6914624612740131

#+begin_src julia 
quantile.(Normal(), [0.05, 0.5, 0.95])
#+end_src

#+RESULTS:
: 3-element Vector{Float64}:
:  -1.6448536269514724
:   0.0
:   1.6448536269514717

There is also maximum likelihood estimation (MLE)

#+begin_src julia
xs = rand(Normal(1, 2), 100)
fit(Normal, xs)
#+end_src

#+RESULTS:
: Normal{Float64}(μ=1.1650560616082066, σ=1.7370190617792156)

* DataFrames.jl

#+begin_src julia :display text/plain
using DataFrames
#+end_src

In Julia, the go-to for working with datasets is =DataFrames.jl=

#+HTML: <div class="small-text">

If you don't want to let go of the =tidyverse= and you don't mind a bunch of magic, you can use https://github.com/TidierOrg/Tidier.jl

#+HTML: </div>

#+REVEAL: split

If you're already familiar with equivalents in R or Python, the following is a great reference: https://dataframes.juliadata.org/stable/man/comparisons/

#+DOWNLOADED: file:///tmp/Spectacle.paVgAL/Screenshot_20230915_114925.png @ 2023-09-15 11:49:35
#+attr_org: :width 600px
[[file:.more-julia/attachments/2023-09-15_11-49-35_Screenshot_20230915_114925.png]]

#+REVEAL: split

There are many different ways to construct a =DataFrame=

#+begin_src julia :display text/plain
df = DataFrame(A=1:3, B=5:7, fixed=1)
#+end_src

#+RESULTS:
: 3×3 DataFrame
:  Row │ A      B      fixed 
:      │ Int64  Int64  Int64 
: ─────┼─────────────────────
:    1 │     1      5      1
:    2 │     2      6      1
:    3 │     3      7      1

#+begin_src julia :display text/plain
DataFrame(Dict("A" => 1:3, "B" => 5:7, "fixed" => 1))
#+end_src

#+RESULTS:
: 3×3 DataFrame
:  Row │ A      B      fixed 
:      │ Int64  Int64  Int64 
: ─────┼─────────────────────
:    1 │     1      5      1
:    2 │     2      6      1
:    3 │     3      7      1

Notice that columns are typed

#+REVEAL: split

Then we can interact with the =DataFrame= in a variety of ways

** Indexing

#+begin_src julia 
df.A
#+end_src

#+RESULTS:
: 3-element Vector{Int64}:
:  1
:  2
:  3

#+begin_src julia
df."A"  # useful when column-names aren't valid Julia symbols
#+end_src

#+RESULTS:
: 3-element Vector{Int64}:
:  1
:  2
:  3

#+begin_src julia
df[:, "A"]
#+end_src

#+RESULTS:
: 3-element Vector{Int64}:
:  1
:  2
:  3

#+REVEAL: split

#+begin_src julia :display text/plain
df[:, [:A, :B]]
#+end_src

#+RESULTS:
: 3×2 DataFrame
:  Row │ A      B     
:      │ Int64  Int64 
: ─────┼──────────────
:    1 │     1      5
:    2 │     2      6
:    3 │     3      7

#+begin_src julia :display text/plain
df[:, Not(:fixed)]
#+end_src

#+RESULTS:
: 3×2 DataFrame
:  Row │ A      B     
:      │ Int64  Int64 
: ─────┼──────────────
:    1 │     1      5
:    2 │     2      6
:    3 │     3      7

#+REVEAL: split

#+begin_src julia
names(df)
#+end_src

#+RESULTS:
: 3-element Vector{String}:
:  "A"
:  "B"
:  "fixed"

** Actual data

Let's load the actual data

Our data is a CSV file

#+begin_src julia 
readdir("data")
#+end_src

#+RESULTS:
: 1-element Vector{String}:
:  "influenza_england_1978_school.csv"

Functionality for different file formats is usually provided by separate packages:
- [[https://github.com/JuliaData/CSV.jl][CSV.jl]]
- [[https://github.com/JuliaData/Arrow.jl][Arrow.jl]] (Apache Arrow)
- [[https://github.com/JuliaData/RData.jl][RData.jl]] (R data files)
- [[https://felipenoris.github.io/XLSX.jl/stable/][XLSX.jl]] (Excel files)
- And more.

#+REVEAL: split

In our case, we're working with a CSV file, so we'll use =CSV.jl=:

#+begin_src julia :display text/plain
using CSV
datafile = CSV.File(joinpath("data", "influenza_england_1978_school.csv"));
#+end_src

#+RESULTS:

And then we can convert this =CSV.File= into a =DataFrame=

#+begin_src julia :display text/plain
data = DataFrame(datafile)
#+end_src

#+RESULTS:
#+begin_example
14×4 DataFrame
 Row │ Column1  date        in_bed  convalescent 
     │ Int64    Date        Int64   Int64        
─────┼───────────────────────────────────────────
   1 │       1  1978-01-22       3             0
   2 │       2  1978-01-23       8             0
   3 │       3  1978-01-24      26             0
   4 │       4  1978-01-25      76             0
   5 │       5  1978-01-26     225             9
   6 │       6  1978-01-27     298            17
   7 │       7  1978-01-28     258           105
   8 │       8  1978-01-29     233           162
   9 │       9  1978-01-30     189           176
  10 │      10  1978-01-31     128           166
  11 │      11  1978-02-01      68           150
  12 │      12  1978-02-02      29            85
  13 │      13  1978-02-03      14            47
  14 │      14  1978-02-04       4            20
#+end_example

#+REVEAL: split

#+begin_quote
Woah, how does this work? We just passed a =CSV.File= to =DataFrames.DataFrame=, and _it just works_?!
#+end_quote

Aye, that's right

This is thanks to [[https://tables.juliadata.org/stable/][Tables.jl]], a simple interface for tabular data

Such light-weight interface packages allow modules to seemlessly interact with each other without explicit dependencies

This is a very typical pattern in Julia

#+REVEAL: split

* Plots.jl

#+begin_src julia 
using Plots
#+end_src

#+RESULTS:

The most commonly used plotting library is [[https://docs.juliaplots.org/stable/][Plots.jl]]

#+REVEAL: split

Has many backends, including:
- GR
- PyPlot
- Plotly
- Unicode
- PGFPlots
- And more

_But_ the code is the same for all backends

#+begin_src julia
# GR is used by default
Plots.backend()
#+end_src

#+RESULTS:
: Plots.GRBackend()

#+REVEAL: split

#+HTML: <div class="side-by-side">

#+HTML: <div>

#+begin_src julia
p1 = plot(1:10, rand(10), size=(450, 200))
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/16d623348c3f18f48e1304d2e2300b5e289e1b86.svg]]

#+HTML: </div>

#+HTML: <div>

#+begin_src julia
p2 = scatter(1:10, rand(10), size=(450, 200))
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/7cc0dffb8818815a0ac1bd1a2ed82b37365dceba.svg]]

#+HTML: </div>

#+HTML: </div>

#+begin_src julia
plot(p1, p2, layout=(1, 2), size=(800, 200))
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/98e1cf87a13e33c358badda9dd87043e02cc227a.svg]]

#+REVEAL: split

A neat example from [[https://docs.juliaplots.org/stable/#simple-is-beautiful][the docs]]

#+begin_src julia 
# Define the Lorenz attractor
Base.@kwdef mutable struct Lorenz
    dt::Float64 = 0.02
    σ::Float64 = 10
    ρ::Float64 = 28
    β::Float64 = 8/3
    x::Float64 = 1
    y::Float64 = 1
    z::Float64 = 1
end

function step!(l::Lorenz)
    dx = l.σ * (l.y - l.x)
    dy = l.x * (l.ρ - l.z) - l.y
    dz = l.x * l.y - l.β * l.z
    l.x += l.dt * dx
    l.y += l.dt * dy
    l.z += l.dt * dz
end

attractor = Lorenz()
#+end_src

#+RESULTS:
: Lorenz(0.02, 10.0, 28.0, 2.6666666666666665, 1.0, 1.0, 1.0)

#+begin_src julia :exports none
outputdir(args...) = joinpath("assets", "outputs", "more-julia", args...)
#+end_src

#+RESULTS:
: outputdir (generic function with 1 method)

#+REVEAL: split

#+begin_src julia :eval no
# Initialize a 3D plot with 1 empty series
plt = plot3d(
    1,
    xlim = (-30, 30),
    ylim = (-30, 30),
    zlim = (0, 60),
    title = "Lorenz Attractor",
    legend = false,
    marker = 2,
)

# Build an animated gif by pushing new points to the plot, saving every 10th frame
anim = @animate for i=1:1500
    step!(attractor)
    push!(plt, attractor.x, attractor.y, attractor.z)
end every 10
gif(anim, outputdir("lorenz.gif"));
#+end_src

#+RESULTS:
: [ Info: Saved animation to /drive-2/Projects/public/Turing-Workshop/2023-MRC-BSU-and-UKHSA/TorsWorkshop/assets/outputs/more-julia/lorenz.gif

#+REVEAL: split

[[./assets/outputs/more-julia/lorenz.gif]]

** Ecosystem

Plots.jl also has a very nice recipe-system

Allows you to define how to plot your own types

As a result, packages often define customized plotting recipes for their types

https://docs.juliaplots.org/latest/ecosystem/#Community-packages

** StatsPlots.jl

For us, [[https://github.com/JuliaPlots/StatsPlots.jl][StatsPlots.jl]] is particularly relevant

#+begin_src julia 
using StatsPlots
#+end_src

#+RESULTS:

It contains custom plotting functionality for dataframes and distibutions

#+REVEAL: split

#+begin_src julia 
plot(Normal())
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/348bd16b3b818fcfbedfaf14f50cb941ca1b7ca3.svg]]

#+REVEAL: split

It also contains the macro =@df= for working with dataframes

#+begin_src julia 
@df data scatter(:date, :in_bed, label=nothing, ylabel="Number of students in bed")
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/1425b863a8661542a4b00f6bce46205730eb2531.svg]]

** TASK Recreate the above plot using standard Plots.jl functionality

That is, don't use =@df data ...=, but call the =scatter= function directly

* DifferentialEquations.jl

#+begin_src julia 
using DifferentialEquations
#+end_src

#+RESULTS:

Everything related to differential equations is provided by [[https://docs.sciml.ai/DiffEqDocs/stable/][=DifferentialEquations.jl=]] and the [[https://sciml.ai/][SciML ecosystem]]

#+REVEAL: split

And I really do mean [[https://docs.sciml.ai/DiffEqDocs/stable/][/everything/]]

#+HTML: <div class="side-by-side">

#+DOWNLOADED: file:///tmp/Spectacle.jWiYMk/Screenshot_20230119_194737.png @ 2023-01-19 19:48:23
[[file:assets/attachments/2023-01-19_19-48-23_Screenshot_20230119_194737.png]]

#+DOWNLOADED: file:///tmp/Spectacle.jWiYMk/Screenshot_20230119_194838.png @ 2023-01-19 19:48:41
[[file:assets/attachments/2023-01-19_19-48-41_Screenshot_20230119_194838.png]]

#+HTML: </div>


** Differential equations

Suppose we have some function $f$ which describes how a state $x$ evolves wrt. $t$
\begin{equation*}
\frac{\mathrm{d} x}{\mathrm{d} t} = f(x, t)
\end{equation*}
which we then need to integrate to obtain the actual state at some time $t$
\begin{equation*}
x(t) = \int_{0}^{t} \frac{\mathrm{d} x}{\mathrm{d} t} \mathrm{d} t = \int_{0}^{t} f(x, t) \mathrm{d} t
\end{equation*}

In many interesting scenarios numerical methods are required to obtain $x(t)$

** Example: SIR model
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Example-SIR-model
:CUSTOM_ID: 2023-01-29-16-57-28-Example-SIR-model
:END:
One particular example of an (ordinary) differential equation that you might have seen recently is the *SIR model* used in epidemiology

#+DOWNLOADED: file:///home/tor/Downloads/sir_illu.png @ 2023-01-19 19:56:00
#+ATTR_ORG: :width 600
#+CAPTION: https://covid19.uclaml.org/model.html (2023-01-19)
[[file:assets/attachments/2023-01-19_19-56-00_sir_illu.png]]

#+REVEAL: split

The temporal dynamics of the sizes of each of the compartments are governed by the following system of ODEs:
\begin{equation*}
\begin{split}
  \frac{\mathrm{d} S}{\mathrm{d} t} &= - \beta S \frac{I}{N} \\
  \frac{\mathrm{d} I}{\mathrm{d} t} &= \beta S \frac{I}{N} - \gamma I \\
  \frac{\mathrm{d} R}{\mathrm{d} t} &= \gamma I
\end{split}
\end{equation*}
where
- $S(t)$ is the number of people susceptible to becoming infected,
- $I(t)$ is the number of people currently infected,
- $R(t)$ is the number of recovered people,
- $β$ is the constant rate of infectious contact between people,
- $\gamma$ the constant recovery rate of infected individuals

#+REVEAL: split

Converting this ODE into code is just

#+begin_src julia
const N = 763 # size of population

function SIR!(
    du,  # buffer for the updated differential equation
    u,   # current state
    p,   # parameters
    t    # current time
)
    S, I, R = u
    β, γ = p

    du[1] = dS = -β * I * S / N
    du[2] = dI = β * I * S / N - γ * I
    du[3] = dR = γ * I
end
#+end_src

#+RESULTS:
: SIR! (generic function with 1 method)

Not too bad!

#+REVEAL: split

Initial conditions are then
\begin{equation*}
\begin{split}
  S(0) &= N - 1 \\
  I(0) &= 1 \\
  R(0) &= 0
\end{split}
\end{equation*}
and we want to integrate from $t = 0$ to $t = 14$

#+begin_src julia
# Include 0 because that's the initial condition before any observations.
tspan = (0.0, 14.0)

# Initial conditions are:
#   S(0) = N - 1; I(0) = 1; R(0) = 0
u0 = [N - 1, 1, 0.0]
#+end_src

#+RESULTS:
: 3-element Vector{Float64}:
:  762.0
:    1.0
:    0.0

#+REVEAL: split

Now we just need to define the overall problem and we can solve:

#+begin_src julia
# Just to check that everything works, we'll just use some "totally random" values for β and γ:
problem_sir = let β = 2.0, γ = 0.6
    ODEProblem(SIR!, u0, tspan, (β, γ))
end
#+end_src

#+RESULTS:
: ODEProblem with uType Vector{Float64} and tType Float64. In-place: true
: timespan: (0.0, 14.0)
: u0: 3-element Vector{Float64}:
:  762.0
:    1.0
:    0.0

#+REVEAL: split

Aaaand

#+begin_src julia
sol = solve(problem_sir)
#+end_src

#+RESULTS:
#+begin_example
retcode: Success
Interpolation: specialized 4th order "free" interpolation, specialized 2nd order "free" stiffness-aware interpolation
t: 23-element Vector{Float64}:
  0.0
  0.0023558376404244326
  0.025914214044668756
  0.11176872871946908
  0.26714420676761075
  0.47653584778586056
  0.7436981238065388
  1.0701182881347182
  1.4556696154809898
  1.8994815718103506
  2.4015425820305163
  2.9657488203418048
  3.6046024613854746
  4.325611232479916
  5.234036476235002
  6.073132270491685
  7.323851265223563
  8.23100744184026
  9.66046960467715
 11.027717843180652
 12.506967592177675
 13.98890399536329
 14.0
u: 23-element Vector{Vector{Float64}}:
 [762.0, 1.0, 0.0]
 [761.9952867607622, 1.003297407481751, 0.001415831756055325]
 [761.9472927630898, 1.036873767352754, 0.015833469557440357]
 [761.7584189579304, 1.1690001128296739, 0.0725809292398516]
 [761.353498610305, 1.4522140137552049, 0.19428737593979384]
 [760.6490369821046, 1.9447820690728455, 0.4061809488225752]
 [759.3950815454128, 2.8210768113583082, 0.7838416432288186]
 [757.0795798160242, 4.437564277195732, 1.4828559067800167]
 [752.6094742865345, 7.552145919430467, 2.8383797940350495]
 [743.573784947305, 13.823077731564027, 5.603137321131049]
 [724.5575481927715, 26.909267078762316, 11.533184728466205]
 [683.6474029897502, 54.51612001957392, 24.836476990675976]
 [598.1841629858786, 109.41164143668018, 55.40419557744127]
 [450.08652743810205, 192.396449154863, 120.51702340703504]
 [259.11626253270623, 256.9925778114915, 246.89115965580237]
 [148.3573731526537, 240.10301213899098, 374.53961470835543]
 [76.52998017846475, 160.6373332952353, 525.8326865263001]
 [55.70519994004921, 108.7634182279299, 598.531381832021]
 [41.39587834423381, 55.09512088924873, 666.5090007665176]
 [35.87067243374374, 27.821838135708532, 699.3074894305479]
 [33.252184333490774, 13.087185981359177, 716.6606296851502]
 [32.08996839417716, 6.105264616193066, 724.8047669896299]
 [32.08428686823946, 6.070415830241046, 724.8452973015196]
#+end_example


#+REVEAL: split

We didn't specify a solver

DifferentialEquations.jl uses =AutoTsit5(Rosenbrock32())= by default 

Which is a composition between

- =Tsit5= (4th order Runge-Kutta), and
- =Rosenbrock32= (3rd order stiff solver)

with automatic switching between the two

#+REVEAL: split

=AutoTsit5(Rosenbrock32())= covers many use-cases well, but see

- https://docs.sciml.ai/DiffEqDocs/stable/solvers/ode_solve/
- https://www.stochasticlifestyle.com/comparison-differential-equation-solver-suites-matlab-r-julia-python-c-fortran/

for more info on choosing a solver

#+REVEAL: split

This is the resulting solution

#+begin_src julia
plot(
    sol,
    linewidth=2, xaxis="Time in days", label=["Suspectible" "Infected" "Recovered"],
    alpha=0.5, size=(500, 300)
)
scatter!(1:14, data.in_bed, label="Data", color="black")
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/780e8cc1c00812abcfba1fe711f0a8b7b168b7e7.svg]]

This doesn't really match the data though; let's do better

#+REVEAL: split

*Approach #1:* find optimal values of $\beta$ and $\gamma$ by minimizing some loss, e.g. sum-of-squares

\begin{equation*}
\ell(\beta, \gamma) = \sum_{i = 1}^{14} \bigg( F(u_0, t_i;\ \beta, \gamma) - y_i \bigg)^2
\end{equation*}

where $\big( y_i \big)_{i = 1}^{14}$ are the observations, $F$ is the integrated system

** Optimization.jl

In Julia, there are /tons/ of packages for performing all kinds of optimization

[[https://docs.sciml.ai/Optimization/stable/][Optimization.jl]] provides a convenient interface to many of them

#+begin_src julia 
using Optimization
#+end_src

#+RESULTS:

#+DOWNLOADED: file:///tmp/Spectacle.paVgAL/Screenshot_20230915_130639.png @ 2023-09-15 13:06:48
#+CAPTION: https://docs.sciml.ai/Optimization/stable/#Overview-of-the-Optimizers (2023-09-15)
#+attr_org: :width 600px
#+attr_html: :width 400px
[[file:.more-julia/attachments/2023-09-15_13-06-48_Screenshot_20230915_130639.png]]


#+REVEAL: split

#+HTML: <div class="small-text">

Recall we want to solve

\begin{equation*}
\min_{\beta, \gamma} \sum_{i = 1}^{14} \bigg( F(u_0, t_i;\ \beta, \gamma) - y_i \bigg)^2
\end{equation*}

where $\big( y_i \big)_{i = 1}^{14}$ are the observations, $F$ is the integrated system

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

First we define the loss

#+begin_src julia
# Define the loss function.
function loss_sir(problem_orig, p)
    # `remake` just, well, remakes the `problem` with parameters `p` replaced.
    problem = remake(problem_orig, p=p)
    # To ensure we get solutions _exactly_ at the timesteps of interest,
    # i.e. every day we have observations, we use `saveat=1` to tell `solve`
    # to save at every timestep (which is one day).
    sol = solve(problem, saveat=1)
    # Extract the 2nd state, the (I)infected, for the dates with observations.
    sol_for_observed = sol[2, 2:15]
    # Compute the sum-of-squares of the infected vs. data.
    return sum(abs2.(sol_for_observed - data.in_bed))
end
#+end_src

#+RESULTS:
: loss_sir (generic function with 1 method)

#+HTML: </div>

#+REVEAL: split

Then we can define our =OptimizationProblem=

#+begin_src julia 
opt_problem = OptimizationProblem(
    OptimizationFunction(
        (p,_) -> loss_sir(problem_sir, p), # function to minimize
        Optimization.AutoForwardDiff()     # use ForwardDiff for automatic differentiation
    ),
    [2.0, 0.5],                            # initial values
    lb = [0, 0],                           # lower bounds on variables
    ub = [Inf, Inf],                       # upper bounds on variables
) 
#+end_src

#+RESULTS:
: OptimizationProblem. In-place: true
: u0: 2-element Vector{Float64}:
:  2.0
:  0.5

#+REVEAL: split

And for general /deterministic/ problems, [[https://julianlsolvers.github.io/Optim.jl/stable/][Optim.jl]] is a good choice

#+begin_src julia
using OptimizationOptimJL
opt = solve(opt_problem, NelderMead())
#+end_src

#+RESULTS:
: u: 2-element Vector{Float64}:
:  1.6692320164955483
:  0.44348639177622445

#+begin_src julia :results scalar
β, λ = opt
β, λ
#+end_src

#+RESULTS:
: (1.6692320164955483, 0.44348639177622445)

#+REVEAL: split

#+begin_src julia
# Solve the problem with the obtained parameters.
problem_sir = remake(problem_sir, p=(β, λ))
sol = solve(problem_sir)

# Plot the solution.
plot(sol, linewidth=2, xaxis="Time in days", label=["Susceptible" "Infected" "Recovered"], alpha=0.5)
# And the data.
scatter!(1:14, data.in_bed, label="Data", color="black")
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/c4a9d9cc8632820164ac0b1c77cded80e46b9a39.svg]]

That's better than our /totally/ "random" guess from earlier!

** Example: SEIR model
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Example-SEIR-model
:CUSTOM_ID: 2023-01-29-16-57-28-Example-SEIR-model
:END:

Adding another compartment to our SIR model: the _(E)xposed_ state

\begin{equation*}
\begin{split}
  \frac{\mathrm{d} S}{\mathrm{d} t} &= - \beta S \frac{I}{N} \\
  \frac{\mathrm{d} {\color{blue} E}}{\mathrm{d} t} &= \beta S \frac{I}{N} - {\color{orange} \sigma} {\color{blue} E} \\
  \frac{\mathrm{d} I}{\mathrm{d} t} &= {\color{orange} \sigma} {\color{blue} E} - \gamma I \\
  \frac{\mathrm{d} R}{\mathrm{d} t} &= \gamma I
\end{split}
\end{equation*}

where we've added a new parameter ${\color{orange} \sigma}$ describing the fraction of people who develop observable symptoms in this time

** TASK Solve the SEIR model using Julia
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Solve-the-SEIR-model-using-Julia
:CUSTOM_ID: 2023-01-29-16-57-28-Solve-the-SEIR-model-using-Julia
:END:

#+begin_src julia :eval no
function SEIR!(
    du,  # buffer for the updated differential equation
    u,   # current state
    p,   # parameters
    t    # current time
)
    N = 763  # population

    S, E, I, R = u  # have ourselves an additional state!
    β, γ, σ = p     # and an additional parameter!

    # TODO: Implement yah fool!
    du[1] = nothing
    du[2] = nothing
    du[3] = nothing
    du[4] = nothing
end
#+end_src

*BONUS:* find minimizers of sum-of-squares

#+begin_src julia :exports (by-backend (reveal "none") (t "code"))
# Some space so you don't cheat.



















# Are you sure?
#+end_src

#+RESULTS:

** SOLUTION Solve the SEIR model using Julia
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Solve-the-SEIR-model-using-Julia
:CUSTOM_ID: 2023-01-29-16-57-28-Solve-the-SEIR-model-using-Julia
:END:

#+begin_src julia
function SEIR!(
    du,  # buffer for the updated differential equation
    u,   # current state
    p,   # parameters
    t    # current time
)
    N = 763  # population
    S, E, I, R = u  # have ourselves an additional state!
    β, γ, σ = p     # and an additional parameter!

    # Might as well cache these computations.
    βSI = β * S * I / N
    σE = σ * E
    γI = γ * I

    du[1] = -βSI
    du[2] = βSI - σE
    du[3] = σE - γI
    du[4] = γI
end
#+end_src

#+RESULTS:
: SEIR! (generic function with 1 method)

#+REVEAL: split

#+begin_src julia
problem_seir = let u0 = [N - 1, 0, 1, 0], β = 2.0, γ = 0.6, σ = 0.8
    ODEProblem(SEIR!, u0, tspan, (β, γ, σ))
end
#+end_src

#+RESULTS:
: ODEProblem with uType Vector{Int64} and tType Float64. In-place: true
: timespan: (0.0, 14.0)
: u0: 4-element Vector{Int64}:
:  762
:    0
:    1
:    0

#+begin_src julia
sol_seir = solve(problem_seir, saveat=1)
#+end_src

#+RESULTS:
#+begin_example
retcode: Success
Interpolation: 1st order linear
t: 15-element Vector{Float64}:
  0.0
  1.0
  2.0
  3.0
  4.0
  5.0
  6.0
  7.0
  8.0
  9.0
 10.0
 11.0
 12.0
 13.0
 14.0
u: 15-element Vector{Vector{Float64}}:
 [762.0, 0.0, 1.0, 0.0]
 [760.1497035901518, 1.277915971753478, 1.015887135649055, 0.5564933024456415]
 [757.5476928906271, 2.425869618233348, 1.6850698824327135, 1.341367608706787]
 [753.081189706403, 4.277014534677882, 2.9468385687120784, 2.6949571902067637]
 [745.3234082630842, 7.455598293492681, 5.155811621098982, 5.065181822323939]
 [731.9851682751213, 12.855816151849933, 8.960337047554939, 9.198678525473571]
 [709.5042941973462, 21.77178343781762, 15.384985521594785, 16.338936843241182]
 [672.8733895183619, 35.77263271085456, 25.88133104438007, 28.472646726403138]
 [616.390571176038, 55.9717775696742, 42.09614416178475, 48.54150709250277]
 [536.453596476594, 81.2428045994271, 64.9673325777641, 80.33626634621449]
 [436.43708330634297, 106.04037246704702, 92.9550757379631, 127.56746848864664]
 [329.60092931771436, 121.08020372279418, 120.48402926084937, 191.83483769864185]
 [233.8471941518982, 119.43669383157659, 139.3233304893263, 270.3927815271987]
 [160.88805352426687, 102.7399386960996, 143.3826208089892, 355.98938697064415]
 [111.72261866282292, 79.02493776169311, 132.78384886713565, 439.46859470834806]
#+end_example

#+REVEAL: split

#+begin_src julia
plot(sol_seir, linewidth=2, xaxis="Time in days", label=["Susceptible" "Exposed" "Infected" "Recovered"], alpha=0.5)
scatter!(1:14, data.in_bed, label="Data")
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/cf61c8af8f10097a63ca84e48ff0b4831f7d8a59.svg]]

Don't look so good. Let's try Optim.jl again.

#+REVEAL: split

#+begin_src julia
function loss_seir(problem, p)
    problem = remake(problem, p=p)
    sol = solve(problem, saveat=1)
    # NOTE: 3rd state is now the (I)nfectious compartment!!!
    sol_for_observed = sol[3, 2:15]
    return sum(abs2.(sol_for_observed - data.in_bed))
end
#+end_src

#+RESULTS:
: loss_seir (generic function with 1 method)

#+begin_src julia
opt = optimize(Base.Fix1(loss_seir, problem_seir), [0, 0, 0], [Inf, Inf, Inf], [2.0, 0.5, 0.9], Fminbox(NelderMead()))
#+end_src

#+RESULTS:
#+begin_example
,* Status: success (reached maximum number of iterations)

,* Candidate solution
   Final objective value:     3.115978e+03

,* Found with
   Algorithm:     Fminbox with Nelder-Mead

,* Convergence measures
   |x - x'|               = 0.00e+00 ≤ 0.0e+00
   |x - x'|/|x'|          = 0.00e+00 ≤ 0.0e+00
   |f(x) - f(x')|         = 0.00e+00 ≤ 0.0e+00
   |f(x) - f(x')|/|f(x')| = 0.00e+00 ≤ 0.0e+00
   |g(x)|                 = 1.77e+05 ≰ 1.0e-08

,* Work counters
   Seconds run:   1  (vs limit Inf)
   Iterations:    3
   f(x) calls:    13286
   ∇f(x) calls:   1
#+end_example

#+REVEAL: split

#+begin_src julia
β, γ, σ = Optim.minimizer(opt)
#+end_src

#+RESULTS:
: 3-element Vector{Float64}:
:  4.853872993924619
:  0.4671485850111774
:  0.8150294098438762

#+begin_src julia
sol_seir = solve(remake(problem_seir, p=(β, γ, σ)), saveat=1)
plot(sol_seir, linewidth=2, xaxis="Time in days", label=["Susceptible" "Exposed" "Infected" "Recovered"], alpha=0.5)
scatter!(1:14, data.in_bed, label="Data", color="black")
#+end_src

#+RESULTS:
[[file:assets/outputs/62ee261fb15aead33e6f49e82e01a7482b56017f.png]]

#+REVEAL: split

#+begin_quote
But...but these are _point estimates_! What about distributions? WHAT ABOUT UNCERTAINTY?!
#+end_quote

No, no that's fair.

Let's do some Bayesian inference then.

BUT FIRST!

** Making our future selves less annoyed
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Making-our-future-selves-less-annoyed
:CUSTOM_ID: 2023-01-29-16-57-28-Making-our-future-selves-less-annoyed
:END:

It's annoying to have all these different loss-functions for /both/ =SIR!= and =SEIR!=

#+HTML: <div class="fragment (appear)">

#+begin_src julia
# Abstract type which we can use to dispatch on.
abstract type AbstractEpidemicProblem end

struct SIRProblem{P} <: AbstractEpidemicProblem
    problem::P
    N::Int
end

function SIRProblem(N::Int; u0 = [N - 1, 1, 0.], tspan = (0, 14), p = [2.0, 0.6])
    return SIRProblem(ODEProblem(SIR!, u0, tspan, p), N)
end
#+end_src

#+RESULTS:
: SIRProblem

Then we can just construct the problem as

#+begin_src julia
sir = SIRProblem(N);
#+end_src

#+RESULTS:

#+HTML: </div>

#+REVEAL: split

And to make it a bit easier to work with, we add some utility functions

#+begin_src julia
# General.
parameters(prob::AbstractEpidemicProblem) = prob.problem.p
initial_state(prob::AbstractEpidemicProblem) = prob.problem.u0
population(prob::AbstractEpidemicProblem) = prob.N

# Specializations.
susceptible(::SIRProblem, u::AbstractMatrix) = u[1, :]
infected(::SIRProblem, u::AbstractMatrix) = u[2, :]
recovered(::SIRProblem, u::AbstractMatrix) = u[3, :]
#+end_src

#+RESULTS:
: recovered (generic function with 2 methods)

So that once we've solved the problem, we can easily extract the compartment we want, e.g.

#+begin_src julia
sol = solve(sir.problem, saveat=1)
infected(sir, sol)
#+end_src

#+RESULTS:
#+begin_example
15-element Vector{Float64}:
   1.0
   4.026799533924021
  15.824575905720002
  56.779007685250534
 154.4310579906169
 248.98982384839158
 243.67838619968524
 181.93939659551987
 120.64627375763271
  75.92085282572398
  46.58644927641269
  28.214678599716418
  16.96318676577873
  10.158687874394722
   6.070415830241046
#+end_example

** TASK Implement =SEIRProblem=
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Implement-SEIRProblem
:CUSTOM_ID: 2023-01-29-16-57-28-Implement-SEIRProblem
:END:

#+begin_src julia :eval no
struct SEIRProblem <: AbstractEpidemicProblem
    # ...
end

function SEIRProblem end

susceptible
exposed
infected
recovered
#+end_src

#+begin_src julia :exports (by-backend (reveal "none") (t "code"))
# Some space so you don't cheat.



















# Are you sure?
#+end_src

#+RESULTS:

** SOLUTION Implement =SEIRProblem=
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Implement-SEIRProblem
:CUSTOM_ID: 2023-01-29-16-57-28-Implement-SEIRProblem
:END:

#+begin_src julia
struct SEIRProblem{P} <: AbstractEpidemicProblem
    problem::P
    N::Int
end

function SEIRProblem(N::Int; u0 = [N - 1, 0, 1, 0.], tspan = (0, 14), p = [4.5, 0.45, 0.8])
    return SEIRProblem(ODEProblem(SEIR!, u0, tspan, p), N)
end

susceptible(::SEIRProblem, u::AbstractMatrix) = u[1, :]
exposed(::SEIRProblem, u::AbstractMatrix) = u[2, :]
infected(::SEIRProblem, u::AbstractMatrix) = u[3, :]
recovered(::SEIRProblem, u::AbstractMatrix) = u[4, :]
#+end_src

#+RESULTS:
: recovered (generic function with 2 methods)

#+REVEAL: split

Now, given a =problem= and a =sol=, we can query the =sol= for the =infected= state _without explicit handling of which =problem= we're working with_

#+begin_src julia
seir = SEIRProblem(N);
sol = solve(seir.problem, saveat=1)
infected(seir, sol)
#+end_src

#+RESULTS:
#+begin_example
15-element Vector{Float64}:
   1.0
   1.9941817088874336
   6.958582307202902
  23.9262335176065
  74.23638542794971
 176.98368495653585
 276.06126059898344
 293.92632518571605
 249.92836195453708
 189.07578975511504
 134.2373192679034
  91.82578430804273
  61.38108478932363
  40.42264366743211
  26.357816296754425
#+end_example

** Same =loss= for both!
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Same-loss-for-both
:CUSTOM_ID: 2023-01-29-16-57-28-Same-loss-for-both
:END:

#+begin_src julia
function loss(problem_wrapper::AbstractEpidemicProblem, p)
    # NOTE: Extract the `problem` from `problem_wrapper`.
    problem = remake(problem_wrapper.problem, p=p)
    sol = solve(problem, saveat=1)
    # NOTE: Now this is completely general!
    sol_for_observed = infected(problem_wrapper, sol)[2:end]
    return sum(abs2.(sol_for_observed - data.in_bed))
end
#+end_src

#+RESULTS:
: loss (generic function with 1 method)

Now we can call the _same =loss= for both_ =SIR= and =SEIR=

#+begin_src julia 
loss(SIRProblem(N), [2.0, 0.6])
#+end_src

#+RESULTS:
: 50257.83978134881

#+begin_src julia 
loss(SEIRProblem(N), [2.0, 0.6, 0.8])
#+end_src

#+RESULTS:
: 287325.105532706



* Turing.jl
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Bayesian-inference
:CUSTOM_ID: 2023-01-29-16-57-28-Bayesian-inference
:END:

#+begin_src julia
using Turing
#+end_src

#+RESULTS:

and so we are finally here

#+RESULTS:

** Simplest demo

#+begin_src julia :async yes
# 1. Define the model
@model function simple_demo(x, y)
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    x ~ Normal(m, sqrt(s))
    y ~ Normal(m, sqrt(s))
end
# 2. Instantiate the model, giving it some data.
model = simple_demo(1.5, 2.0)
# 3. Sample.
chain = sample(model, NUTS(), 1000);
#+end_src

#+RESULTS:
: [36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
: [36m[1m└ [22m[39m  ϵ = 0.8
: [32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00[39m
:

#+REVEAL: split

#+begin_src julia 
chain
#+end_src

#+RESULTS:
#+begin_example
Chains MCMC chain (1000×14×1 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 1
Samples per chain = 1000
Wall duration     = 1.06 seconds
Compute duration  = 1.06 seconds
parameters        = s, m
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯
      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯

           s    2.1682    2.1282    0.1191   424.6322   384.1415    0.9994     ⋯
           m    1.1267    0.8762    0.0439   461.9315   330.6685    0.9999     ⋯
                                                                1 column omitted

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           s    0.5461    1.0918    1.5478    2.3830    8.9461
           m   -0.7510    0.6568    1.1621    1.6589    2.8254
#+end_example

#+REVEAL: split

#+begin_src julia 
plot(chain)
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/1e1f607dc21364143f2f8b67328de490ce367807.svg]]

#+REVEAL: split

#+begin_quote
Okay, what is going on here?
#+end_quote

#+begin_src julia :display text/plain :eval no
# 1. Define the model
@model function simple_demo(x, y)
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    x ~ Normal(m, sqrt(s))
    y ~ Normal(m, sqrt(s))
end
# 2. Instantiate the model, giving it some data.
model = simple_demo(1.5, 2.0)
# 3. Sample.
chain = sample(model, NUTS(), 1000);
#+end_src

Let's break it down!

#+REVEAL: split

To define a model in Turing.jl, we use the =@model= macro

#+begin_src julia 
@model function simple_demo(x, y)
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    x ~ Normal(m, sqrt(s))
    y ~ Normal(m, sqrt(s))
end
#+end_src

#+RESULTS:
: simple_demo (generic function with 4 methods)

which, as we can see, results in a few methods =simple_demo=

- One method is for evaluation of the model
- The rest (one, here) are for constructing the =Model=

#+REVEAL: split

We've seen a =function= before, so that part isn't new

#+begin_src julia :eval no
@model function simple_demo(x)
    ...
end
#+end_src

Roughly, =@model= "transforms" the function =simple_demo= in a "certain way"

#+HTML: <div class="small-text">

#+HTML: <div class="fragment (appear)">

If you /really/ want to have a look, you can execute the following code block

#+begin_src julia :exports code :eval no
@macroexpand @model function demo()
    x ~ Normal()
    return nothing
end
#+end_src

#+RESULTS:
#+begin_example
quote
    function demo(__model__::DynamicPPL.Model, __varinfo__::DynamicPPL.AbstractVarInfo, __context__::AbstractPPL.AbstractContext; )
        #= In[29]:1 =#
        begin
            #= In[29]:1 =#
            #= In[29]:2 =#
            begin
                var"##dist#437" = Normal()
                var"##vn#434" = (DynamicPPL.resolve_varnames)((AbstractPPL.VarName){:x}(), var"##dist#437")
                var"##isassumption#435" = begin
                        if (DynamicPPL.contextual_isassumption)(__context__, var"##vn#434")
                            if !((DynamicPPL.inargnames)(var"##vn#434", __model__)) || (DynamicPPL.inmissings)(var"##vn#434", __model__)
                                true
                            else
                                x === missing
                            end
                        else
                            false
                        end
                    end
                if (DynamicPPL.contextual_isfixed)(__context__, var"##vn#434")
                    x = (DynamicPPL.getfixed_nested)(__context__, var"##vn#434")
                elseif var"##isassumption#435"
                    begin
                        (var"##value#438", __varinfo__) = (DynamicPPL.tilde_assume!!)(__context__, (DynamicPPL.unwrap_right_vn)((DynamicPPL.check_tilde_rhs)(var"##dist#437"), var"##vn#434")..., __varinfo__)
                        x = var"##value#438"
                        var"##value#438"
                    end
                else
                    if !((DynamicPPL.inargnames)(var"##vn#434", __model__))
                        x = (DynamicPPL.getconditioned_nested)(__context__, var"##vn#434")
                    end
                    (var"##value#436", __varinfo__) = (DynamicPPL.tilde_observe!!)(__context__, (DynamicPPL.check_tilde_rhs)(var"##dist#437"), x, var"##vn#434", __varinfo__)
                    var"##value#436"
                end
            end
            #= In[29]:3 =#
            begin
                #= /home/tor/.julia/packages/DynamicPPL/YThRW/src/compiler.jl:555 =#
                var"##retval#439" = nothing
                #= /home/tor/.julia/packages/DynamicPPL/YThRW/src/compiler.jl:556 =#
                return (var"##retval#439", __varinfo__)
            end
        end
    end
    begin
        $(Expr(:meta, :doc))
        function demo(; )
            #= In[29]:1 =#
            return (DynamicPPL.Model)(demo, NamedTuple{()}(()); )
        end
    end
end
#+end_example

to see the actual code being generated

#+HTML: </div>

#+HTML: </div>

#+REVEAL: split

Then we have the "tilde-statements"

#+begin_src julia :eval no
s ~ InverseGamma(2, 3)
m ~ Normal(0, sqrt(s))

x ~ Normal(m, sqrt(s))
y ~ Normal(m, sqrt(s))
#+end_src

#+HTML: <div class="fragment (appear)">

_Important:_ only lines of the form =LEFT ~ RIGHT= are touched by =@model=

⟹ Everything that is _not_ of the form =LEFT ~ RIGHT= is _not_ touched by =@model=

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

#+begin_quote
If it's valid Julia code, it's valid inside a =@model= block
#+end_quote

#+HTML: </div>

#+REVEAL: split

But in our simple demo model, =s= and =m= are treated differently than =x= and =y=

=s= and =m= are considered as random variables to be inferred

=x= and =y= are considered as data / conditioned

#+HTML: <div class="fragment (appear)">

Basically, =L ~ R= is considered a /conditioned/ variable if either
1. =L= is present in the arguments of the function defining the model, or
2. =L= is /conditioned/ using =condition(model, left = ...)= or similar.
3. =L= is a /literal/, e.g. =1.5 ~ Normal()=.

_Otherwise_, =L= is considered a random variable

#+HTML: </div>

#+REVEAL: split

The following are all equivalent

#+begin_src julia :results none
# (1): using the arguments of the function
@model function simple_demo_v1(x, y)
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    x ~ Normal(m, sqrt(s))
    y ~ Normal(m, sqrt(s))
end
model_v1 = simple_demo_v1(1.5, 2.0)

# (2): using the `|` operator / `condition`
@model function simple_demo_v2()
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    x ~ Normal(m, sqrt(s))
    y ~ Normal(m, sqrt(s))
end
model_v2 = simple_demo_v2() | (x = 1.5, y = 2.0)

# (3): when `L` is a literal
@model function simple_demo_v3()
    s ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s))

    1.5 ~ Normal(m, sqrt(s))
    2.0 ~ Normal(m, sqrt(s))
end
model_v3 = simple_demo_v3()
#+end_src

#+REVEAL: split

#+begin_src julia
chain_v1 = sample(model_v1, NUTS(), 1000; progress=false)
chain_v2 = sample(model_v2, NUTS(), 1000; progress=false)
chain_v3 = sample(model_v3, NUTS(), 1000; progress=false)
plot(chainscat(chain_v1, chain_v2, chain_v3))
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/ae7432222c2ded4319fbd5722be20d1312f53a11.svg]]


#+REVEAL: split


#+REVEAL: split

And then we can call it to get a =Model=

#+begin_src julia 
model = simple_demo([1.0, 2.0, 3.0])
#+end_src

#+RESULTS:
: Model(
:   args = (:x,)
:   defaults = ()
:   context = DynamicPPL.DefaultContext()
: )

** Back to our working example

#+REVEAL: split

We'll use the following model
\begin{equation*}
\begin{split}
  \beta &\sim \mathcal{N}_{ + }(2, 1) \\
  \gamma &\sim \mathcal{N}_{ + }(0.4, 0.5) \\
  \phi^{-1} &\sim \mathrm{Exponential}(1/5) \\
   y_i &\sim \mathrm{NegativeBinomial2}\big(F(u_0, t_i;\ \beta, \gamma), \phi \big)
\end{split}
\end{equation*}
where 
- $\big( y_i \big)_{i = 1}^{14}$ are the observations, 
- $F$ is the integrated system, and
- $\phi$ is the over-dispersion parameter.

#+REVEAL: split

#+begin_src julia
plot(
    plot(truncated(Normal(2, 1); lower=0), label=nothing, title="β"),
    plot(truncated(Normal(0.4, 0.5); lower=0), label=nothing, title="γ"),
    plot(Exponential(1/5), label=nothing, title="ϕ⁻¹"),
    layout=(3, 1)
)
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/80c76ad5d3151ec0f3e044a075b2a5b88956d14b.svg]]

#+REVEAL: split

A =NegativeBinomial(r, p)= represents the number of trials to achieve $r$ successes, where each trial has a probability $p$ of success

A =NegativeBinomial2(μ, ϕ)= is the same, but parameterized using the mean $μ$ and /dispersion/ $\phi$

#+begin_src julia
# `NegativeBinomial` already exists, so let's just make an alternative constructor instead.
function NegativeBinomial2(μ, ϕ)
    p = 1/(1 + μ/ϕ)
    r = ϕ
    return NegativeBinomial(r, p)
end
#+end_src

#+RESULTS:
: NegativeBinomial2 (generic function with 1 method)

#+begin_src julia
# Let's just make sure we didn't do something stupid.
μ = 2; ϕ = 3;
dist = NegativeBinomial2(μ, ϕ)
# Source: https://mc-stan.org/docs/2_20/functions-reference/nbalt.html
mean(dist) ≈ μ && var(dist) ≈ μ + μ^2 / ϕ
#+end_src

#+RESULTS:
: true

#+REVEAL: split

Can be considered a generalization of =Poisson=

#+begin_src julia :eval no
μ = 2.0
anim = @animate for ϕ ∈ [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 25.0, 100.0]
    p = plot(size=(500, 300))
    plot!(p, Poisson(μ); label="Poisson($μ)")
    plot!(p, NegativeBinomial2(μ, ϕ), label="NegativeBinomial2($μ, $ϕ)")
    xlims!(0, 20); ylims!(0, 0.35);
    p
end
gif(anim, outputdir("negative_binomial.gif"), fps=2);
#+end_src

#+RESULTS:
: [ Info: Saved animation to /drive-2/Projects/public/Turing-Workshop/2023-MRC-BSU-and-UKHSA/TorsWorkshop/assets/outputs/more-julia/negative_binomial.gif

[[./assets/outputs/more-julia/negative_binomial.gif]]

#+REVEAL: split

And here's the full model

#+begin_src julia
@model function sir_model(
    num_days;                                  # Number of days to model
    tspan = (0.0, float(num_days)),            # Timespan to model
    u0 = [N - 1, 1, 0.0],                      # Initial state
    p0 = [2.0, 0.6],                           # Placeholder parameters
    problem = ODEProblem(SIR!, u0, tspan, p0)  # Create problem once so we can `remake`.
)
    β ~ truncated(Normal(2, 1); lower=0)
    γ ~ truncated(Normal(0.4, 0.5); lower=0)
    ϕ⁻¹ ~ Exponential(1/5)
    ϕ = inv(ϕ⁻¹)

    problem_new = remake(problem, p=[β, γ])  # Replace parameters `p`.
    sol = solve(problem_new, saveat=1)       # Solve!

    sol_for_observed = sol[2, 2:num_days + 1]  # Timesteps we have observations for.
    in_bed = Vector{Int}(undef, num_days)
    for i = 1:length(sol_for_observed)
        # Add a small constant to `sol_for_observed` to make things more stable.
        in_bed[i] ~ NegativeBinomial2(sol_for_observed[i] + 1e-5, ϕ)
    end

    # Some quantities we might be interested in.
    return (R0 = β / γ, recovery_time = 1 / γ, infected = sol_for_observed)
end
#+end_src

#+RESULTS:
: sir_model (generic function with 2 methods)

Let's break it down

#+REVEAL: split

#+begin_src julia :eval no :tangle no
@model function sir_model(...)
#+end_src

This is the =@model= macro from =Turing=

It takes in a standard Julia function and from this defines two things:
1. A method for constructing the =Turing.Model=
2. A method for executing the model.

#+REVEAL: split

#+begin_src julia 
function sir_model(
    num_days;                                  # Number of days to model
    tspan = (0.0, float(num_days)),            # Timespan to model
    u0 = [N - 1, 1, 0.0],                      # Initial state
    p0 = [2.0, 0.6],                           # Placeholder parameters
    problem = ODEProblem(SIR!, u0, tspan, p0)  # Create problem once so we can `remake`.
)
    ...
end
#+end_src

#+REVEAL: split

#+begin_src julia :eval no :tangle no
β ~ truncated(Normal(2, 1); lower=0)
γ ~ truncated(Normal(0.4, 0.5); lower=0)
ϕ⁻¹ ~ Exponential(1/5)
ϕ = inv(ϕ⁻¹)
#+end_src

defines our prior

=truncated= is just a way of restricting the domain of the distribution you pass it

#+REVEAL: split

#+begin_src julia :eval no :tangle no
problem_new = remake(problem, p=[β, γ])  # Replace parameters `p`.
sol = solve(problem_new, saveat=1)       # Solve!
#+end_src

We then remake the problem, now with the parameters =[β, γ]= sampled above

=saveat = 1= gets us the solution at the timesteps =[0, 1, 2, ..., 14]=

#+REVEAL: split

Then we extract the timesteps we have observations for

#+begin_src julia :eval no :tangle no
sol_for_observed = sol[2, 2:num_days + 1]  # Timesteps we have observations for.
#+end_src

and define what's going to be a likelihood (once we add observations)

#+begin_src julia :eval no :tangle no
in_bed = Vector{Int}(undef, num_days)
for i = 1:length(sol_for_observed)
    # Add a small constant to `sol_for_observed` to make things more stable.
    in_bed[i] ~ NegativeBinomial2(sol_for_observed[i] + 1e-5, ϕ)
end
#+end_src

#+REVEAL: split

Finally we return some values that might be of interest to

#+begin_src julia :eval no :tangle no
# Some quantities we might be interested in.
return (R0 = β / γ, recovery_time = 1 / γ, infected = sol_for_observed)
#+end_src

This is useful for a post-sampling diagnostics, debugging, etc.

#+REVEAL: split

#+begin_src julia
model = sir_model(length(data.in_bed))
#+end_src

#+RESULTS:
: Model(
:   args = (:num_days, :tspan, :u0, :p0, :problem)
:   defaults = (:tspan, :u0, :p0, :problem)
:   context = DynamicPPL.DefaultContext()
: )

The model is just another function, so we can call it to check that it works

#+HTML: <div class="fragment (appear)">

#+begin_src julia
model().infected
#+end_src

#+RESULTS:
#+begin_example
14-element Vector{Float64}:
   4.629880098761509
  20.51149899075346
  76.01682025343723
 170.19683994223317
 188.1484617193551
 130.8534765547809
  74.41578155120635
  38.968883878630315
  19.70373448216648
   9.79913321292256
   4.837363010506036
   2.3788515858966703
   1.167609404493511
   0.5726457769454385
#+end_example

Hey, it does!

#+HTML: </div>

** Is the prior reasonable?
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Is-the-prior-reasonable
:CUSTOM_ID: 2023-01-29-16-57-28-Is-the-prior-reasonable
:END:

Before we do any inference, we should check if the prior is reasonable

From domain knowledge we know that (for influenza at least)
#+ATTR_REVEAL: :frag (appear)
- $R_0$ is typically between 1 and 2
- =recovery_time= ($1 / \gamma$) is usually ~1 week

#+HTML: <div class="fragment (appear)">

We want to make sure that your prior belief reflects this knowledge while still being flexible enough to accommodate the observations

#+HTML: </div>

#+REVEAL: split

To check this we'll just simulate some draws from our prior model, i.e. the model /without/ conditioning on =in_bed=

There are two ways to sample form the prior

#+HTML: <div class="fragment (appear)">

#+begin_src julia
# 1. By just calling the `model`, which returns a `NamedTuple` containing the quantities of interest
print(model())
#+end_src

#+RESULTS:
: (R0 = 1.5744220262579935, recovery_time = 1.0914322040400142, infected = [1.686706278885068, 2.8316832197830895, 4.716863573067216, 7.756274929408289, 12.489841589345032, 19.460362335608362, 28.856011572547263, 39.90424926578871, 50.43351515502132, 57.45932083686618, 58.82922439267481, 54.59403067743465, 46.666918039077295, 37.41622088550463])

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

Or by just calling =sample= using =Prior=

#+begin_src julia
# Sample from prior.
chain_prior = sample(model, Prior(), 10_000);
#+end_src

#+RESULTS:
: 
Sampling:  17%|███████                                  |  ETA: 0:00:01
Sampling:  38%|███████████████▋                         |  ETA: 0:00:01
Sampling:  58%|███████████████████████▋                 |  ETA: 0:00:00
Sampling:  72%|█████████████████████████████▊           |  ETA: 0:00:00
Sampling:  87%|███████████████████████████████████▋     |  ETA: 0:00:00
Sampling: 100%|█████████████████████████████████████████| Time: 0:00:00

#+HTML: </div>

#+REVEAL: split

Let's have a look at the prior predictive

#+begin_src julia
p = plot(legend=false, size=(600, 300))
plot_trajectories!(p, group(chain_prior, :in_bed); n = 1000)
hline!([N], color="red")
#+end_src

#+RESULTS:
[[file:assets/outputs/f110b08ba2b09629fd9d3c498a45a8ab21e0915a.png]]

#+ATTR_REVEAL: :frag (appear)
For certain values we get number of infected /larger/ than the actual population

#+ATTR_REVEAL: :frag (appear)
But this is includes the randomness from =NegativeBinomial2= likelihood

#+ATTR_REVEAL: :frag (appear)
Maybe more useful to inspect the (I)nfected state from the ODE solution?

#+REVEAL: split

We can also look at the =generated_quantities=, i.e. the values from the =return= statement in our model

Our =return= looked like this

#+begin_src julia :eval no :tangle no
# Some quantities we might be interested in.
return (R0 = β / γ, recovery_time = 1 / γ, infected = sol_for_observed)
#+end_src

and so =generated_quantities= (conditioned on =chain_prior=) gives us

#+begin_src julia
quantities_prior = generated_quantities(
    model,
    MCMCChains.get_sections(chain_prior, :parameters)
)
print(quantities_prior[1])
#+end_src

#+RESULTS:
: (R0 = 4.605690136988736, recovery_time = 2.233647692485423, infected = [4.984997209753707, 24.029092981830093, 99.82240918848625, 261.07361336182714, 344.54043982014423, 295.98314301752475, 215.79118631762378, 148.29201252405755, 99.44804435344652, 65.92385902526553, 43.43722911865047, 28.520552363047532, 18.69045660211042, 12.232600393221547])

#+REVEAL: split

We can convert it into a =Chains= using a utility function of mine

#+begin_src julia
# Convert to `Chains`.
chain_quantities_prior = to_chains(quantities_prior);

# Plot.
p = plot(legend=false, size=(600, 300))
plot_trajectories!(p, group(chain_quantities_prior, :infected); n = 1000)
hline!([N], color="red")
#+end_src

#+RESULTS:
[[file:assets/outputs/a1b75cd5d378febc7029c1ed63db2831e09ab9ee.png]]

#+HTML: <div class="x-small-text">

*NOTE:* =to_chains= is not part of "official" Turing.jl because the =return= can contain /whatever/ you want, and so it's not always possible to convert into a =Chains=

#+HTML: </div>

#+REVEAL: split

And the quantiles for the trajectories

#+begin_src julia
p = plot(legend=false, size=(600, 300))
plot_trajectory_quantiles!(p, group(chain_quantities_prior, :infected))
hline!(p, [N], color="red")
#+end_src

#+RESULTS:
[[file:assets/outputs/3f7598c6f0df5f376d9f89161504465787bacf25.png]]

#+REVEAL: split


#+begin_src julia :display text/plain
DataFrame(quantile(chain_quantities_prior[:, [:R0, :recovery_time], :]))
#+end_src

#+RESULTS:
: 2×6 DataFrame
:  Row │ parameters     2.5%      25.0%    50.0%    75.0%    97.5%   
:      │ Symbol         Float64   Float64  Float64  Float64  Float64 
: ─────┼─────────────────────────────────────────────────────────────
:    1 │ R0             0.487053  2.09801  3.70619  7.31515  62.3576
:    2 │ recovery_time  0.706025  1.21271  1.88363  3.46962  29.1143

Compare to our prior knowledge of $R_0 \in [1, 2]$ and $(1/\gamma) \approx 1$ for influenza

Do we really need probability mass on $R_0 \ge 10$?

** TASK What's wrong with the current prior?
:PROPERTIES:
:ID:       2023-01-29-16-57-28-What-s-wrong-with-the-current-prior
:CUSTOM_ID: 2023-01-29-16-57-28-What-s-wrong-with-the-current-prior
:END:

#+HTML: <div class="side-by-side">

#+HTML: <div style="margin: auto;">

The SIR model

\begin{equation*}
\begin{split}
  \frac{\mathrm{d} S}{\mathrm{d} t} &= - \beta S \frac{I}{N} \\
  \frac{\mathrm{d} I}{\mathrm{d} t} &= \beta S \frac{I}{N} - \gamma I \\
  \frac{\mathrm{d} R}{\mathrm{d} t} &= \gamma I
\end{split}
\end{equation*}

#+HTML: </div>

#+HTML: <div>

And here's the current priors

#+HTML: <div class="x-small-text">

#+begin_src julia 
plot(
    plot(truncated(Normal(2, 1); lower=0), label=nothing, title="β"),
    plot(truncated(Normal(0.4, 0.5); lower=0), label=nothing, title="γ"),
    plot(Exponential(1/5), label=nothing, title="ϕ⁻¹"),
    layout=(3, 1)
)
#+end_src

#+RESULTS:
[[file:assets/outputs/80c76ad5d3151ec0f3e044a075b2a5b88956d14b.png]]

#+HTML: </div>

#+HTML: </div>

#+HTML: </div>

** SOLUTION Recovery time shouldn't be several years
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Recovery-time-shouldn-t-be-several-years
:CUSTOM_ID: 2023-01-29-16-57-28-Recovery-time-shouldn-t-be-several-years
:END:

We mentioned that =recovery_time=, which is expressed as $1 / \gamma$, is ~1 week

We're clearly putting high probability on regions near 0, i.e. /long/ recovery times

#+begin_src julia
plot(truncated(Normal(0.4, 0.5); lower=0), label=nothing, title="γ", size=(500, 300))
#+end_src

#+RESULTS:
[[file:assets/outputs/ed3058dc552f72dbe5151feb938d103b75251f63.png]]

_Should probably be putting less probability mass near 0_

** SOLUTION ${\color{red} \gamma}$ should not be larger than 1
:PROPERTIES:
:ID:       2023-01-29-16-57-28-color-red-gamma-should-not-be-larger-than-1
:CUSTOM_ID: 2023-01-29-16-57-28-color-red-gamma-should-not-be-larger-than-1
:END:
\begin{equation*}
\begin{split}
  \frac{\mathrm{d} S}{\mathrm{d} t} &= - \beta S \frac{I}{N} \\
  \frac{\mathrm{d} I}{\mathrm{d} t} &= \beta S \frac{I}{N} - {\color{red} \gamma I} \\
  \frac{\mathrm{d} R}{\mathrm{d} t} &= {\color{red} \gamma I}
\end{split}
\end{equation*}

If ${\color{red} \gamma} > 1$ ⟹ (R)ecovered increase by /more/ than the (I)nfected

⟹ _healthy people are recovering_

#+HTML: <div class="small-text" style="color: red;">

[2023-01-27] This was, _rightly so_, contested when I presented this, as the conservation is preserved by the system independent of the value of $\gamma$ (as long as it's positive). There is an argument to be made about having a recovery time of <1 day, i.e. $\gamma > 1$, being unreasonable, but *the reasoning above is incorrect!*

#+HTML: </div>

#+REVEAL: split

#+HTML: <div style="color: red;">

Just to convince ourselves that it is indeed not a problem with $\gamma > 1$, we can just run the model with a large $\gamma$ and look at the =in_bed= under this prior.

#+HTML: </div>

#+HTML: <div class="small-text">

#+begin_src julia 
model_gamma = model | (γ = 1.5, )
chain_gamma = sample(model_gamma, Prior(), 10_000, progress=false)

p1 = plot_trajectories!(plot(legend=false), group(chain_gamma, :in_bed))
hline!(p1, [N], color="red")

p2 = plot_trajectory_quantiles!(plot(legend=false), group(chain_gamma, :in_bed))
hline!(p2, [N], color="red")

plot(p1, p2, layout=(2,1), size=(600, 400))
#+end_src

#+RESULTS:
[[file:assets/outputs/856d2889f03822cd9dffef93e27f71c0c4c4c645.png]]

#+HTML: </div>

#+REVEAL: split

Maybe something like

#+begin_src julia
plot(Beta(2, 5), label="new", size=(500, 300))
plot!(truncated(Normal(0.4, 0.5); lower=0), label="old", color="red")
#+end_src

#+RESULTS:
[[file:assets/outputs/bc9d6892a46d578ee1b80fb8d98ab74a44e0e9ce.png]]

- [X] Bounded at 1 to exclude recovery time of less than 1 day
- [X] Allows smaller values (i.e. longer recovery time) but rapidly decreases near zero

** SOLUTION What if ${\color{red} \beta} > N$?
:PROPERTIES:
:ID:       2023-01-29-16-57-28-What-if-color-red-beta-N
:CUSTOM_ID: 2023-01-29-16-57-28-What-if-color-red-beta-N
:END:
Then for $t = 0$ we have
\begin{equation*}
\frac{\mathrm{d} S}{\mathrm{d} t} \bigg|_{t = 0} = - {\color{red} \beta} S \frac{I}{N} > - N (N - 1) \frac{1}{N} = - (N - 1)
\end{equation*}

i.e. we /immediately/ infect everyone on the very first time-step

Also doesn't seem very realistic

#+REVEAL: split

/But/ under our current prior does this matter?

#+begin_src julia
# ℙ(β > N) = 1 - ℙ(β ≤ N)
1 - cdf(truncated(Normal(2, 1); lower=0), N)
#+end_src

#+RESULTS:
: 0.0

Better yet

#+begin_src julia
quantile(truncated(Normal(2, 1); lower=0), 0.95)
#+end_src

#+RESULTS:
: 3.6559843567138275

i.e. 95% of the probability mass falls below ~3.65

⟹ _Current prior for $\beta$ seems fine (✓)_

#+REVEAL: split

Before we change the prior, let's also make it a bit easier to change the prior using =@submodel=

#+HTML: <div class="fragment (appear)">

=@submodel= allows you call models within models, e.g.

#+begin_src julia
@model function A()
    x_hidden_from_B ~ Normal()
    x = x_hidden_from_B + 100
    return x
end

@model function B()
    @submodel x = A()
    y ~ Normal(x, 1)

    return (; x, y)
end
#+end_src

#+RESULTS:
: B (generic function with 2 methods)

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

#+begin_src julia
# So if we call `B` we only see `x` and `y`
println(B()())
#+end_src

#+RESULTS:
: (x = 101.54969461293537, y = 102.48444715535948)

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

#+begin_src julia
# While if we sample from `B` we get the latent variables
println(rand(B()))
#+end_src

#+RESULTS:
: (x_hidden_from_B = 0.18899969500711336, y = 99.3704062023967)

#+HTML: </div>

#+REVEAL: split

To avoid clashes of variable-names, we can specify a =prefix=

#+begin_src julia
@model A() = (x ~ Normal(); return x + 100)

@model function B()
    # Given it a prefix to use for the variables in `A`.
    @submodel prefix=:inner x_inner = A()
    x ~ Normal(x_inner, 1)

    return (; x_inner, x)
end
#+end_src

#+RESULTS:
: B (generic function with 2 methods)

#+begin_src julia
print(rand(B()))
#+end_src

#+RESULTS:
: (var"inner.x" = 0.45580772395367214, x = 100.5643900955249)

#+REVEAL: split

=@submodel= is useful as it allows you to:
1. Easy to swap out certain parts of your model.
2. Can re-use models across projects and packages.

When working on larger projects, this really shines

#+REVEAL: split

Equipped with =@submodel= we can replace

#+begin_src julia :eval no :tangle no
β ~ truncated(Normal(2, 1); lower=0)
γ ~ truncated(Normal(0.4, 0.5); lower=0)
#+end_src

with

#+begin_src julia :eval no :tangle no
@submodel p = prior(problem_wrapper)
#+end_src

#+HTML: <div class="fragment (appear)">

where =prior= can be something like

#+begin_src julia
@model function prior_original(problem_wrapper::SIRProblem)
    β ~ truncated(Normal(2, 1); lower=0)
    γ ~ truncated(Normal(0.4, 0.5); lower=0)

    return [β, γ]
end

@model function prior_improved(problem_wrapper::SIRProblem)
    # NOTE: Should probably also lower mean for `β` since
    # more probability mass on small `γ` ⟹ `R0 =  β / γ` grows.
    β ~ truncated(Normal(1, 1); lower=0)
    # NOTE: New prior for `γ`.
    γ ~ Beta(2, 5)

    return [β, γ]
end
#+end_src

#+RESULTS:
: prior_improved (generic function with 2 methods)

#+HTML: </div>

#+REVEAL: split

#+begin_src julia
@model function epidemic_model(
    problem_wrapper::AbstractEpidemicProblem,
    prior  # NOTE: now we just pass the prior as an argument
)
    # NOTE: And use `@submodel` to embed the `prior` in our model.
    @submodel p = prior(problem_wrapper)

    ϕ⁻¹ ~ Exponential(1/5)
    ϕ = inv(ϕ⁻¹)

    problem_new = remake(problem_wrapper.problem, p=p)  # Replace parameters `p`.
    sol = solve(problem_new, saveat=1)                  # Solve!

    # Extract the `infected`.
    sol_for_observed = infected(problem_wrapper, sol)[2:end]

    # NOTE: `arraydist` is faster for larger dimensional problems,
    # and it does not require explicit allocation of the vector.
    in_bed ~ arraydist(NegativeBinomial2.(sol_for_observed .+ 1e-5, ϕ))

    β, γ = p[1:2]
    return (R0 = β / γ, recovery_time = 1 / γ, infected = sol_for_observed)
end
#+end_src

#+RESULTS:
: epidemic_model (generic function with 3 methods)

#+REVEAL: split

#+HTML: <div class="x-small-text">

Another neat trick is to return early if integration fail

#+HTML: </div>

#+begin_src julia
@model function epidemic_model(
    problem_wrapper::AbstractEpidemicProblem,
    prior  # now we just pass the prior as an argument
)
    # And use `@submodel` to embed the `prior` in our model.
    @submodel p = prior(problem_wrapper)

    ϕ⁻¹ ~ Exponential(1/5)
    ϕ = inv(ϕ⁻¹)

    problem_new = remake(problem_wrapper.problem, p=p)  # Replace parameters `p`.
    sol = solve(problem_new, saveat=1)                  # Solve!

    # NOTE: Return early if integration failed.
    if !issuccess(sol)
        Turing.@addlogprob! -Inf  # NOTE: Causes automatic rejection.
        return nothing
    end

    # Extract the `infected`.
    sol_for_observed = infected(problem_wrapper, sol)[2:end]

    # `arraydist` is faster for larger dimensional problems,
    # and it does not require explicit allocation of the vector.
    in_bed ~ arraydist(NegativeBinomial2.(sol_for_observed .+ 1e-5, ϕ))

    β, γ = p[1:2]
    return (R0 = β / γ, recovery_time = 1 / γ, infected = sol_for_observed)
end
#+end_src

#+RESULTS:
: epidemic_model (generic function with 3 methods)

#+REVEAL: split

Equipped with this we can now easily construct /two/ models using different priors

#+begin_src julia
sir = SIRProblem(N);
model_original = epidemic_model(sir, prior_original);
model_improved = epidemic_model(sir, prior_improved);
#+end_src

#+RESULTS:

but using the same underlying =epidemic_model=

#+begin_src julia
chain_prior_original = sample(model_original, Prior(), 10_000; progress=false);
chain_prior_improved = sample(model_improved, Prior(), 10_000; progress=false);
#+end_src

#+RESULTS:

Let's compare the resulting priors over some of the quantities of interest

#+REVEAL: split

Let's compare the =generated_quantities=, e.g. $R_0$

#+HTML: <div class="small-text">

#+begin_src julia
chain_quantities_original = to_chains(
    generated_quantities(
        model_original,
        MCMCChains.get_sections(chain_prior_original, :parameters)
    );
);

chain_quantities_improved = to_chains(
    generated_quantities(
        model_improved,
        MCMCChains.get_sections(chain_prior_improved, :parameters)
    );
);
#+end_src

#+RESULTS:

#+begin_src julia
p = plot(; legend=false, size=(500, 200))
plot_trajectories!(p, group(chain_quantities_original, :infected); n = 100, trajectory_color="red")
plot_trajectories!(p, group(chain_quantities_improved, :infected); n = 100, trajectory_color="blue")
hline!([N], color="red", linestyle=:dash)
#+end_src

#+RESULTS:
[[file:assets/outputs/2e1579a71c4072534cabe41a233cd283543874e6.png]]

#+HTML: </div>

#+REVEAL: split

#+HTML: <div class="small-text">

#+begin_src julia
plt1 = plot(legend=false)
plot_trajectory_quantiles!(plt1, group(chain_quantities_original, :infected))
hline!(plt1, [N], color="red", linestyle=:dash)

plt2 = plot(legend=false)
plot_trajectory_quantiles!(plt2, group(chain_quantities_improved, :infected))
hline!(plt2, [N], color="red", linestyle=:dash)

plot(plt1, plt2, layout=(2, 1))
#+end_src

#+RESULTS:
[[file:assets/outputs/cffea7a80541e6f61fd182de85cb18bc8520b0b4.png]]

#+HTML: </div>

This makes sense: if half of the population is immediately infected ⟹ number of infected tapers wrt. time as they recover

#+REVEAL: split

For =model_improved= we then have

#+begin_src julia :display text/plain
DataFrame(quantile(chain_quantities_improved[:, [:R0, :recovery_time], :]))
#+end_src

#+RESULTS:
: 2×6 DataFrame
:  Row │ parameters     2.5%      25.0%    50.0%    75.0%    97.5%   
:      │ Symbol         Float64   Float64  Float64  Float64  Float64 
: ─────┼─────────────────────────────────────────────────────────────
:    1 │ R0             0.295478  2.30191  4.52993  8.41333  35.9603
:    2 │ recovery_time  1.55995   2.55853  3.81049  6.31316  23.6136

Compare to =model_original=

#+begin_src julia :display text/plain
DataFrame(quantile(chain_quantities_original[:, [:R0, :recovery_time], :]))
#+end_src

#+RESULTS:
: 2×6 DataFrame
:  Row │ parameters     2.5%      25.0%    50.0%    75.0%    97.5%   
:      │ Symbol         Float64   Float64  Float64  Float64  Float64 
: ─────┼─────────────────────────────────────────────────────────────
:    1 │ R0             0.495283  2.10977  3.7219   7.30474  59.9345
:    2 │ recovery_time  0.693234  1.20832  1.88465  3.48181  29.4579

** TASK Make =epidemic_model= work for =SEIRProblem=
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Make-epidemic-model-work-for-SEIRProblem
:CUSTOM_ID: 2023-01-29-16-57-28-Make-epidemic-model-work-for-SEIRProblem
:END:
1. [ ] Implement a prior which also includes $\sigma$ and execute
   =epidemic_model= with it
2. [ ] Can we make a better prior for $\sigma$? Do we even need one?

#+begin_src julia :eval no
@model function prior_original(problem_wrapper::SEIRProblem)
    # TODO: Implement
end
#+end_src

#+begin_src julia :exports (by-backend (reveal "none") (t "code"))
# Some space so you don't cheat.



















# Are you sure?
#+end_src

#+RESULTS:


** SOLUTION
:PROPERTIES:
:ID:       2023-01-29-16-57-28-
:CUSTOM_ID: 2023-01-29-16-57-28-
:END:
#+begin_src julia
@model function prior_original(problem_wrapper::SEIRProblem)
    β ~ truncated(Normal(2, 1); lower=0)
    γ ~ truncated(Normal(0.4, 0.5); lower=0)
    σ ~ truncated(Normal(0.8, 0.5); lower=0)

    return [β, γ, σ]
end
#+end_src

#+RESULTS:
: prior_original (generic function with 4 methods)

#+begin_src julia
model_seir = epidemic_model(SEIRProblem(N), prior_original)
print(model_seir())
#+end_src

#+RESULTS:
: (R0 = 1.249905922185047, recovery_time = 1.1275375476185503, infected = [0.41458760197101757, 0.175982147386383, 0.07874256170480576, 0.03911764946193381, 0.02298161307777028, 0.01641950959805482, 0.013759122196211936, 0.012689081710378131, 0.012267493170560804, 0.012110652999146742, 0.012061533724674748, 0.012055660061838289, 0.01206858103422115, 0.012088341860835633])

** WARNING Consult with domain experts
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Consult-with-domain-experts
:CUSTOM_ID: 2023-01-29-16-57-28-Consult-with-domain-experts
:END:
This guy should _not_ be the one setting your priors!

#+ATTR_HTML: :height 400px
#+ATTR_ORG: :width 600
[[file:assets/attachments/2023-01-18_14-49-24_471337_3317365246956_1262712540_o.jpg]]

Get an actual scientist to do that...

** Condition
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Condition
:CUSTOM_ID: 2023-01-29-16-57-28-Condition
:END:
Now let's actually involve the data

#+HTML: <div class="fragment (appear)">

We can condition a =Model= as so

#+begin_src julia
# Condition on the observations.
model = epidemic_model(SIRProblem(N), prior_improved)
model_conditioned = model | (in_bed = data.in_bed,)
#+end_src

#+RESULTS:
: Model(
:   args = (:problem_wrapper, :prior)
:   defaults = ()
:   context = ConditionContext((in_bed = [3, 8, 26, 76, 225, 298, 258, 233, 189, 128, 68, 29, 14, 4],), DynamicPPL.DefaultContext())
: )

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

You know what time it is: /inference time/!

#+HTML: </div>


** Metropolis-Hastings (MH)
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Metropolis-Hastings--MH
:CUSTOM_ID: 2023-01-29-16-57-28-Metropolis-Hastings--MH
:END:

#+begin_src julia
chain_mh = sample(model_conditioned, MH(), MCMCThreads(), 10_000, 4; discard_initial=5_000);
#+end_src

#+RESULTS:

Rhat is /okay-ish/ but not great, and ESS is pretty low innit?

#+REVEAL: split

#+begin_src julia
plot(chain_mh; size=(800, 500))
#+end_src

#+RESULTS:
[[file:assets/outputs/7763ba1e7b6bb223fde4d453cab14cbcf723b75a.png]]

Eeehh doesn't look the greatest

#+REVEAL: split

Difficult to trust these results, but let's check if it at least did /something/ useful

#+begin_src julia
# We're using the unconditioned model!
predictions_mh = predict(model, chain_mh)
#+end_src

#+RESULTS:
#+begin_example
Chains MCMC chain (10000×14×4 Array{Float64, 3}):

Iterations        = 1:1:10000
Number of chains  = 4
Samples per chain = 10000
parameters        = in_bed[1], in_bed[2], in_bed[3], in_bed[4], in_bed[5], in_bed[6], in_bed[7], in_bed[8], in_bed[9], in_bed[10], in_bed[11], in_bed[12], in_bed[13], in_bed[14]
internals         = 

Summary Statistics
  parameters       mean       std   naive_se      mcse          ess      rhat 
      Symbol    Float64   Float64    Float64   Float64      Float64   Float64 

   in_bed[1]     3.3239    2.1843     0.0109    0.0157   30552.3306    1.0007
   in_bed[2]    10.9294    5.2800     0.0264    0.0765    3416.2516    1.0035
   in_bed[3]    34.2420   15.1777     0.0759    0.3451    1224.2460    1.0094
   in_bed[4]    93.9719   40.8019     0.2040    1.0230     986.9341    1.0117
   in_bed[5]   189.9567   76.0954     0.3805    1.6855    1228.9914    1.0092
   in_bed[6]   251.4305   93.8553     0.4693    1.3148    3319.4564    1.0035
   in_bed[7]   238.4202   87.8089     0.4390    1.0119    5807.9491    1.0019
   in_bed[8]   187.8321   70.0577     0.3503    1.0287    3162.2209    1.0030
   in_bed[9]   133.3270   50.5663     0.2528    0.8771    2085.0535    1.0031
  in_bed[10]    90.4691   35.6730     0.1784    0.6878    1522.7552    1.0053
  in_bed[11]    60.0969   24.7118     0.1236    0.5338    1282.4864    1.0065
  in_bed[12]    39.1181   16.8669     0.0843    0.3941    1036.7038    1.0085
  in_bed[13]    25.4850   11.6236     0.0581    0.2900     899.2603    1.0112
  in_bed[14]    16.4625    8.0545     0.0403    0.2026     874.3031    1.0101

Quantiles
  parameters       2.5%      25.0%      50.0%      75.0%      97.5% 
      Symbol    Float64    Float64    Float64    Float64    Float64 

   in_bed[1]     0.0000     2.0000     3.0000     5.0000     8.0000
   in_bed[2]     3.0000     7.0000    10.0000    14.0000    23.0000
   in_bed[3]    12.0000    24.0000    32.0000    42.0000    70.0000
   in_bed[4]    35.0000    67.0000    87.0000   113.0000   192.0000
   in_bed[5]    76.0000   138.0000   179.0000   228.0000   373.0000
   in_bed[6]   102.0000   188.0000   240.0000   301.0000   469.0000
   in_bed[7]    97.0000   178.0000   228.0000   287.0000   439.0000
   in_bed[8]    75.0000   140.0000   180.0000   226.0000   350.0000
   in_bed[9]    52.0000    98.0000   128.0000   161.0000   250.0000
  in_bed[10]    34.0000    66.0000    86.0000   110.0000   172.0000
  in_bed[11]    21.0000    43.0000    57.0000    73.0000   117.0000
  in_bed[12]    13.0000    28.0000    37.0000    48.0000    78.0000
  in_bed[13]     8.0000    17.0000    24.0000    32.0000    53.0000
  in_bed[14]     4.0000    11.0000    15.0000    21.0000    35.0000
#+end_example

#+REVEAL: split

#+begin_src julia
plot_trajectories!(plot(legend=false, size=(600, 300)), predictions_mh; data=data)
#+end_src

#+RESULTS:
[[file:assets/outputs/53d5bcefafe0e9c29e4ee0f560bc2648d8b43118.png]]

#+begin_src julia
plot_trajectory_quantiles!(plot(legend=false, size=(600, 300)), predictions_mh; data=data)
#+end_src

#+RESULTS:
[[file:assets/outputs/0d84524cc352ba3471bf66156dda0a8cfd1e66b9.png]]

Okay, it's not /completely/ useless, but my trust-issues are still present.

Metropolis-Hastings have disappointed me one too many times before.

** So instead, let's go =NUTS=
:PROPERTIES:
:ID:       2023-01-29-16-57-28-So-instead-let-s-go-NUTS
:CUSTOM_ID: 2023-01-29-16-57-28-So-instead-let-s-go-NUTS
:END:
That's right, we're reaching to the *No U-Turn sampler (NUTS)*

*** 
:PROPERTIES:
:reveal_background_iframe: file:///home/tor/Projects/public/mcmc-demo/app.html?closeControls=true&algorithm=HamiltonianMH&target=standard&seed=1&autoplay=true&histBins=100
:ID:       2023-01-29-16-57-28-
:CUSTOM_ID: 2023-01-29-16-57-28-
:END:

#+ATTR_REVEAL: :frag (appear)
[[https://chi-feng.github.io/mcmc-demo/app.html][https://chi-feng.github.io/mcmc-demo/app.html]]

** 
:PROPERTIES:
:ID:       2023-01-29-16-57-28-
:CUSTOM_ID: 2023-01-29-16-57-28-
:END:

#+begin_quote
Wooaah there! =NUTS= requires gradient information!

How are you going to get that through that =solve=?
#+end_quote

Good question, voice in my head

#+ATTR_REVEAL: :frag (appear)
I'm obviously not going to it myself

** Automatic differentiation (AD) in Julia
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Automatic-differentiation--AD--in-Julia
:CUSTOM_ID: 2023-01-29-16-57-28-Automatic-differentiation--AD--in-Julia
:END:
- [[https://github.com/JuliaDiff/ForwardDiff.jl][ForwardDiff.jl]]: forward-mode AD /(default in Turing.jl)/
- [[https://github.com/JuliaDiff/ReverseDiff.jl][ReverseDiff.jl]]: tape-based reverse-mode AD
- [[https://github.com/FluxML/Zygote.jl][Zygote.jl]]: source-to-source reverse-mode AD
- And more...

#+HTML: <div class="fragment (appear)">

Up-and-coming

- [[https://github.com/EnzymeAD/Enzyme.jl][Enzyme.jl]]: Julia bindings for [[https://github.com/EnzymeAD/Enzyme.jl][Enzyme]] which ADs LLVM (low-level)
- [[https://github.com/JuliaDiff/Diffractor.jl][Diffractor.jl]]: experimental mixed-mode AD meant to replace Zygote.jl

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

Of importance
- [[https://github.com/JuliaDiff/ChainRulesCore.jl][ChainRulesCore.jl]]: light-weight package for defining rules, compatible with many of the above

#+HTML: </div>

#+REVEAL: split

*Important*

#+begin_quote
When you write code, you don't have to make a choice which one you
want to use!
#+end_quote

All the (stable) ones, will (mostly) work

/But/ how you write code will affect performance characteristics

Takes a bit of know-how + a bit of digging to go properly "vroom!"

** Differentiating through =solve=
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Differentiating-through-solve
:CUSTOM_ID: 2023-01-29-16-57-28-Differentiating-through-solve
:END:
With that being said, differentiating through numerical =solve= is not necessarily trivial to do efficiently

There are numerous ways of approaching this problem

#+ATTR_HTML: :width 400px
#+ATTR_ORG: :width 400
[[file:assets/attachments/2023-01-22_12-30-07_Screenshot_20230122_122936.png]]

[[https://arxiv.org/abs/1812.01892][https://arxiv.org/abs/1812.01892]] is /great/ resource

#+HTML: <div class="fragment (appear)">

But this is why we have [[https://github.com/SciML/SciMLSensitivity.jl][=SciMLSensitivity.jl=]]

[[https://docs.sciml.ai/SciMLSensitivity/stable/manual/differential_equation_sensitivities/#Choosing-a-Sensitivity-Algorithm][SciMLSensitivity.jl docs]] also provides a great overview of different approaches

#+HTML: </div>

#+REVEAL: split

#+begin_src julia
using SciMLSensitivity
#+end_src

#+RESULTS:

It offers

1. /Discrete sensitivity analysis/ or the /"Direct" method/: just use
   =ForwardDiff.Dual= in the =solve=.
2. /Continuous local sensitivity analysis (CSA)/: extends the original
   system such that the =solve= gives you both the solution and the the
   gradient simultaenously.
3. /Adjoint methods/: construct a backwards system whose solution gives
   us the gradient.

Just do =solve(problem, solver, sensealg = ...)=

** Back to being =NUTS=
   :PROPERTIES:
   :CUSTOM_ID: back-to-being-nuts
   :ID:       2023-01-29-16-57-28-Back-to-being-NUTS
   :END:

#+begin_src julia
chain = sample(model_conditioned, NUTS(0.8), MCMCThreads(), 1000, 4);
#+end_src

#+RESULTS:
#+begin_example
┌ Info: Found initial step size
└   ϵ = 0.8
┌ Info: Found initial step size
└   ϵ = 0.4
┌ Info: Found initial step size
└   ϵ = 0.025
┌ Info: Found initial step size
└   ϵ = 0.05
┌ Warning: The current proposal will be rejected due to numerical error(s).
│   isfinite.((θ, r, ℓπ, ℓκ)) = (true, false, false, false)
└ @ AdvancedHMC ~/.julia/packages/AdvancedHMC/4fByY/src/hamiltonian.jl:49
#+end_example

#+REVEAL: split

#+begin_src julia
chain
#+end_src

#+RESULTS:
#+begin_example
Chains MCMC chain (1000×15×4 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 4
Samples per chain = 1000
Wall duration     = 8.48 seconds
Compute duration  = 33.22 seconds
parameters        = β, γ, ϕ⁻¹
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat    ⋯
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64    ⋯

           β    1.7305    0.0532     0.0008    0.0012   2305.3519    1.0006    ⋯
           γ    0.5305    0.0430     0.0007    0.0009   2432.0514    0.9998    ⋯
         ϕ⁻¹    0.1349    0.0727     0.0011    0.0015   2211.5812    0.9994    ⋯
                                                                1 column omitted

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           β    1.6279    1.6970    1.7281    1.7618    1.8401
           γ    0.4464    0.5024    0.5294    0.5580    0.6172
         ϕ⁻¹    0.0394    0.0843    0.1180    0.1699    0.3165
#+end_example

Muuuch better! Both ESS and Rhat is looking good

#+REVEAL: split

#+begin_src julia
plot(chain; size=(800, 500))
#+end_src

#+RESULTS:
[[file:assets/outputs/ba6a57881c744560992e6aeb0ff47cfb9aaf9238.png]]

#+REVEAL: split

#+begin_src julia
# Predict using the results from NUTS.
predictions = predict(model, chain)
#+end_src

#+RESULTS:
#+begin_example
Chains MCMC chain (1000×14×4 Array{Float64, 3}):

Iterations        = 1:1:1000
Number of chains  = 4
Samples per chain = 1000
parameters        = in_bed[1], in_bed[2], in_bed[3], in_bed[4], in_bed[5], in_bed[6], in_bed[7], in_bed[8], in_bed[9], in_bed[10], in_bed[11], in_bed[12], in_bed[13], in_bed[14]
internals         = 

Summary Statistics
  parameters       mean       std   naive_se      mcse         ess      rhat 
      Symbol    Float64   Float64    Float64   Float64     Float64   Float64 

   in_bed[1]     3.2840    2.1793     0.0345    0.0327   4055.3023    0.9998
   in_bed[2]    10.7950    5.4560     0.0863    0.0749   3714.2037    1.0005
   in_bed[3]    34.0883   15.7327     0.2488    0.2596   3642.2139    1.0001
   in_bed[4]    93.0002   41.3318     0.6535    0.7048   3577.5458    1.0012
   in_bed[5]   187.5652   79.0050     1.2492    1.2543   3478.9204    1.0010
   in_bed[6]   248.9330   97.2779     1.5381    1.6339   4056.1039    1.0008
   in_bed[7]   234.6240   89.1151     1.4090    1.2074   4022.8396    0.9996
   in_bed[8]   185.4033   71.8807     1.1365    1.0479   3860.3337    0.9997
   in_bed[9]   130.9750   50.4204     0.7972    0.8315   3694.8750    0.9999
  in_bed[10]    88.2115   35.8989     0.5676    0.6532   3401.2244    0.9999
  in_bed[11]    59.4943   25.4118     0.4018    0.4155   3460.7875    0.9997
  in_bed[12]    38.6793   16.7184     0.2643    0.2929   3697.7714    1.0005
  in_bed[13]    24.8678   11.4870     0.1816    0.2081   3578.3080    1.0001
  in_bed[14]    15.9450    8.2237     0.1300    0.1376   3389.6307    0.9996

Quantiles
  parameters      2.5%      25.0%      50.0%      75.0%      97.5% 
      Symbol   Float64    Float64    Float64    Float64    Float64 

   in_bed[1]    0.0000     2.0000     3.0000     5.0000     8.0000
   in_bed[2]    2.9750     7.0000    10.0000    14.0000    24.0000
   in_bed[3]   11.0000    24.0000    32.0000    42.0000    73.0000
   in_bed[4]   33.0000    65.0000    87.0000   113.0000   190.0000
   in_bed[5]   69.0000   134.0000   177.0000   226.0000   376.0000
   in_bed[6]   95.0000   184.0000   237.0000   299.0000   473.0250
   in_bed[7]   88.0000   175.0000   224.0000   283.0000   440.0250
   in_bed[8]   70.0000   137.0000   176.0000   225.0000   351.0000
   in_bed[9]   49.0000    96.0000   126.0000   159.0000   245.0000
  in_bed[10]   31.9750    64.0000    84.0000   107.0000   170.0000
  in_bed[11]   19.0000    42.0000    57.0000    73.0000   118.0250
  in_bed[12]   13.0000    27.0000    36.0000    48.0000    77.0000
  in_bed[13]    7.0000    17.0000    23.0000    31.0000    51.0000
  in_bed[14]    4.0000    10.0000    15.0000    20.0000    36.0000
#+end_example

#+REVEAL: split

#+begin_src julia
plot_trajectories!(plot(legend=false, size=(600, 300)), predictions; n = 1000, data=data)
#+end_src

#+RESULTS:
[[file:assets/outputs/870b8fafd890396444043fd96c7d303fb7722627.png]]

#+REVEAL: split

#+begin_src julia
plot_trajectory_quantiles!(plot(legend=false, size=(600, 300)), predictions; data=data)
#+end_src

#+RESULTS:
[[file:assets/outputs/145ba1245ad3196f9bb3001254df5b3cf451e778.png]]

** Simulation-based calibration (SBC) [[https://arxiv.org/abs/1804.06788][Talts et. al. (2018)]]
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Simulation-based-calibration--SBC--https-arxiv-dot-org-abs-1804-dot-06788-Talts-et-dot-al-dot--2018
:CUSTOM_ID: 2023-01-29-16-57-28-Simulation-based-calibration--SBC--https-arxiv-dot-org-abs-1804-dot-06788-Talts-et-dot-al-dot--2018
:END:
1. Sample from prior $\theta_1, \dots, \theta_n \sim p(\theta)$.
2. Sample datasets $\mathcal{D}_i \sim p(\cdot \mid \theta_i)$ for $i = 1, \dots, n$.
3. Obtain (approximate) $p(\theta \mid \mathcal{D}_i)$ for $i = 1, \dots, n$.

For large enough (n), the "combination" of the posteriors should recover the prior!

"Combination" here usually means computing some statistic and comparing against what it should be

#+ATTR_HTML: :width 800px
#+ATTR_ORG: :width 400
[[file:assets/attachments/2023-01-22_12-09-24_Screenshot_20230122_120848.png]]

#+REVEAL: split

That's very expensive → in practice we just do this once or twice

#+begin_src julia
# Sample from the conditioned model so we don't get the `in_bed` variables too
using Random  # Just making usre the numbers of somewhat interesting
rng = MersenneTwister(43);
test_values = rand(rng, NamedTuple, model_conditioned)
#+end_src

#+RESULTS:
| β | = | 1.2254566808077714 | γ | = | 0.27594266205681933 | ϕ⁻¹ | = | 0.13984179162984164 |

Now we condition on those values and run once to generate data

#+begin_src julia
model_test = model | test_values
#+end_src

#+RESULTS:
: Model(
:   args = (:problem_wrapper, :prior)
:   defaults = ()
:   context = ConditionContext((β = 1.2254566808077714, γ = 0.27594266205681933, ϕ⁻¹ = 0.13984179162984164), DynamicPPL.DefaultContext())
: )

#+begin_src julia
in_best_test = rand(rng, model_test).in_bed;
#+end_src

#+RESULTS:

#+REVEAL: split

Next, inference!

#+begin_src julia
model_test_conditioned = model | (in_bed = in_best_test,)
#+end_src

#+RESULTS:
: Model(
:   args = (:problem_wrapper, :prior)
:   defaults = ()
:   context = ConditionContext((in_bed = [1, 9, 11, 45, 159, 136, 270, 123, 463, 376, 231, 148, 99, 162],), DynamicPPL.DefaultContext())
: )

#+begin_src julia
# Let's just do a single chain here.
chain_test = sample(model_test_conditioned, NUTS(0.8), 1000);
#+end_src

#+RESULTS:
: ┌ Info: Found initial step size
: └   ϵ = 0.0125
: 
Sampling:   4%|█▊                                       |  ETA: 0:00:05
Sampling:  12%|████▊                                    |  ETA: 0:00:02
Sampling:  25%|██████████▏                              |  ETA: 0:00:01
Sampling:  39%|███████████████▉                         |  ETA: 0:00:01
Sampling:  52%|█████████████████████▍                   |  ETA: 0:00:01
Sampling:  66%|███████████████████████████              |  ETA: 0:00:00
Sampling:  80%|████████████████████████████████▊        |  ETA: 0:00:00
Sampling:  93%|██████████████████████████████████████   |  ETA: 0:00:00
Sampling: 100%|█████████████████████████████████████████| Time: 0:00:01

#+REVEAL: split

Did we recover the parameters?

#+HTML: <div class="small-text">

#+begin_src julia
ps = []
for sym in [:β, :γ, :ϕ⁻¹]
    p = density(chain_test[:, [sym], :])
    vline!([test_values[sym]])
    push!(ps, p)
end
plot(ps..., layout=(3, 1), size=(600, 400))
#+end_src

#+RESULTS:
[[file:assets/outputs/be1c12fbb76e6f039cc149b75842d8ee461b8f89.png]]

#+HTML: </div>

Yay!

** Samplers in Turing.jl
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Samplers-in-Turing-dot-jl
:CUSTOM_ID: 2023-01-29-16-57-28-Samplers-in-Turing-dot-jl
:END:
- Metropolis-Hastings, MALA, emcee ([[https://github.com/TuringLang/AdvancedMH.jl][AdvancedMH.jl]])
- Hamiltonian Monte Carlo, NUTS ([[https://github.com/TuringLang/AdvancedMH.jl][AdvancedHMC.jl]])
- SMC ([[https://github.com/TuringLang/AdvancedPS.jl][AdvancedPS.jl]])
- Elliptical Slice Sampling ([[https://github.com/TuringLang/EllipticalSliceSampling.jl][EllipticalSliceSampling.jl]])
- Nested sampling ([[https://github.com/TuringLang/NestedSamplers.jl][NestedSamplers.jl]])
- (Experimental) Tempered sampling ([[https://github.com/Julia-Tempering/Pigeons.jl][Pigeons.jl]] and [[https://github.com/TuringLang/MCMCTempering.jl][MCMCTempering.jl]])

#+REVEAL: split

You can also combine some of these in Turing.jl

#+HTML: <div class="small-text">

#+begin_src julia
using LinearAlgebra: I

@model function linear_regression(X)
    num_params = size(X, 1)
    β ~ MvNormal(ones(num_params))
    σ² ~ InverseGamma(2, 3)
    y ~ MvNormal(vec(β' * X), σ² * I)
end

# Generate some dummy data.
X = randn(2, 1_000); lin_reg = linear_regression(X); true_vals = rand(lin_reg)

# Condition.
lin_reg_conditioned = lin_reg | (y = true_vals.y,);
#+end_src

#+RESULTS:

We can then do =Gibbs= but sampling $β$ using =ESS= and $\sigma^2$ using =HMC=

#+begin_src julia :eval no
chain_ess_hmc = sample(lin_reg_conditioned, Gibbs(ESS(:β), HMC(1e-3, 16, :σ²)), 1_000)
#+end_src

#+HTML: </div>

#+REVEAL: split

#+HTML: <div class="small-text">

#+begin_src julia
chain_ess_hmc = sample(lin_reg_conditioned, Gibbs(ESS(:β), HMC(1e-3, 16, :σ²)), 1_000)
#+end_src

#+RESULTS:
:RESULTS:
: 
Sampling:  10%|████▎                                    |  ETA: 0:00:01
Sampling:  58%|███████████████████████▊                 |  ETA: 0:00:00
Sampling: 100%|█████████████████████████████████████████| Time: 0:00:00
#+begin_example
Chains MCMC chain (1000×4×1 Array{Float64, 3}):

Iterations        = 1:1:1000
Number of chains  = 1
Samples per chain = 1000
Wall duration     = 3.39 seconds
Compute duration  = 3.39 seconds
parameters        = β[1], β[2], σ²
internals         = lp

Summary Statistics
  parameters      mean       std   naive_se      mcse        ess      rhat   e ⋯
      Symbol   Float64   Float64    Float64   Float64    Float64   Float64     ⋯

        β[1]   -1.6886    0.1124     0.0036    0.0075   211.1299    1.0006     ⋯
        β[2]   -0.7129    0.0581     0.0018    0.0024   433.8817    1.0015     ⋯
          σ²    1.9929    0.0893     0.0028    0.0113    48.7541    1.0021     ⋯
                                                                1 column omitted

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

        β[1]   -1.7810   -1.7259   -1.6992   -1.6630   -1.6026
        β[2]   -0.7994   -0.7469   -0.7146   -0.6805   -0.6266
          σ²    1.8287    1.9345    1.9893    2.0446    2.1821
#+end_example
:END:

Could potentially lead to improvements

*NOTE:* Usually /very/ difficult to choose sampler parameters in this case

#+HTML: </div>

#+REVEAL: split

Means one can also mix discrete and continuous

#+HTML: <div class="small-text">

#+begin_src julia 
@model function mixture(n)
    cluster ~ filldist(Categorical([0.25, 0.75]), n)
    μ ~ MvNormal([-10.0, 10.0], I)
    x ~ arraydist(Normal.(μ[cluster], 1))
end

model_mixture = mixture(10)
fake_values_mixture = rand(model_mixture)
model_mixture_conditioned = model_mixture | (x = fake_values_mixture.x, )
chain_discrete = sample(
    model_mixture_conditioned, Gibbs(PG(10, :cluster), HMC(1e-3, 16, :μ)), MCMCThreads(), 1_000, 4
)
#+end_src

#+RESULTS:
:RESULTS:
: 
Sampling (4 threads):  75%|█████████████████████▊       |  ETA: 0:00:00
Sampling (4 threads): 100%|█████████████████████████████| Time: 0:00:00
#+begin_example
Chains MCMC chain (1000×13×4 Array{Float64, 3}):

Iterations        = 1:1:1000
Number of chains  = 4
Samples per chain = 1000
Wall duration     = 9.1 seconds
Compute duration  = 35.2 seconds
parameters        = cluster[1], cluster[2], cluster[3], cluster[4], cluster[5], cluster[6], cluster[7], cluster[8], cluster[9], cluster[10], μ[1], μ[2]
internals         = lp

Summary Statistics
   parameters       mean       std   naive_se      mcse         ess      rhat  ⋯
       Symbol    Float64   Float64    Float64   Float64     Float64   Float64  ⋯

   cluster[1]     1.0245    0.1546     0.0024    0.0135     49.1752    1.0586  ⋯
   cluster[2]     1.0517    0.2215     0.0035    0.0244     24.8047    1.1288  ⋯
   cluster[3]     1.1227    0.3282     0.0052    0.0375     18.5621    1.2743  ⋯
   cluster[4]     1.9995    0.0224     0.0004    0.0004   4002.3197    0.9999  ⋯
   cluster[5]     1.9725    0.1636     0.0026    0.0195     25.6370    1.1311  ⋯
   cluster[6]     1.9835    0.1274     0.0020    0.0139     61.8799    1.0723  ⋯
   cluster[7]     1.2712    0.4447     0.0070    0.0532     10.2500    1.9732  ⋯
   cluster[8]     1.0083    0.0905     0.0014    0.0065    128.4567    1.0179  ⋯
   cluster[9]     1.3710    0.4831     0.0076    0.0588     10.7308    1.8081  ⋯
  cluster[10]     2.0000    0.0000     0.0000    0.0000         NaN       NaN  ⋯
         μ[1]   -10.2644    1.1909     0.0188    0.1496      8.0814    8.8935  ⋯
         μ[2]     9.6664    0.8387     0.0133    0.1037      8.4436    3.7077  ⋯
                                                                1 column omitted

Quantiles
   parameters       2.5%      25.0%      50.0%     75.0%     97.5% 
       Symbol    Float64    Float64    Float64   Float64   Float64 

   cluster[1]     1.0000     1.0000     1.0000    1.0000    1.0000
   cluster[2]     1.0000     1.0000     1.0000    1.0000    2.0000
   cluster[3]     1.0000     1.0000     1.0000    1.0000    2.0000
   cluster[4]     2.0000     2.0000     2.0000    2.0000    2.0000
   cluster[5]     1.0000     2.0000     2.0000    2.0000    2.0000
   cluster[6]     2.0000     2.0000     2.0000    2.0000    2.0000
   cluster[7]     1.0000     1.0000     1.0000    2.0000    2.0000
   cluster[8]     1.0000     1.0000     1.0000    1.0000    1.0000
   cluster[9]     1.0000     1.0000     1.0000    2.0000    2.0000
  cluster[10]     2.0000     2.0000     2.0000    2.0000    2.0000
         μ[1]   -11.8037   -11.3141   -10.4745   -9.3795   -8.2947
         μ[2]     8.4038     8.9560     9.6131   10.4825   11.0633
#+end_example
:END:

#+HTML: </div>

#+REVEAL: split

#+HTML: <div class="x-small-text">

#+begin_src julia 
ps = []
for (i, realizations) in enumerate(eachcol(Array(group(chain_discrete, :cluster))))
    p = density(realizations, legend=false, ticks=false); vline!(p, [fake_values_mixture.cluster[i]])
    push!(ps, p)
end
plot(ps..., layout=(length(ps) ÷ 2, 2), size=(600, 40 * length(ps)))
#+end_src

#+RESULTS:
[[file:assets/outputs/f64dea8a5533f00397817ee6703ff49dcb105c25.png]]

#+HTML: </div>

Again, this is difficult to get to work properly on non-trivial examples

_But_ it is possible

** Other utilities for Turing.jl
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Other-utilities-for-Turing-dot-jl
:CUSTOM_ID: 2023-01-29-16-57-28-Other-utilities-for-Turing-dot-jl
:END:
- [[https://github.com/TuringLang/TuringGLM.jl][TuringGLM.jl]]: GLMs using the formula-syntax from R but using Turing.jl under the hood
- [[https://github.com/TuringLang/TuringBenchmarking.jl][TuringBenchmarking.jl]]: useful for benchmarking Turing.jl models
- [[https://github.com/TuringLang/TuringCallbacks.jl][TuringCallbacks.jl]]: on-the-fly visualizations using =tensorboard=

*** TuringGLM.jl

#+begin_src julia 
using TuringGLM
#+end_src

#+RESULTS:

#+begin_src julia :exports none
using TuringGLM, DataDeps, DataFrames, CSV, StatsPlots
#+end_src

#+RESULTS:

#+REVEAL: split

We'll use the KidIQ dataset for a quick example

#+begin_src julia
register(DataDep(
    "kidiq",
    "Survey of adult American women and their respective children from 2007",
    "https://raw.githubusercontent.com/TuringLang/TuringGLM.jl/bbc9129fc2d1ff7a1026fe2189b6580303d5c9f5/data/kidiq.csv",
))
#+end_src

#+RESULTS:
: DataDep("kidiq", "https://raw.githubusercontent.com/TuringLang/TuringGLM.jl/bbc9129fc2d1ff7a1026fe2189b6580303d5c9f5/data/kidiq.csv", nothing, DataDeps.fetch_default, identity, "Survey of adult American women and their respective children from 2007")

#+begin_src julia :display text/plain
fname = joinpath(datadep"kidiq", "kidiq.csv")
kidiq = DataFrame(CSV.File(fname))
#+end_src

#+RESULTS:
#+begin_example
434×4 DataFrame
 Row │ kid_score  mom_hs  mom_iq    mom_age 
     │ Int64      Int64   Float64   Int64   
─────┼──────────────────────────────────────
   1 │        65       1  121.118        27
   2 │        98       1   89.3619       25
   3 │        85       1  115.443        27
   4 │        83       1   99.4496       25
   5 │       115       1   92.7457       27
   6 │        98       0  107.902        18
   7 │        69       1  138.893        20
   8 │       106       1  125.145        23
   9 │       102       1   81.6195       24
  10 │        95       1   95.0731       19
  11 │        91       1   88.577        23
  ⋮  │     ⋮        ⋮        ⋮         ⋮
 425 │        42       1   78.2446       27
 426 │       102       1  127.676        29
 427 │       104       1  124.515        23
 428 │        59       0   80.464        21
 429 │        93       0   74.8607       25
 430 │        94       0   84.8774       21
 431 │        76       1   92.9904       23
 432 │        50       0   94.8597       24
 433 │        88       1   96.8566       21
 434 │        70       1   91.2533       25
                            413 rows omitted
#+end_example

#+REVEAL: split

Now we can create the formula

#+begin_src julia :display text/plain
fm = @formula(kid_score ~ mom_hs * mom_iq)
#+end_src

#+RESULTS:
: FormulaTerm
: Response:
:   kid_score(unknown)
: Predictors:
:   mom_hs(unknown)
:   mom_iq(unknown)
:   mom_hs(unknown) & mom_iq(unknown)

which can then easily be converted into a Turing.jl-model

#+begin_src julia :display text/plain
model = turing_model(fm, kidiq);
#+end_src

#+RESULTS:

And then we can use our standard Turing.jl workflow:

#+begin_src julia :display text/plain :async yes
chns = sample(model, NUTS(), 1000)
#+end_src

#+RESULTS:
:RESULTS:
: [36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
: [36m[1m└ [22m[39m  ϵ = 0.00078125
: [32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:01[39m
: 
#+begin_example
Chains MCMC chain (1000×17×1 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 1
Samples per chain = 1000
Wall duration     = 1.72 seconds
Compute duration  = 1.72 seconds
parameters        = α, β[1], β[2], β[3], σ
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m ess_bulk [0m [1m ess_tail [0m [1m    rhat [0m [1m e[0m ⋯
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m  [0m ⋯

           α   31.3373    6.0901    0.3698   280.1918   384.7312    1.0005     ⋯
        β[1]    0.6108    2.1583    0.2464   296.4324    83.2314    1.0025     ⋯
        β[2]    0.5123    0.0682    0.0040   310.4550   330.9380    1.0010     ⋯
        β[3]    0.0474    0.0316    0.0026   312.0172   134.9725    0.9998     ⋯
           σ   18.2138    0.6364    0.0264   583.5792   564.1153    1.0013     ⋯
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m   25.0% [0m [1m   50.0% [0m [1m   75.0% [0m [1m   97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m

           α   19.2411   27.3696   31.5349   35.2764   43.1019
        β[1]   -1.9778   -0.4569    0.2894    1.0749    5.5943
        β[2]    0.3839    0.4694    0.5107    0.5555    0.6518
        β[3]   -0.0127    0.0294    0.0493    0.0670    0.1008
           σ   17.0081   17.7909   18.1895   18.6149   19.5446
#+end_example
:END:



*** TODO TuringCallbacks.jl

#+begin_src julia 
using TuringCallbacks
#+end_src

#+RESULTS:

#+begin_src julia 
model = simple_demo(1.5, 2.0);
#+end_src

#+begin_src julia :async yes
logdir = mktempdir()
#+end_src

#+RESULTS:
: "/tmp/jl_yrYT2p"

#+begin_src julia :async yes
callback = TensorBoardCallback(joinpath(logdir, "logs"); include_hyperparams=true)
chain = sample(model, NUTS(), 1000; callback=callback);
#+end_src

#+RESULTS:
: [36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
: [36m[1m└ [22m[39m  ϵ = 0.8500000000000001
: [32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:01[39m
:

#+REVEAL: split

If you have =tensorboard= installed, you can then run

#+begin_src sh :eval no
python3 -m tensorboard.main --logdir /tmp/jl_yrYT2p
#+end_src

#+DOWNLOADED: file:///home/tor/Downloads/tensorboard_demo_histograms_screen.png @ 2023-01-25 20:50:11
#+ATTR_HTML: :width 600px
#+ATTR_ORG: :width 600
[[file:assets/attachments/2023-01-25_20-50-11_tensorboard_demo_histograms_screen.png]]



** Downsides of using Turing.jl
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Downsides-of-using-Turing-dot-jl
:CUSTOM_ID: 2023-01-29-16-57-28-Downsides-of-using-Turing-dot-jl
:END:

#+ATTR_REVEAL: :frag (appear)
- No depedency-extraction of the model ⟹ can't do things like automatic marginalization
  - /But/ it's not impossible; just a matter of development effort
  - Ongoing work in =TuringLang= to make a [[https://www.mrc-bsu.cam.ac.uk/software/bugs/][BUGS]] compatible model "compiler" / parser (in colab with Andrew Thomas & others)
- NUTS performance is at the mercy of AD in Julia
- You _can_ put anything in a model; whether you _should_ is a another matter


* Case study: making a project from scratch

** Set up

First we need to set up our project.

The simplest way to do this is to do

#+begin_src julia
pwd()
#+end_src

#+RESULTS:
: "/drive-2/Projects/public/Turing-Workshop/2023-MRC-BSU-and-UKHSA/TorsWorkshop"

This will generate a folder that looks something like

* Case study: ???

** Covid19: ImperialReport13

** Epimap

* [cite:@kxs052]


#+begin_src julia
using DifferentialEquations, Turing, StatsPlots, LinearAlgebra
#+end_src

#+RESULTS:

#+begin_src julia 
struct SEIR{T1,T2}
    k::T1
    γ::T2
    N::Int
end

function (seir::SEIR)(du, u, p, t)
    S, E, I, R = u
    k, γ, N = seir.k, seir.γ, seir.N
    β = only(p)

    du[1] = -β * S * I / N
    du[2] = β * S * I / N - k * E
    du[3] = k * E - γ * I
    du[4] = γ * I

    return nothing
end
#+end_src

#+RESULTS:

#+begin_src julia
tspan = (0.0, 100.0)
u0 = [999.0, 1, 0, 0]
probf = SEIR(0.25, 0.05, 1000)
prob = ODEProblem(probf, u0, tspan, [0.2])
solve(prob)
#+end_src

#+RESULTS:
#+begin_example
retcode: Success
Interpolation: specialized 4th order "free" interpolation, specialized 2nd order "free" stiffness-aware interpolation
t: 28-element Vector{Float64}:
   0.0
   0.0056540236999814706
   0.062194260699796174
   0.30072981111184066
   0.6950381565675945
   1.2203000465133529
   1.9420478964976093
   2.875425767640001
   4.059411303485391
   5.5012350028501915
   7.231365280264414
   9.310611835029093
  11.822720603708422
   ⋮
  29.994131638154776
  36.38399041740069
  43.15031946908468
  49.925654813444986
  56.64173164949659
  63.21608779006091
  69.68044151998856
  76.04965890277947
  82.24922997649685
  88.32704521258971
  94.43532129748762
 100.0
u: 28-element Vector{Vector{Float64}}:
 [999.0, 1.0, 0.0, 0.0]
 [998.9999992020507, 0.9985882901773383, 0.0014123080849147874, 1.9968700047033795e-7]
 [998.999903990443, 0.9846672028102782, 0.015404780329859823, 2.402641680625783e-5]
 [998.9978071088109, 0.9297127930516831, 0.0719313259662255, 0.0005487721711357369]
 [998.988715035506, 0.8511476794063842, 0.15731320394797277, 0.002824081139604212]
 [998.9668112144358, 0.767041034463036, 0.25784211124172, 0.008305639859413096]
 [998.9208316832985, 0.6827118072407354, 0.3766438333267149, 0.01981267613405121]
 [998.8381486559025, 0.6147899451473634, 0.5065547781759926, 0.04050662077415447]
 [998.7012919960837, 0.5764444949375392, 0.6475005783296947, 0.07476293064911178]
 [998.4923402920977, 0.5785427877223749, 0.8020426609011208, 0.12707425927887211]
 [998.1839760718743, 0.6275609878677972, 0.9841692988996603, 0.20429364135823094]
 [997.7273588313809, 0.7325590501006006, 1.2214003176662998, 0.318681800852206]
 [997.0331285904066, 0.9130060315711355, 1.5611701374154396, 0.49269524060683106]
 ⋮
 [982.0702623929466, 4.978328695298856, 8.678429708473596, 4.272979203280957]
 [967.3040268930682, 8.929117975760578, 15.706379061376614, 8.060476069794523]
 [939.2612602725887, 16.244165918604498, 29.079324826448026, 15.415248982358625]
 [889.935318260028, 28.490110433785002, 52.673150775667786, 28.90142053051913]
 [809.536141204184, 46.627383515369715, 91.26328180674686, 52.57319347369934]
 [693.548729931949, 68.30245900142862, 146.9156269837328, 91.23318408288952]
 [549.4058252982588, 86.32392859844263, 214.79100213149258, 149.47924397180583]
 [400.5060115349084, 91.46932009526371, 279.51869582345205, 228.50597254637566]
 [275.14644582873996, 81.21206493893693, 321.279498493012, 322.36199073931095]
 [184.50529783539324, 62.256830342268195, 330.969490819117, 422.2683810032214]
 [124.27616729855168, 42.68631251399805, 311.97675224228794, 521.0607679451622]
 [89.37978460338655, 28.53709293175161, 278.6199759741896, 603.4631464906721]
#+end_example

#+begin_src julia
using Random: Random

struct EulerMaruyama{F, G}
    μ::F
    σ::G
end

EulerMaruyama() = EulerMaruyama((θ, x) -> 0, (θ, x) -> 1)

(em::EulerMaruyama)(θ, x, δ, m=1) = em(Random.default_rng(), θ, x, δ, m)
function (em::EulerMaruyama)(rng::Random.AbstractRNG, θ, x, δ, m=1)
    # TODO: Do we need to adjust the size in our `randn` call?
    for _ = 1:m
        μ = em.μ(θ, x)
        σ = em.σ(θ, x)
        x = @. x + δ * μ + sqrt(δ) * σ * randn(rng)
    end

    return x
end
#+end_src

#+RESULTS:

#+begin_src julia
# Simple OU process.
ou = EulerMaruyama((θ, x) -> -θ[1] * x, (θ, x) -> θ[2])

θ = 1.0; σ = sqrt(2); δ = 0.05;
t = 12
xs = let
    x = 10.0
    xs = [x]
    for _ = 1:ceil(Int, t / δ)
        x = ou([θ, σ], x, δ)
        push!(xs, x)
    end
    xs
end

plot(0:δ:(length(xs) - 1) * δ, xs, legend=false)
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/7c3945985ca929a14b309bb0a74f6a7a934b07c7.svg]]


#+begin_src julia 
# Algorithm 1: Particle Filter Algorithm
"""
    particle_filter(y, x₀, ode; n, N)

# Arguments
- `y`: observations
- `x₀`: initial states
- `ode`: ODE model

# Keyword Arguments
- `n`: number of observations (default: length(y))
- `N`: number of particles (default: length(x₀))
- `M`: number of discretization steps between each observation time (default: 1)
"""
function particle_filter(x₀, V₀, y, θ, ts, prob, f, em; n = length(y), N = 10, m = 1, hinv=exp)
    # TODO: Change it all to work on log-scale as much as possible.
    # Buffer for the particles.
    V = Matrix{typeof(V₀)}(undef, N, n)
    for j = 1:N
        V[j, 1] = V₀
    end
    x = Matrix{typeof(x₀)}(undef, N, n)
    for j = 1:N
        x[j, 1] = x₀
    end

    α = zeros(N, n)
    W = zeros(N, n)
    L = one(eltype(x₀))

    δ = 1 / (m + 1)
    for i = 1:n - 1
        # Remake the problem to the next observation time.
        prob_t = remake(prob, tspan=(ts[i], ts[i + 1]))
        # For each particle.
        for j = 1:N
            # Sample x[i + 1] given x[i].
            x_j = em(θ, x[j, i], δ, m)
            x[j, i + 1] = x_j

            # Solve ODE between the two observation times.
            β = hinv(x_j)
            prob_t_j = remake(prob_t, p=[β,])
            V_t = solve(prob_t_j)  # TODO: Disable storing of the intermediate steps.
            V[j, i + 1] = V_t[end]

            # Compute the likelihood of the observation at time i + 1.
            # TODO: Should this be a Turing.jl model?
            α[j, i + 1] = f(y[i + 1], V[j, 1:i + 1])
        end

        W[:, i + 1] = normalize(α[:, i + 1], 1); L = L * mean(α);
        # TODO: Resample
        # resample(V[1:i + 1, :], x[1:i + 1, :])
    end

    return (; V, W, x, L)
end
#+end_src

#+RESULTS:
: particle_filter

#+begin_src julia
f(y, V) = pdf(Normal(V[end][3], 1), y)
hinv = exp
N = 10
m = 1
δ = 1 / (m + 1)
tspan = (0, 10)
ts = tspan[1]:δ:tspan[2]
n = length(t)

θ = [0.1, 1.0]
x₀ = rand(Normal())

xs_true = Float64[x₀]
for i = 1:n
    x = ou(θ, xs_true[i], δ, m)
    push!(xs_true, x)
end
xs_true
#+end_src

#+RESULTS:
#+begin_example
22-element Vector{Float64}:
 -1.12242819079607
 -1.2789181783083663
 -1.3371907554484166
 -2.0489365575431835
 -1.7646314016956766
 -1.4247881205691546
 -1.7357235770154544
 -2.342842274020485
 -1.9977452028674236
 -1.2514088596080764
 -0.6770661562948815
 -1.0418801776033924
  0.09954442056688761
  0.961889665495137
  0.36146941984340397
 -0.6453352223026647
 -1.173442479212794
 -1.1894362974934403
 -0.6825406471715564
 -0.8078409525165806
 -0.9262782719934382
 -0.4064042872790363
#+end_example

#+begin_src julia
βs = hinv.(xs_true)
#+end_src

#+RESULTS:
#+begin_example
22-element Vector{Float64}:
 0.32548848614158143
 0.278338249993427
 0.26258229129036514
 0.12887187856955037
 0.17124989727471235
 0.2405594274035819
 0.1762726073231259
 0.09605423741097523
 0.13564078113365008
 0.28610143603048266
 0.5081055099107689
 0.35279074873372446
 1.1046675396084433
 2.616636372040736
 1.4354371254210647
 0.5244866929970244
 0.30930034644041815
 0.3043928026996435
 0.5053314910310189
 0.445819573507736
 0.39602486789154107
 0.6660408411465664
#+end_example

#+begin_src julia
V₀ = [999.0, 1, 0, 0]
probf = SEIR(0.25, 0.05, 1000)
prob = ODEProblem(probf, V₀, tspan, [0.2])
#+end_src

#+RESULTS:
: ODEProblem with uType Vector{Float64} and tType Int64. In-place: true
: timespan: (0, 10)
: u0: 4-element Vector{Float64}:
:  999.0
:    1.0
:    0.0
:    0.0

#+begin_src julia
let i = 1, prob = prob
    β = βs[i]
    prob = remake(prob, p=[β,], tspan=(ts[i], ts[i + 1]))
    V_t = solve(prob)[end]
end
#+end_src

#+RESULTS:
: 4-element Vector{Float64}:
:  993.3190416945545
:    3.1728353938753147
:    3.001339155384102
:    0.5067837561861378

#+begin_src julia
V_true = zeros(length(V₀), n)
V_true[:, 1] = V₀
for i = 1:n - 1
    β = βs[i]
    prob = remake(prob, u0=V_true[:, i], p=[β,], tspan=(ts[i], ts[i + 1]))
    V_t = solve(prob)
    V_true[:, i + 1] = V_t[end]
end
#+end_src

#+RESULTS:

#+begin_src julia
infected_true = V_true[3, :]
plot(t, infected_true, label="true", size=(400, 200))
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/563d73e92cd45919dee49e0194798f012bdc11ab.svg]]


#+begin_src julia
V_true
#+end_src

#+RESULTS:
: 4×21 Matrix{Float64}:
:  999.0  998.99        998.967       …  994.412     993.778     993.149
:    1.0    0.891787      0.809146         2.84271     3.10543     3.33332
:    0.0    0.11641       0.218253         2.37223     2.68042     3.01127
:    0.0    0.00148933    0.00569941       0.372657    0.435763    0.506867

#+begin_src julia
ys = map(eachcol(V_true)) do V
    V[3] + rand(Normal())
end
#+end_src

#+RESULTS:
#+begin_example
21-element Vector{Float64}:
 -1.1070438506683415
  1.1799188261497753
 -1.043698409644979
  0.105700706540722
  0.6392635684868881
 -1.4286126519516427
  2.1778154972966166
 -1.2325746684740246
 -0.4309810726629638
  2.571173341013002
  0.26621712248891116
  1.411363688579649
  2.6835787566250566
  2.316511901215801
  3.3531217706386975
  2.3118928273956763
  3.531014987621324
  0.7693504656586543
  1.630892366540588
  2.78467523776301
  1.2175869897851939
#+end_example

#+begin_src julia
x₀
#+end_src

#+RESULTS:
: -1.12242819079607

#+begin_src julia
res = particle_filter(x₀, V₀, ys, θ, ts, prob, f, ou)
#+end_src

#+RESULTS:
: '(V = ((999.0  1.0  0.0  0.0) (992.567480127893  3.880315843173082  3.0450556277023084  0.5071484012317086) … (990.7133459949242  5.62655438672759  3.1520616790880354  0.5080379392601324) (988.7828600662668  7.445932393827948  3.2622579062239914  0.5089496336812359); (999.0  1.0  0.0  0.0) (992.6932636586903  3.7618956335496923  3.0377531670628914  0.5070875406970607) … (993.4976068653611  3.004773835095676  2.9909225368519397  0.5066967626913121) (993.5798386118314  2.92738310279547  2.9861216309794822  0.506656654393703); … ; (999.0  1.0  0.0  0.0) (993.3109644536562  3.1804377901343153  3.0018100682125457  0.5067876879969937) … (990.895217786711  5.455213675289441  3.141617237213093  0.5079513007864767) (990.9691384977791  5.385576460323743  3.1373689924565196  0.5079160494406713); (999.0  1.0  0.0  0.0) (993.5633231242866  2.9429261166225547  2.9870860469516614  0.5066647121392447) … (0.00023754632423463065  912.8347412092216  85.8805648161066  1.2844564283475757) (35.75193960300995  907.3788217484798  55.969048276002326  0.9001903725079801))  W = (0.0 0.09407181538492553 … 0.10976543229233554 0.0829141032162489; 0.0 0.09535930443048059 … 0.11495756722904851 0.1403707814524568; … ; 0.0 0.10187863779416834 … 0.11018141761988716 0.10620416009211028; 0.0 0.10463723320908247 … 0.0 0.0)  x = (-1.12242819079607 -0.15785259504279137 … 0.7593877728138361 1.2363086139719777; -1.12242819079607 -0.266738620394919 … -1.6135183799767705 -1.9595286365674185; … ; -1.12242819079607 -1.1050481706865976 … 0.6993247136681618 0.6738028035274951; -1.12242819079607 -1.879732948700301 … 6.80209962235263 5.881568012675738)  L = 1.6879074072754601e-31)

#+begin_src julia
res.V[:, end]
#+end_src

#+RESULTS:
#+begin_example
10-element Vector{Vector{Float64}}:
 [988.7828600662668, 7.445932393827948, 3.2622579062239914, 0.5089496336812359]
 [993.5798386118314, 2.92738310279547, 2.9861216309794822, 0.506656654393703]
 [990.4009286899848, 5.920907505995813, 3.1699773437956957, 0.508186460223752]
 [992.9093549208264, 3.558467561713464, 3.0251946893293007, 0.5069828281308638]
 [989.8525665940492, 6.437641481392905, 3.201345700700277, 0.5084462238575093]
 [993.1155895165085, 3.364334306768634, 3.0131934705078325, 0.5068827062150829]
 [987.4089196152484, 8.741521136066499, 3.3399692713404945, 0.5095899773446905]
 [993.6841888244178, 2.829179631956875, 2.980025828214709, 0.5066057154106645]
 [990.9691384977791, 5.385576460323743, 3.1373689924565196, 0.5079160494406713]
 [35.75193960300995, 907.3788217484798, 55.969048276002326, 0.9001903725079801]
#+end_example

* Julia: The Good, the Bad, and the Ugly
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Julia-The-Good-the-Bad-and-the-Ugly
:CUSTOM_ID: 2023-01-29-16-57-28-Julia-The-Good-the-Bad-and-the-Ugly
:END:

An honest take from a little 27-year old Norwegian boy

*** The Good
:PROPERTIES:
:ID:       2023-01-29-16-57-28-The-Good
:CUSTOM_ID: 2023-01-29-16-57-28-The-Good
:END:
- Speed
- Composability (thank you multiple dispatch)
- No need to tie yourself to an underlying computational framework
- Interactive
- Transparency
- Very easy to call into other languages

*** Speed
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Speed
:CUSTOM_ID: 2023-01-29-16-57-28-Speed
:END:

I think you got this already...

*** Composability
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Composability
:CUSTOM_ID: 2023-01-29-16-57-28-Composability
:END:

We've seen some of that

Defining =infected(problem_wrapper, u)= allowed us to abstract away how to extract the compartment of interest

*** Transparency
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Transparency
:CUSTOM_ID: 2023-01-29-16-57-28-Transparency
:END:

For starters, almost all the code you'll end up using is pure Julia

Hence, you can always look at the code

You can find the implementation by using =@which=

#+begin_src julia 
# Without arguments
@which sum
#+end_src

#+RESULTS:
: Base

#+begin_src julia :display text/plain
# With arguments
@which sum([1.0])
#+end_src

#+RESULTS:
: sum(a::AbstractArray; dims, kw...) in Base at reducedim.jl:994

#+REVEAL: split

And yeah, you can even look into the macros

#+HTML: <div class="small-text">

#+begin_src julia 
@macroexpand @model f() = x ~ Normal()
#+end_src

#+RESULTS:
#+begin_example
quote
    function f(__model__::DynamicPPL.Model, __varinfo__::DynamicPPL.AbstractVarInfo, __context__::AbstractPPL.AbstractContext; )
        #= In[113]:1 =#
        begin
            var"##dist#1413" = Normal()
            var"##vn#1410" = (DynamicPPL.resolve_varnames)((AbstractPPL.VarName){:x}(), var"##dist#1413")
            var"##isassumption#1411" = begin
                    if (DynamicPPL.contextual_isassumption)(__context__, var"##vn#1410")
                        if !((DynamicPPL.inargnames)(var"##vn#1410", __model__)) || (DynamicPPL.inmissings)(var"##vn#1410", __model__)
                            true
                        else
                            x === missing
                        end
                    else
                        false
                    end
                end
            begin
                #= /home/tor/.julia/packages/DynamicPPL/WBmMU/src/compiler.jl:539 =#
                var"##retval#1415" = if var"##isassumption#1411"
                        begin
                            (var"##value#1414", __varinfo__) = (DynamicPPL.tilde_assume!!)(__context__, (DynamicPPL.unwrap_right_vn)((DynamicPPL.check_tilde_rhs)(var"##dist#1413"), var"##vn#1410")..., __varinfo__)
                            x = var"##value#1414"
                            var"##value#1414"
                        end
                    else
                        if !((DynamicPPL.inargnames)(var"##vn#1410", __model__))
                            x = (DynamicPPL.getvalue_nested)(__context__, var"##vn#1410")
                        end
                        (var"##value#1412", __varinfo__) = (DynamicPPL.tilde_observe!!)(__context__, (DynamicPPL.check_tilde_rhs)(var"##dist#1413"), x, var"##vn#1410", __varinfo__)
                        var"##value#1412"
                    end
                #= /home/tor/.julia/packages/DynamicPPL/WBmMU/src/compiler.jl:540 =#
                return (var"##retval#1415", __varinfo__)
            end
        end
    end
    begin
        $(Expr(:meta, :doc))
        function f(; )
            #= In[113]:1 =#
            return (DynamicPPL.Model)(f, NamedTuple(), NamedTuple())
        end
    end
end
#+end_example

#+HTML: </div>

#+REVEAL: split

I told you didn't want to see that.

Can make it /a bit/ cleaner by removing linenums:

#+HTML: <div class="x-small-text">

#+begin_src julia 
@macroexpand(@model f() = x ~ Normal()) |> Base.remove_linenums!
#+end_src

#+RESULTS:
#+begin_example
quote
    function f(__model__::DynamicPPL.Model, __varinfo__::DynamicPPL.AbstractVarInfo, __context__::AbstractPPL.AbstractContext; )
        begin
            var"##dist#1419" = Normal()
            var"##vn#1416" = (DynamicPPL.resolve_varnames)((AbstractPPL.VarName){:x}(), var"##dist#1419")
            var"##isassumption#1417" = begin
                    if (DynamicPPL.contextual_isassumption)(__context__, var"##vn#1416")
                        if !((DynamicPPL.inargnames)(var"##vn#1416", __model__)) || (DynamicPPL.inmissings)(var"##vn#1416", __model__)
                            true
                        else
                            x === missing
                        end
                    else
                        false
                    end
                end
            begin
                var"##retval#1421" = if var"##isassumption#1417"
                        begin
                            (var"##value#1420", __varinfo__) = (DynamicPPL.tilde_assume!!)(__context__, (DynamicPPL.unwrap_right_vn)((DynamicPPL.check_tilde_rhs)(var"##dist#1419"), var"##vn#1416")..., __varinfo__)
                            x = var"##value#1420"
                            var"##value#1420"
                        end
                    else
                        if !((DynamicPPL.inargnames)(var"##vn#1416", __model__))
                            x = (DynamicPPL.getvalue_nested)(__context__, var"##vn#1416")
                        end
                        (var"##value#1418", __varinfo__) = (DynamicPPL.tilde_observe!!)(__context__, (DynamicPPL.check_tilde_rhs)(var"##dist#1419"), x, var"##vn#1416", __varinfo__)
                        var"##value#1418"
                    end
                return (var"##retval#1421", __varinfo__)
            end
        end
    end
    begin
        $(Expr(:meta, :doc))
        function f(; )
            return (DynamicPPL.Model)(f, NamedTuple(), NamedTuple())
        end
    end
end
#+end_example

#+HTML: </div>

#+REVEAL: split

#+begin_src julia
f(x) = 2x
#+end_src

#+RESULTS:
: f (generic function with 1 method)

You can inspect the type-inferred and lowered code

#+begin_src julia
@code_typed f(1)
#+end_src

#+RESULTS:
: CodeInfo(
: 1 ─ %1 = Base.mul_int(2, x)::Int64
: └──      return %1
: ) => Int64

#+REVEAL: split

You can inspect the LLVM code

#+begin_src julia
@code_llvm f(1)
#+end_src

#+RESULTS:
: ;  @ In[115]:1 within `f`
: define i64 @julia_f_52767(i64 signext %0) #0 {
: top:
: ; ┌ @ int.jl:88 within `*`
:    %1 = shl i64 %0, 1
: ; └
:   ret i64 %1
: }

#+REVEAL: split

And even the resulting machine code

#+begin_src julia
@code_native f(1)
#+end_src

#+RESULTS:
#+begin_example
	.text
	.file	"f"
	.globl	julia_f_52804                   # -- Begin function julia_f_52804
	.p2align	4, 0x90
	.type	julia_f_52804,@function
julia_f_52804:                          # @julia_f_52804
; ┌ @ In[115]:1 within `f`
	.cfi_startproc
# %bb.0:                                # %top
; │┌ @ int.jl:88 within `*`
	leaq	(%rdi,%rdi), %rax
; │└
	retq
.Lfunc_end0:
	.size	julia_f_52804, .Lfunc_end0-julia_f_52804
	.cfi_endproc
; └
                                        # -- End function
	.section	".note.GNU-stack","",@progbits
#+end_example

It really just depends on which level of "I hate my life" you're currently at

*** Calling into other languages
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Calling-into-other-languages
:CUSTOM_ID: 2023-01-29-16-57-28-Calling-into-other-languages
:END:
- [[https://docs.julialang.org/en/v1/manual/calling-c-and-fortran-code/][C and Fortran comes built-in stdlib]]
- [[https://juliainterop.github.io/RCall.jl/stable/][RCall.jl]]: call into =R=
- [[https://github.com/JuliaPy/PyCall.jl][PyCall.jl]]: call into =python=
- Etc.

When working with =Array=, etc. memory is usually shared ⟹ fairly low overhead

*** C and Fortran
:PROPERTIES:
:ID:       2023-01-29-16-57-28-C-and-Fortran
:CUSTOM_ID: 2023-01-29-16-57-28-C-and-Fortran
:END:
#+begin_src julia 
# Define the Julia function
function mycompare(a, b)::Cint
    println("mycompare($a, $b)")  # NOTE: Let's look at the comparisons made.
    return (a < b) ? -1 : ((a > b) ? +1 : 0)
end

# Get the corresponding C function pointer.
mycompare_c = @cfunction(mycompare, Cint, (Ref{Cdouble}, Ref{Cdouble}))

# Array to sort.
arr = [1.3, -2.7, 4.4, 3.1];

# Call in-place quicksort.
ccall(:qsort, Cvoid, (Ptr{Cdouble}, Csize_t, Csize_t, Ptr{Cvoid}),
      arr, length(arr), sizeof(eltype(arr)), mycompare_c)
#+end_src

#+RESULTS:
: mycompare(1.3, -2.7)
: mycompare(4.4, 3.1)
: mycompare(-2.7, 3.1)
: mycompare(1.3, 3.1)

#+begin_src julia 
# All sorted!
arr
#+end_src

#+RESULTS:
: 4-element Vector{Float64}:
:  -2.7
:   1.3
:   3.1
:   4.4

[[https://docs.julialang.org/en/v1/manual/calling-c-and-fortran-code/#Creating-C-Compatible-Julia-Function-Pointers][Example is from Julia docs]]

*** RCall.jl                                                       :noexport:
:PROPERTIES:
:ID:       2023-01-29-16-57-28-RCall-dot-jl
:CUSTOM_ID: 2023-01-29-16-57-28-RCall-dot-jl
:END:

*** PyCall.jl                                                      :noexport:
:PROPERTIES:
:ID:       2023-01-29-16-57-28-PyCall-dot-jl
:CUSTOM_ID: 2023-01-29-16-57-28-PyCall-dot-jl
:END:

*** The Bad
:PROPERTIES:
:ID:       2023-01-29-16-57-28-The-Bad
:CUSTOM_ID: 2023-01-29-16-57-28-The-Bad
:END:
Sometimes
- your code might just slow down without a seemingly good reason,
- someone did bad, and Julia can't tell which method to call, or
- someone forces the Julia compiler to compile insane amounts of code

*** "Why is my code suddenly slow?"
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Why-is-my-code-suddenly-slow
:CUSTOM_ID: 2023-01-29-16-57-28-Why-is-my-code-suddenly-slow
:END:

One word: *type-instability*

Sometimes the Julia compiler can't quite infer what types fully

#+HTML: <div class="fragment (appear)">

*Result:* python-like performance (for those particular function calls)

#+begin_src julia 
# NOTE: this is NOT `const`, and so it could become some other type
# at any given point without `my_func` knowing about it!
global_variable = 1
my_func_unstable(x) = global_variable * x
#+end_src

#+RESULTS:
: my_func_unstable (generic function with 1 method)

#+begin_src julia 
@btime my_func_unstable(2.0);
#+end_src

#+RESULTS:
:   30.668 ns (2 allocations: 32 bytes)

#+HTML: </div>

#+REVEAL: split

Luckily there are tools for inspecting this

#+begin_src julia 
@code_warntype my_func_unstable(2.0)
#+end_src

#+RESULTS:
: MethodInstance for my_func_unstable(::Float64)
:   from my_func_unstable(x) in Main at In[121]:4
: Arguments
:   #self#::Core.Const(my_func_unstable)
:   x::Float64
: Body::Any
: 1 ─ %1 = (Main.global_variable * x)::Any
: └──      return %1
: 

See that =Any= there? _'tis a big no-no!_

#+REVEAL: split

Once discovered, it can be fixed

#+begin_src julia 
const constant_global_variable = 1
my_func_fixed(x) = constant_global_variable * x
@code_warntype my_func_fixed(2.0)
#+end_src

#+RESULTS:
: MethodInstance for my_func_fixed(::Float64)
:   from my_func_fixed(x) in Main at In[124]:2
: Arguments
:   #self#::Core.Const(my_func_fixed)
:   x::Float64
: Body::Float64
: 1 ─ %1 = (Main.constant_global_variable * x)::Float64
: └──      return %1
: 

So long Python performance!

#+begin_src julia 
@btime my_func_fixed(2.0);
#+end_src

#+RESULTS:
:   1.696 ns (0 allocations: 0 bytes)


#+REVEAL: split

/But/ this is not always so easy to discover (though this is generally rare)

#+begin_src julia 
# HACK: Here we explicitly tell Julia what type `my_func_unstable`
# returns. This is _very_ rarely a good idea because it just hides
# the underlying problem from `@code_warntype`!
my_func_forced(x) = my_func_unstable(x)::typeof(x)
@code_warntype my_func_forced(2.0)
#+end_src

#+RESULTS:
#+begin_example
MethodInstance for my_func_forced(::Float64)
  from my_func_forced(x) in Main at In[126]:4
Arguments
  #self#::Core.Const(my_func_forced)
  x::Float64
Body::Float64
1 ─ %1 = Main.my_func_unstable(x)::Any
│   %2 = Main.typeof(x)::Core.Const(Float64)
│   %3 = Core.typeassert(%1, %2)::Float64
└──      return %3
#+end_example

We can still see the =Any= in there, but on a first glance it looks like =my_func_forced= is type-stable

There are more natural cases where this might occur, e.g. unfortunate closures deep in your callstack

#+REVEAL: split

To discovery these there are a couple of more advanced tools:
- [[https://github.com/JuliaDebug/Cthulhu.jl][Cthulhu.jl]]: Allows you to step through your code like a debugger and perform =@code_warntype=
- [[https://github.com/aviatesk/JET.jl][JET.jl]]: Experimental package which attempts to automate the process

And even simpler: profile using [[https://github.com/timholy/ProfileView.jl][ProfileView.jl]] and look for code-paths that /should/ be fast but take up a lot of the runtime

#+REVEAL: split

#+begin_src julia 
using ProfileView
#+end_src

#+RESULTS:

#+begin_src julia :eval no
@profview foreach(_ -> my_func_unstable(2.0), 1_000_000)
#+end_src

#+DOWNLOADED: file:///tmp/Spectacle.wcviMK/Screenshot_20230125_011603.png @ 2023-01-25 01:16:13
#+ATTR_HTML: :height 350px
#+ATTR_ORG: :width 600
[[file:assets/attachments/2023-01-25_01-16-13_Screenshot_20230125_011603.png]]

Note that there's no sign of multiplication here

But most of the runtime is the =./reflection.jl= at the top there

That's Julia looking up the type at runtime

*** Method ambiguity
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Method-ambiguity
:CUSTOM_ID: 2023-01-29-16-57-28-Method-ambiguity
:END:
#+begin_src julia 
ambiguous_function(x, y::Int) = y
ambiguous_function(x::Int, y) = x

# NOTE: Here we have `ambiguous_function(x::Int, y::Int)`
# Which one should we hit?!
ambiguous_function(1, 2)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: MethodError: ambiguous_function(::Int64, ::Int64) is ambiguous. Candidates:
:   ambiguous_function(x, y::Int64) in Main at In[128]:1
:   ambiguous_function(x::Int64, y) in Main at In[128]:2
: Possible fix, define
:   ambiguous_function(::Int64, ::Int64)
: 
: Stacktrace:
:  [1] top-level scope
:    @ In[128]:6
:END:

But here Julia warns us, and so we can fix this by just doing as it says: define =ambiguous_function(::Int64, ::Int64)=

#+begin_src julia 
ambiguous_function(::Int64, ::Int64) = "neato"
ambiguous_function(1, 2)
#+end_src

#+RESULTS:
: "neato"

*** Long compilation times
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Long-compilation-times
:CUSTOM_ID: 2023-01-29-16-57-28-Long-compilation-times
:END:
In Julia, for better or worse, we can generate code

*Problem:* it can be /lots/ of code of we really want to

*Result:* first execution can be /slow/

#+HTML: <div class="fragment (appear)">

*Time to first plot (TTFP)* is Julia's worst enemy

But things are always improving

#+DOWNLOADED: file:///tmp/Spectacle.wcviMK/Screenshot_20230125_012853.png @ 2023-01-25 01:29:05
[[file:assets/attachments/2023-01-25_01-29-05_Screenshot_20230125_012853.png]]

#+HTML: </div>

*** Another example: mis-use of =@generated=
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Another-example-mis-use-of-generated
:CUSTOM_ID: 2023-01-29-16-57-28-Another-example-mis-use-of-generated
:END:

#+begin_src julia 
# NOTE: `@generated` only has access to static information, e.g. types of arguments.
# Here I'm using the special type `Val` to make a number `N` static.
@generated function unrolled_addition(::Val{N}) where {N}
    expr = Expr(:block)
    push!(expr.args, :(x = 0))
    for i = 1:N
        push!(expr.args, :(x += $(3.14 * i)))
    end

    return expr
end
#+end_src

#+RESULTS:
: unrolled_addition (generic function with 1 method)

When I call this with some =Val(N)=, Julia will execute this /at compile-time/!

#+begin_src julia 
# NOTE: At runtime, it then just returns the result immediately
@code_typed unrolled_addition(Val(10))
#+end_src

#+RESULTS:
: CodeInfo(
: 1 ─     return 172.70000000000002
: ) => Float64

But if I just change the value =10= to =11=, it's a /completely/ different type!

#+REVEAL: split

So Julia has to compile =unrolled_addition= from scratch

#+begin_src julia 
@time @eval unrolled_addition(Val(11));
#+end_src

#+RESULTS:
:   0.010027 seconds (11.61 k allocations: 654.885 KiB, 8.32% compilation time)

Or a bit crazier

#+begin_src julia 
@time @eval unrolled_addition(Val(10_001));
#+end_src

#+RESULTS:
:   0.429549 seconds (1.19 M allocations: 48.946 MiB, 99.94% compilation time)

Here it took ~0.4s, of which 99.95% was compilation time

I think you get the idea

#+REVEAL: split

But boy is it fast to run!

#+begin_src julia 
@btime unrolled_addition(Val(10_001));
#+end_src

#+RESULTS:
:   1.637 ns (0 allocations: 0 bytes)

#+begin_src julia 
function not_unrolled_addition(N)
    x = 0
    for i = 1:N
        x += 3.14 * i
    end

    return x
end
#+end_src

#+RESULTS:
: not_unrolled_addition (generic function with 1 method)

#+begin_src julia 
@btime not_unrolled_addition(10_001);
#+end_src

#+RESULTS:
:   11.138 μs (0 allocations: 0 bytes)

#+REVEAL: split

*Funny side-note:* at first I did the following

#+begin_src julia 
@generated function unrolled_addition_old(::Val{N}) where {N}
    expr = Expr(:block)
    push!(expr.args, :(x = 0))
    for i = 1:N
        push!(expr.args, :(x += $i))  # NOTE: No 3.14!
    end
    return expr
end
function not_unrolled_addition_old(N)
    x = 0
    for i = 1:N
        x += i  # NOTE: No 3.14!
    end
    return x
end
#+end_src

#+RESULTS:
: not_unrolled_addition_old (generic function with 1 method)

#+begin_src julia 
@btime unrolled_addition_old(Val(10_001));
@btime not_unrolled_addition_old(10_001);
#+end_src

#+RESULTS:
:   1.538 ns (0 allocations: 0 bytes)
:   3.495 ns (0 allocations: 0 bytes)

LLVM probably recognized the pattern of =not_unrolled_addition_old= and unrolls it for us

Let's check!

#+REVEAL: split

#+begin_src julia 
# NOTE: The one LLVM failed to unroll
@code_llvm not_unrolled_addition(10_001)
#+end_src

#+RESULTS:
#+begin_example
;  @ In[135]:1 within `not_unrolled_addition`
define { {}*, i8 } @julia_not_unrolled_addition_53856([8 x i8]* noalias nocapture align 8 dereferenceable(8) %0, i64 signext %1) #0 {
top:
;  @ In[135]:3 within `not_unrolled_addition`
; ┌ @ range.jl:5 within `Colon`
; │┌ @ range.jl:393 within `UnitRange`
; ││┌ @ range.jl:400 within `unitrange_last`
     %.inv = icmp sgt i64 %1, 0
     %. = select i1 %.inv, i64 %1, i64 0
; └└└
  br i1 %.inv, label %L18.preheader, label %union_move16

L18.preheader:                                    ; preds = %top
;  @ In[135]:5 within `not_unrolled_addition`
; ┌ @ range.jl:883 within `iterate`
; │┌ @ promotion.jl:477 within `==`
    %.not30 = icmp eq i64 %., 1
; └└
  br i1 %.not30, label %union_move, label %L51

L51:                                              ; preds = %L51, %L18.preheader
  %value_phi1032 = phi double [ %value_phi10, %L51 ], [ 3.140000e+00, %L18.preheader ]
  %value_phi431 = phi i64 [ %2, %L51 ], [ 1, %L18.preheader ]
; ┌ @ range.jl:883 within `iterate`
   %2 = add i64 %value_phi431, 1
; └
;  @ In[135]:4 within `not_unrolled_addition`
; ┌ @ promotion.jl:389 within `*`
; │┌ @ promotion.jl:359 within `promote`
; ││┌ @ promotion.jl:336 within `_promote`
; │││┌ @ number.jl:7 within `convert`
; ││││┌ @ float.jl:146 within `Float64`
       %3 = sitofp i64 %2 to double
; │└└└└
; │ @ promotion.jl:389 within `*` @ float.jl:385
   %4 = fmul double %3, 3.140000e+00
; └
;  @ In[135] within `not_unrolled_addition`
  %value_phi10 = fadd double %value_phi1032, %4
;  @ In[135]:5 within `not_unrolled_addition`
; ┌ @ range.jl:883 within `iterate`
; │┌ @ promotion.jl:477 within `==`
    %.not = icmp eq i64 %2, %.
; └└
  br i1 %.not, label %L18.union_move_crit_edge, label %L51

post_union_move:                                  ; preds = %union_move16, %union_move
  %tindex_phi1429 = phi i8 [ 2, %union_move16 ], [ 1, %union_move ]
;  @ In[135]:7 within `not_unrolled_addition`
  %5 = insertvalue { {}*, i8 } { {}* null, i8 undef }, i8 %tindex_phi1429, 1
  ret { {}*, i8 } %5

L18.union_move_crit_edge:                         ; preds = %L51
;  @ In[135]:5 within `not_unrolled_addition`
  %phi.cast = bitcast double %value_phi10 to i64
  br label %union_move

union_move:                                       ; preds = %L18.union_move_crit_edge, %L18.preheader
  %value_phi10.lcssa = phi i64 [ %phi.cast, %L18.union_move_crit_edge ], [ 4614253070214989087, %L18.preheader ]
;  @ In[135]:7 within `not_unrolled_addition`
  %6 = bitcast [8 x i8]* %0 to i64*
  store i64 %value_phi10.lcssa, i64* %6, align 8
  br label %post_union_move

union_move16:                                     ; preds = %top
  %7 = bitcast [8 x i8]* %0 to i64*
  store i64 0, i64* %7, align 8
  br label %post_union_move
}
#+end_example

#+REVEAL: split

#+begin_src julia 
# NOTE: The one LLVM seems to have unrolled.
@code_llvm not_unrolled_addition_old(10_001)
#+end_src

#+RESULTS:
#+begin_example
;  @ In[137]:9 within `not_unrolled_addition_old`
define i64 @julia_not_unrolled_addition_old_53858(i64 signext %0) #0 {
top:
;  @ In[137]:11 within `not_unrolled_addition_old`
; ┌ @ range.jl:5 within `Colon`
; │┌ @ range.jl:393 within `UnitRange`
; ││┌ @ range.jl:400 within `unitrange_last`
     %.inv = icmp sgt i64 %0, 0
     %. = select i1 %.inv, i64 %0, i64 0
; └└└
  br i1 %.inv, label %L18.preheader, label %L35

L18.preheader:                                    ; preds = %top
;  @ In[137]:13 within `not_unrolled_addition_old`
  %1 = shl nuw i64 %., 1
  %2 = add nsw i64 %., -1
  %3 = zext i64 %2 to i65
  %4 = add nsw i64 %., -2
  %5 = zext i64 %4 to i65
  %6 = mul i65 %3, %5
  %7 = lshr i65 %6, 1
  %8 = trunc i65 %7 to i64
  %9 = add i64 %1, %8
  %10 = add i64 %9, -1
;  @ In[137]:14 within `not_unrolled_addition_old`
  br label %L35

L35:                                              ; preds = %L18.preheader, %top
  %value_phi10 = phi i64 [ 0, %top ], [ %10, %L18.preheader ]
  ret i64 %value_phi10
}
#+end_example

*** The Ugly
:PROPERTIES:
:ID:       2023-01-29-16-57-28-The-Ugly
:CUSTOM_ID: 2023-01-29-16-57-28-The-Ugly
:END:

#+REVEAL: split

_*Reverse-mode automatic differentiation*_

ForwardDiff.jl is a pure joy, but slows down as dimensionality grows

Then one should reach for ReverseDiff.jl or Zygote.jl

#+HTML: <div class="fragment (appear)">
Most of the time it works really well, but sometimes you hit a real sharp edge

And sharp edges cut; they cut /deep/

Like _"16X slower when the function is implemented more efficiently"-deep_

#+DOWNLOADED: file:///tmp/Spectacle.wcviMK/Screenshot_20230125_010111.png @ 2023-01-25 01:01:31
[[file:assets/attachments/2023-01-25_01-01-31_Screenshot_20230125_010111.png]]

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

If you want to see a man in pain, you can find the full issue [[https://github.com/TuringLang/Turing.jl/issues/1934][here]]

On the flip-side, once addressed (a type-instability), it's [[https://github.com/TuringLang/DistributionsAD.jl/pull/231][3X faster than before]]

#+HTML: </div>

*** Overall
:PROPERTIES:
:ID:       2023-01-29-16-57-28-Overall
:CUSTOM_ID: 2023-01-29-16-57-28-Overall
:END:

Julia is pretty darn awesome

Easy to get going, and you can always make it faster by just optimizing your Julia code

No need to drop down to C++

#+REVEAL: split
Buuuut it can't beat Python at deep learning

#+REVEAL: split
Otherwise, it's worth a try

Godspeed to you

#+REVEAL: split
Fin.

* Going fast                                                       :noexport:
#+begin_src julia :exports none
using TorsWorkshop, Turing, StatsPlots
#+end_src

#+RESULTS:

Consider the following item-response model

#+begin_src julia
using StatsFuns

@model function irt(y, i, p; I = maximum(i), P = maximum(p))
    theta ~ filldist(Normal(), P)
    beta ~ filldist(Normal(), I)

    for n in eachindex(y)
        y[n] ~ Bernoulli(logistic(theta[p[n]] - beta[i[n]]))
    end

    return (; theta, beta, y)
end
#+end_src

#+RESULTS:
: irt (generic function with 2 methods)

Throughout this tutorial, we will see how we can improve this model step-by-step and make it run /much/ faster.

But before we do that, let's simulate some data so we can test our models.

#+begin_src julia 
P = 100   # number of users
I = 3     # number of items
N = I * P # number of observations
i_and_p = vec(collect(Iterators.product(1:I, 1:P)))
i = map(first, i_and_p); p = map(last, i_and_p);
model_generation = irt(Vector{Union{Missing,Int}}(missing, N), i, p)
#+end_src

#+RESULTS:
: Model(
:   args = (:y, :i, :p)
:   defaults = (:I, :P)
:   context = DynamicPPL.DefaultContext()
: )

#+begin_src julia :results scalar
theta_true, beta_true, y = model_generation()
#+end_src

#+RESULTS:
: (theta = [-0.8125131033205886, 0.40222461650587743, 0.4847315071119002, 0.4334098121490582, 1.2448742400518944, 0.026456640814830863, 0.1125570605015704, -0.03663608541182643, -0.2797565154042482, 1.2019992206388352  …  1.9895510032951704, 1.5619189619672569, 1.5726087337065626, 0.11589158272898376, 0.1920180971700441, 1.0391944034277607, 0.5389881966315481, -0.8299382479664033, 0.06017106559407176, 0.13481559604773133], beta = [0.31975216222261077, -0.3608499802930717, 1.5117710713737413], y = Union{Missing, Int64}[1, 0, 0, 0, 1, 0, 1, 1, 0, 0  …  0, 0, 0, 1, 1, 1, 1, 0, 1, 1])

#+begin_src julia :results scalar
# `y` has `eltype` `Union{Missing,Int}`, so let's just make this a vector of `Int`s
y = map(identity, y)
#+end_src

#+RESULTS:
#+begin_example
300-element Vector{Int64}:
 1
 0
 0
 0
 1
 0
 1
 1
 0
 0
 0
 0
 1
 ⋮
 1
 1
 0
 0
 0
 1
 1
 1
 1
 0
 1
 1
#+end_example

And then we can =condition= the model on the data to try and infer the true parameters

#+begin_src julia :results scalar :async yes
model = irt(y, i, p)
chain = sample(model, NUTS(), 1000);
#+end_src

#+RESULTS:
: [36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
: [36m[1m└ [22m[39m  ϵ = 0.8
: [32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:05[39m
:

Let us remind ourselves of the true parameters

#+begin_src julia :results scalar
beta_true
#+end_src

#+RESULTS:
: 3-element Vector{Float64}:
:   0.31975216222261077
:  -0.3608499802930717
:   1.5117710713737413

And compare them to the inferred parameters

#+begin_src julia :results scalar
density(group(chain, :beta))
#+end_src

#+RESULTS:
[[file:assets/outputs/more-julia/57117de3e36d8de7f9e01b01af01e3b1fdeae3d7.svg]]

Okay neat, so the inference seems to have worked.

** Improvement #1: =BernoulliLogit= instead of =Bernoulli= and =logistic=

The first improvement is a simple one: instead of using =Bernoulli= and =logistic=, we can use =BernoulliLogit=, which is a more efficient implementation of the same thing.

=BernoulliLogit= should be:
1. Faster.
2. Numerically more stable.

#+begin_src julia 
@model function irt_01(y, i, p; I = maximum(i), P = maximum(p))
    theta ~ filldist(Normal(), P)
    beta ~ filldist(Normal(), I)

    for n in eachindex(y)
        y[n] ~ BernoulliLogit(theta[p[n]] - beta[i[n]])
    end

    return (; theta, beta, y)
end
#+end_src

#+RESULTS:
: irt_01 (generic function with 2 methods)

#+begin_src julia :async yes
model_01 = irt_01(y, i, p)
chain = sample(model_01, NUTS(), 1000);
#+end_src

#+RESULTS:
: [36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
: [36m[1m└ [22m[39m  ϵ = 0.8
: [32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:05[39m
:

Okay, so it doesn't seem to be so much faster that we notice it in the actual sampling time.

But if we want to make sure, we can perform some benchmarks! For this we'll use [[https://github.com/TuringLang/TuringBenchmarking.jl][=TuringBenchmark.jl=]].

** TuringBenchmarking.jl

#+begin_src julia 
using TuringBenchmarking
#+end_src

#+RESULTS:

Equipped with this, we can easily check the performance of the original model

#+begin_src julia :async yes
# Limit ourselves to `ForwardDiff` since this is what we used above.
benchmark_model(model, adbackends=[:ForwardDiff], verbose=false)
#+end_src

#+RESULTS:
#+begin_example
2-element BenchmarkTools.BenchmarkGroup:
  tags: []
  "evaluation" => 2-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "linked" => Trial(7.500 μs)
	  "standard" => Trial(7.436 μs)
  "gradient" => 1-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "Turing.Essential.ForwardDiffAD{40, true}()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["ForwardDiff"]
		  "linked" => Trial(135.579 μs)
		  "standard" => Trial(135.739 μs)
#+end_example

And then of the model with =BernoulliLogit=

#+begin_src julia :async yes
benchmark_model(model_01, adbackends=[:ForwardDiff], verbose=false)
#+end_src

#+RESULTS:
#+begin_example
2-element BenchmarkTools.BenchmarkGroup:
  tags: []
  "evaluation" => 2-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "linked" => Trial(8.516 μs)
	  "standard" => Trial(8.039 μs)
  "gradient" => 1-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "Turing.Essential.ForwardDiffAD{40, true}()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["ForwardDiff"]
		  "linked" => Trial(122.274 μs)
		  "standard" => Trial(106.949 μs)
#+end_example

Look at that! The =gradient= computations are indeed slightly faster! We're not looking at much, but that's a solid ~10% improvement so we're going to take that.

/But/ all of these computations are only for =3= items and =100= users, which resulted in =300= observations; what happens when the dimensionality increases to, say, =20= items and =1_000= users, i.e. =20_000= observations?

Before we check, we make a few observations about the performances above:
1. =gradient= computation is roughly 15-20X slower than =evaluation=.
   - This will be a useful metric to keep track of as we increase the dimensionality.
2. =linked= computation is roughly 10-15% faster than =standard=.
   - The =linked= computation includes the transformation from unconstrained to constrained parameters.
   - Most gradient-based samplers, e.g. =HMC= and =NUTS=, need to work in unconstrained space, so this is the main runtime we'll be looking at.
   - Even though =linked= is what we really care about, it's useful to keep an eye out for a large discrepancy between =linked= and =standard=, as this would indicate that the transformation is expensive relative to the model itself, which shouldn't /usually/ be the case (exceptions exist).

** Increasing dimensionality

#+begin_src julia :results scalar
P = 1_000   # number of users
I = 20     # number of items
N = I * P # number of observations
i_and_p = vec(collect(Iterators.product(1:I, 1:P)))
i = map(first, i_and_p); p = map(last, i_and_p);
model_generation = irt(Vector{Union{Missing,Int}}(missing, N), i, p)
theta_true, beta_true, y = model_generation()
y = map(identity, y) # convert to Vector{Int}
length(theta_true), length(beta_true), length(y)
#+end_src

#+RESULTS:
: (1000, 20, 20000)

Let's now see how our two models perform on this new data.

#+begin_src julia :async yes
model = irt(y, i, p)
benchmark_model(model, adbackends=[:ForwardDiff], verbose=false)
#+end_src

#+RESULTS:
#+begin_example
2-element BenchmarkTools.BenchmarkGroup:
  tags: []
  "evaluation" => 2-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "linked" => Trial(400.827 μs)
	  "standard" => Trial(400.451 μs)
  "gradient" => 1-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "Turing.Essential.ForwardDiffAD{40, true}()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["ForwardDiff"]
		  "linked" => Trial(59.337 ms)
		  "standard" => Trial(57.017 ms)
#+end_example

#+begin_src julia :async yes
model_01 = irt_01(y, i, p)
benchmark_model(model_01, adbackends=[:ForwardDiff], verbose=false)
#+end_src

#+RESULTS:
#+begin_example
2-element BenchmarkTools.BenchmarkGroup:
  tags: []
  "evaluation" => 2-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "linked" => Trial(522.404 μs)
	  "standard" => Trial(590.827 μs)
  "gradient" => 1-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "Turing.Essential.ForwardDiffAD{40, true}()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["ForwardDiff"]
		  "linked" => Trial(43.888 ms)
		  "standard" => Trial(44.386 ms)
#+end_example

Okay, that is /much/ slower. A few observations:
1. =evaluation= computation is roughly 100X larger than before.
2. =gradient= computation is roughly 100X slower than =evaluation=.
   - This is a huge increase from the 15-20X we saw before.

/But/ is this surprising?
1. Not particularly so. We increased the number of observations from =300= to =20_000=, i.e. a 1000X increase, the number of parameters from =3 + 100= to =1_000 + 20=, i.e. ~3X increase, hence it's not too surprising that the computation is now taking ~100X longer.
2. This is maybe a bit more surprising, /but/ forward-mode AD is known to scale poorly with increasing dimensionality in the number of parameters (keeping the number of outputs fixed), so this is not too surprising either.

So, what can we do here?

(1) requires optimizing the underlying computations, but given how simple our model is, this is probably not what we should try first.

(2) seems like something we can easily improve on: let's try using reverse-mode AD instead.

** Improvement #2: Reverse-mode AD

A few reverse-mode AD backends are supported in Turing.jl:
- ReverseDiff.jl
- Zygote.jl
- Tracker.jl (though this is being phased out)

Given that Tracker.jl is being phased out, we'll focus on the first two.

Doing a deep-dive on how the different backends work and their differences is beyond the scope of this tutorial, but it's worth pointing out some simple differences between ReverseDiff.jl and Zygote.jl.

*** ReverseDiff.jl

ReverseDiff.jl is a tape-based AD system, i.e. it records the computation as it happens and then computes the gradient by traversing the tape in reverse.

Moreover, the type of computations that are actually recorded is limited. For example

#+begin_src julia 
using ReverseDiff: ReverseDiff
#+end_src

#+RESULTS:

#+begin_src julia
f(x) = 2 .* x
g(x) = sum(f(x))
#+end_src

#+RESULTS:
: g (generic function with 1 method)

#+begin_src julia
tape = ReverseDiff.GradientTape(g, ([1.0],))
#+end_src

#+RESULTS:
: typename(ReverseDiff.GradientTape)(g)

#+begin_src julia
tape.tape
#+end_src

#+RESULTS:
#+begin_example
2-element Vector{ReverseDiff.AbstractInstruction}:
 SpecialInstruction((broadcast, *)):
  input:  (2,
           [TrackedReal<IMM>(1.0, 0.0, 62v, 1, 7yl)])
  output: [TrackedReal<CEM>(2.0, 0.0, 62v, 1, 12c)]
  cache:  (nothing,
           CartesianIndex(1,))
 SpecialInstruction(sum):
  input:  [TrackedReal<I6g>(2.0, 0.0, 62v, 1, 12c)]
  output: TrackedReal<onZ>(2.0, 0.0, 62v, ---)
  cache:  nothing
#+end_example

Here we see that the tape has only recorded /two/ operations:
1. =broadcast=, which is a special instruction that is used to record element-wise operations.
2. =sum=, which is a special instruction that is used to record reductions.

Importantly, there are _no instructions referencing =g= nor =f=!_ But this makes sense: we don't need to know that =sum= occurs in =g=, nor that =broadcast= occurs in =f=, we just need to know that these operations occurred and what their inputs and outputs were. Hence, that's all ReverseDiff.jl records in its tape.

To compute the gradient of =g= wrt. =[1.0]=, the above tape is traversed and each of the instructions has a special pullback that is executed to eventually bring the gradient back to the inputs.

**** Tape compilation

As we will see, ReverseDiff.jl has two "modes" of operation: an interpreted mode and a compiled mode.

The default is the "interpreted" mode, which just means that the tape we saw above is /not/ compiled but interpreted. In effect, it just means that Julia will treat the tape as a =Vector{Any}=, and so every pullback call will be a dynamic dispatch.

This is not ideal, so we can instead compile the tape, which will result in a concretely type =Vector=, where each function is the pullback for the corresponding instruction. This will then allow Julia to do static dispatch, which will be much faster.

The *downside* of the compiled mode is that it does /not/ support conditional statements! Consider the following example:

#+begin_src julia 
h(x) = sum(x) > 0 ? 2 .* x : x
#+end_src

#+RESULTS:
: h (generic function with 1 method)

#+begin_src julia
# `sum(x) > 0` here so we get the first branch
ReverseDiff.GradientTape(h, ([1.0],)).tape
#+end_src

#+RESULTS:
#+begin_example
2-element Vector{ReverseDiff.AbstractInstruction}:
 SpecialInstruction(sum):
  input:  [TrackedReal<DrI>(1.0, 0.0, JTv, 1, 4fG)]
  output: TrackedReal<99F>(1.0, 0.0, JTv, ---)
  cache:  nothing
 SpecialInstruction((broadcast, *)):
  input:  (2,
           [TrackedReal<Bmd>(1.0, 0.0, JTv, 1, 4fG)])
  output: [TrackedReal<Gsu>(2.0, 0.0, JTv, 1, Emj)]
  cache:  (nothing,
           CartesianIndex(1,))
#+end_example

#+begin_src julia
# `sum(x) < 0` here so we get the second branch
ReverseDiff.GradientTape(h, ([-1.0],)).tape
#+end_src

#+RESULTS:
: 1-element Vector{ReverseDiff.AbstractInstruction}:
:  SpecialInstruction(sum):
:   input:  [TrackedReal<4eI>(-1.0, 0.0, FPK, 1, 3Ak)]
:   output: TrackedReal<LoT>(-1.0, 0.0, FPK, ---)
:   cache:  nothing

_The tape changes with the input!_

This is not a problem when we're working in "interpreted mode" because in that scenario, we will re-record the tape everytime we call it on a new input.

But when we work in "compiled mode", we will execute record the tape /once/, compile it, and then execute the same (compiled) tape over and over again /for different inputs/!

Hence, if there are conditionals like the above in our function, the gradient computations could easily end up being incorrect if we decide to compile the tape.

*** TODO Zygote.jl

** Speed

/But/ this was only for =3= items and =100= users, which resulted in =300= observations and it already took us 5s.

What if we had =20= items and =1_000= users, i.e. =20_000= observations?

Sure, we could just run inference and check, but an alternative is to just benchmark the important computations using [[https://github.com/TuringLang/TuringBenchmarking.jl][=TuringBenchmark.jl=]]

#+begin_src julia 
using TuringBenchmarking
#+end_src

#+RESULTS:

Equipped with this, we can easily check the performance of the model

#+begin_src julia :async yes
# Limit ourselves to `ForwardDiff` first since this is what we used above.
benchmark_model(model, adbackends=[:ForwardDiff], verbose=false)
#+end_src

#+RESULTS:
#+begin_example
2-element BenchmarkTools.BenchmarkGroup:
  tags: []
  "evaluation" => 2-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "linked" => Trial(7.647 μs)
	  "standard" => Trial(7.581 μs)
  "gradient" => 1-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "Turing.Essential.ForwardDiffAD{40, true}()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["ForwardDiff"]
		  "linked" => Trial(155.313 μs)
		  "standard" => Trial(155.458 μs)
#+end_example

Here we see that the gradient computation takes roughly 20X the time of the evaluation; this is a useful metric to look at when trying to gauge whether we can gain anything from, say, switching AD backend. 20X is not too crazy, so we will stick with =ForwardDiff= for now.

** How long will =NUTS= take?

Using these numbers we can calculate a rough upper-bound on how long =NUTS= should take.
- =NUTS= with a =max_tree_depth= of =10= (which is the default), means we will /at most/ perform =2^10= leapfrog steps.
- Each leapfrog step requires a single evaluation of the model and its gradient.

Hence, if we want to perform, say, 1000 iterations, then

#+begin_src julia 
using Unitful
#+end_src

#+RESULTS:

#+begin_src julia
nuts = NUTS()
nuts.max_depth
#+end_src

#+RESULTS:
: 10

#+begin_src julia
num_iterations = 1_000
time_per_gradient = 155u"μs"
time_per_iteration = time_per_gradient * 2^nuts.max_depth

# Total sampling time is then:
uconvert(u"s", float(num_iterations * time_per_iteration))
#+end_src

#+RESULTS:
: 158.72 s

Let's just convert the above into a function while we're at it

#+begin_src julia 
function maximum_runtime_nuts(nuts::NUTS, time_per_gradient, num_iterations)
    time_per_iteration = time_per_gradient * 2^nuts.max_depth
    return uconvert(u"s", float(num_iterations * time_per_iteration))
end

maximum_runtime_nuts(nuts, time_per_gradient, num_iterations)
#+end_src

#+RESULTS:
: 158.72 s

Clearly we didn't need ~160s in the above call to =sample=, so we most certainly didn't perform the full =2^10= leapfrog steps at every iteration.

We can inspect this

#+begin_src julia :display text/plain
MCMCChains.get_sections(chain, :internals)
#+end_src

#+RESULTS:
#+begin_example
Chains MCMC chain (1000×12×1 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 1
Samples per chain = 1000
Wall duration     = 4.9 seconds
Compute duration  = 4.9 seconds
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
                    parameters        mean       std      mcse    ess_bulk     ⋯
                        Symbol     Float64   Float64   Float64     Float64     ⋯

                            lp   -299.4536    7.4833    0.3839    378.9357     ⋯
                       n_steps      7.0000    0.0000       NaN         NaN     ⋯
                     is_accept      1.0000    0.0000       NaN         NaN     ⋯
               acceptance_rate      0.8079    0.1646    0.0050   1120.9592     ⋯
                   log_density   -299.4536    7.4833    0.3839    378.9357     ⋯
            hamiltonian_energy    351.1155   10.6849    0.5821    334.9187     ⋯
      hamiltonian_energy_error      0.0025    0.5267    0.0114   2092.6571   1 ⋯
  max_hamiltonian_energy_error      0.2286    0.8456    0.0259   1066.4405     ⋯
                    tree_depth      3.0000    0.0000       NaN         NaN     ⋯
               numerical_error      0.0000    0.0000       NaN         NaN     ⋯
                     step_size      0.5232    0.0000    0.0000         NaN     ⋯
                 nom_step_size      0.5232    0.0000    0.0000         NaN     ⋯
                                                               3 columns omitted

Quantiles
                    parameters        2.5%       25.0%       50.0%       75.0% ⋯
                        Symbol     Float64     Float64     Float64     Float64 ⋯

                            lp   -315.5843   -304.2502   -298.9775   -294.0920 ⋯
                       n_steps      7.0000      7.0000      7.0000      7.0000 ⋯
                     is_accept      1.0000      1.0000      1.0000      1.0000 ⋯
               acceptance_rate      0.4489      0.7020      0.8355      0.9554 ⋯
                   log_density   -315.5843   -304.2502   -298.9775   -294.0920 ⋯
            hamiltonian_energy    332.6281    343.4749    350.6393    357.9514 ⋯
      hamiltonian_energy_error     -1.1342     -0.3142      0.0001      0.3238 ⋯
  max_hamiltonian_energy_error     -1.4096     -0.5314      0.4361      0.8492 ⋯
                    tree_depth      3.0000      3.0000      3.0000      3.0000 ⋯
               numerical_error      0.0000      0.0000      0.0000      0.0000 ⋯
                     step_size      0.5232      0.5232      0.5232      0.5232 ⋯
                 nom_step_size      0.5232      0.5232      0.5232      0.5232 ⋯
                                                                1 column omitted
#+end_example

Here we see that =n_steps= is on average =7= (in fact, it seems to be /constantly/ =7=).
And so we can see that we indeed should be spending much less than ~160s to obtain our chain.

Note that in the above, we have actually performed 1000 + 500 iterations since we have a warmup phase of 500 iterations.
In addition, the above chain does not include the statistics from those initial 500 iterations, so it could be that the average number of leapfrog steps is higher than =7=.

Nonetheless, the above analysis is useful to gaige roughly how long we should expect =NUTS= to take.

So. With this in mind, when we're trying to figure out which way to express our model, we really only need to consider the time it takes to evaluate the log-joint probability and its the gradient.

** Back to benchmarking

Now, let's try and increase to our hypothetical =20= items and =1_000= users as we mentioned before, and see what this does to our runtime.

#+begin_src julia 
P = 1_000   # number of users
I = 20      # number of items
N = I * P   # number of observations
i_and_p = vec(collect(Iterators.product(1:I, 1:P)))
i = map(first, i_and_p); p = map(last, i_and_p);
model_generation = irt(Vector{Union{Missing,Int}}(missing, N), i, p)

# Generate some data.
theta_true, beta_true, y = model_generation()
y = map(identity, y);

# Condition on the data.
model = irt(y, i, p)
#+end_src

#+RESULTS:
: Model(
:   args = (:y, :i, :p)
:   defaults = (:I, :P)
:   context = DynamicPPL.DefaultContext()
: )

#+begin_src julia
benchmark_model(model, adbackends=[:ForwardDiff], verbose=false)
#+end_src

#+RESULTS:
#+begin_example
2-element BenchmarkTools.BenchmarkGroup:
  tags: []
  "evaluation" => 2-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "linked" => Trial(456.722 μs)
	  "standard" => Trial(455.808 μs)
  "gradient" => 1-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "Turing.Essential.ForwardDiffAD{40, true}()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["ForwardDiff"]
		  "linked" => Trial(61.859 ms)
		  "standard" => Trial(61.893 ms)
#+end_example

Okay, so that's quite the increase in the runtime for the gradient evaluation: it went 20X the evaluation time to now 120X the evaluation time!
If we were to run =NUTS= for this model, we would potentially be looking at

#+begin_src julia 
uconvert(u"hr", maximum_runtime_nuts(nuts, 61.859u"ms", num_iterations))
#+end_src

#+RESULTS:
: 17.59544888888889 hr

That's quite long.

** Can we do better?

This far, we've only been using the default =ForwardDiff= backend for automatic differentiation.
As we saw, there was a drastic increase in the difference between the evaluation time and the gradient time once we increased the dimensionality of the problem.
This sort of behavior is very typical for forward-mode automatic differentiation (AD) approaches: these scale poorly with increasing dimensionality in the input (assuming the dimensionality of the output is fixed).

In contrast, reverse-mode AD have, generally speaking, much better scaling properties for the input dimensionality.
And in Julia, we have several reverse-mode AD packages available to us:
- ReverseDiff.jl
- Zygote.jl
- Tracker.jl

Let's give ReverseDiff.jl and Zygote.jl a go and see how they compare to ForwardDiff.jl.

#+begin_src julia 
results = benchmark_model(
    model,
    adbackends=[:ForwardDiff, :ReverseDiff, :ReverseDiff_compiled, :Zygote];
    verbose=false
)
#+end_src

#+RESULTS:
#+begin_example
2-element BenchmarkTools.BenchmarkGroup:
  tags: []
  "evaluation" => 2-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "linked" => Trial(475.519 μs)
	  "standard" => Trial(475.309 μs)
  "gradient" => 4-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "Turing.Essential.ReverseDiffAD{false}()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["ReverseDiff"]
		  "linked" => Trial(17.669 ms)
		  "standard" => Trial(17.599 ms)
	  "Turing.Essential.ReverseDiffAD{true}()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["ReverseDiff"]
		  "linked" => Trial(6.165 ms)
		  "standard" => Trial(5.637 ms)
	  "Turing.Essential.ForwardDiffAD{40, true}()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["ForwardDiff"]
		  "linked" => Trial(65.857 ms)
		  "standard" => Trial(65.289 ms)
	  "Turing.Essential.ZygoteAD()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["Zygote"]
		  "linked" => Trial(1.916 s)
		  "standard" => Trial(1.915 s)
#+end_example

Pffft, here we see /lots/ of variability! We can use =@tagged= from BenchmarkTools.jl, which is re-exported from TuringBenchmarking.jl, to filter out the results we're interested in.

#+begin_src julia 
results[@tagged :Zygote]
#+end_src

#+RESULTS:
: 1-element BenchmarkTools.BenchmarkGroup:
:   tags: []
:   "gradient" => 1-element BenchmarkTools.BenchmarkGroup:
: 	  tags: []
: 	  "Turing.Essential.ZygoteAD()" => 2-element BenchmarkTools.BenchmarkGroup:
: 		  tags: ["Zygote"]
: 		  "linked" => Trial(1.916 s)
: 		  "standard" => Trial(1.915 s)

Wow, this is even /slower/ than ForwardDiff; we're looking at a ~40X increase in the runtime for the gradient computation!

In fact, this is not too surprising: Zygote is, for reasons that are beyond the scope of this tutorial, particularly slow for models involving for-loops, which we do have in our =itr= model.

In contrast, ReverseDiff.jl does a much better job with this model:

#+begin_src julia 
results[@tagged :ReverseDiff]
#+end_src

#+RESULTS:
#+begin_example
1-element BenchmarkTools.BenchmarkGroup:
  tags: []
  "gradient" => 2-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "Turing.Essential.ReverseDiffAD{false}()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["ReverseDiff"]
		  "linked" => Trial(17.669 ms)
		  "standard" => Trial(17.599 ms)
	  "Turing.Essential.ReverseDiffAD{true}()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["ReverseDiff"]
		  "linked" => Trial(6.165 ms)
		  "standard" => Trial(5.637 ms)
#+end_example

Note that we have two different results for ReverseDiff.jl:
- =ReverseDiffAD{false}= is ReverseDiff.jl /without/ compilation of the resulting tape.
- =ReverseDiffAD{true}= is ReverseDiff.jl /with/ compilation of the resulting tape.

As we see above, compilation gets us a ~3X speedup in the gradient computation vs. not compiling the tape, /but/ both approaches are still much, much faster than ForwardDiff.jl (and most certainly Zygote.jl)!

If we redo our calculation for "how long does NUTS take" with =ReverseDiffAD{true}=, we now instead get

#+begin_src julia
uconvert(u"hr", maximum_runtime_nuts(nuts, 6.165u"ms", num_iterations))
#+end_src

#+RESULTS:
: 1.7536 hr

That is /much/ better than 18 hours!

** Can we do better?

So we've identified ReverseDiff.jl with tape-compilation as our best bet for automatic differentiation for this particular model, but can we improve it even further?

For example, can we maybe implement the model in a slightly different way that would make it easier for the AD package to compute the gradient?

*** Avoiding for-loops

#+begin_src julia 
@model function irt_01(y, i, p; I = maximum(i), P = maximum(p))
    theta ~ filldist(Normal(), P)
    beta ~ filldist(Normal(), I)

    # NOTE: We've now replaced the for-loop with a single multivariate distribution
    y ~ product_distribution(Bernoulli.(logistic.(theta[p] .- beta[i])))

    return (; theta, beta, y)
end

model_01 = irt_01(y, i, p)
#+end_src

#+RESULTS:
: Model(
:   args = (:y, :i, :p)
:   defaults = (:I, :P)
:   context = DynamicPPL.DefaultContext()
: )

#+begin_src julia :async yes
results_01 = benchmark_model(
    model_01,
    adbackends=[:ForwardDiff, :ReverseDiff, :ReverseDiff_compiled, :Zygote];
    verbose=false
)
#+end_src

#+RESULTS:
#+begin_example
2-element BenchmarkTools.BenchmarkGroup:
  tags: []
  "evaluation" => 2-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "linked" => Trial(356.451 μs)
	  "standard" => Trial(355.911 μs)
  "gradient" => 4-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "Turing.Essential.ReverseDiffAD{false}()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["ReverseDiff"]
		  "linked" => Trial(10.077 ms)
		  "standard" => Trial(10.758 ms)
	  "Turing.Essential.ReverseDiffAD{true}()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["ReverseDiff"]
		  "linked" => Trial(6.610 ms)
		  "standard" => Trial(4.262 ms)
	  "Turing.Essential.ForwardDiffAD{40, true}()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["ForwardDiff"]
		  "linked" => Trial(131.610 ms)
		  "standard" => Trial(130.713 ms)
	  "Turing.Essential.ZygoteAD()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["Zygote"]
		  "linked" => Trial(61.949 ms)
		  "standard" => Trial(61.957 ms)
#+end_example

Here we see a few differences from our previous benchmarks:
- ForwardDiff.jl is even worse now.
- ReverseDiff.jl is also worse, albeit only slightly!
- Zygote.jl has improved by, well, /a lot/. We went from almost 2s down to ~60ms, which is almost 40X faster!

Now, it's fair to ask the question: is any of surprising?
- The fact that Zygote.jl has improved now that we've removed the for-loop in favour of a multivariate distribution is /not/ surprising; as we said, for-loop are bad for Zygote.jl.
- But it /is/ surprising that ReverseDiff.jl (and ForwardDiff.jl) are now slower than before; we would expect the broadcasted statement to be faster than a for-loop, so why should the gradient computation then suddenly be slower?
  - One thing to notice here is that only the /linked/ version of ReverseDiff.jl is slower; the /standard/ version is actually faster than before. This indicates that the problem lies with the transformations Turing applies to move the parameters from constrained to unconstrained space. 

To understand this, we need to do some _profiling_.

*** Profiling the gradient computation

Julia comes with [[https://docs.julialang.org/en/v1/manual/profile/][built-in profiling capabilities]], but, as mentioned in the official docs, viewing the profiling results is probably best done using an external package.

In the following, I will use [[https://github.com/JuliaPerf/PProf.jl][PProf.jl]] which lets you view the profiling results in a web browser.

First let us profile ReverseDiff.jl, which was the fastest but then got slower once we introduced the array-based distribution, which we intuitlvey expected to be faster.

BenchmarkTools.jl provides us with a convenient way of profiling a function, so let's use that.

#+begin_src julia
# 1. Construct the benchmarking suite for our model with ReverseDiff.jl
suite_01 = TuringBenchmarking.make_turing_suite(
    model_01,
    adbackends=[:ReverseDiff_compiled];
)
#+end_src

#+RESULTS:
#+begin_example
2-element BenchmarkTools.BenchmarkGroup:
  tags: []
  "evaluation" => 2-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "linked" => Benchmark(evals=1, seconds=5.0, samples=10000)
	  "standard" => Benchmark(evals=1, seconds=5.0, samples=10000)
  "gradient" => 1-element BenchmarkTools.BenchmarkGroup:
	  tags: []
	  "Turing.Essential.ReverseDiffAD{true}()" => 2-element BenchmarkTools.BenchmarkGroup:
		  tags: ["ReverseDiff"]
		  "linked" => Benchmark(evals=1, seconds=5.0, samples=10000)
		  "standard" => Benchmark(evals=1, seconds=5.0, samples=10000)
#+end_example

#+begin_src julia
# 2. Extract the benchmark of interest.
b = suite_01["gradient"]["Turing.Essential.ReverseDiffAD{true}()"]["linked"]
#+end_src

#+RESULTS:
: Benchmark(evals=1, seconds=5.0, samples=10000)

#+begin_src julia
using BenchmarkTools, PProf
# 3. Set up the benchmarking environment.
BenchmarkTools.warmup(b)
#+end_src

#+RESULTS:
: BenchmarkTools.Trial: 1 sample with 1 evaluation.
:  Single result which took 7.954 ms (0.00% GC) to evaluate,
:  with a memory estimate of 8.16 KiB, over 2 allocations.

#+begin_src julia
# 4. Clear the profiling data.
BenchmarkTools.Profile.clear()
# 5. Profile the running of the benchmark.
@pprof BenchmarkTools.run(b; seconds=30, gctrial=false, gcsample=false)
#+end_src

#+RESULTS:
: "profile.pb.gz"


* Automatic Differentiation                                        :noexport:
* Conventions                                                      :noexport:

