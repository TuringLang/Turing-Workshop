#+SETUPFILE: ~/org-blog/setup.org
#+OPTIONS: tex:t toc:nil date:nil
#+PROPERTY: header-args:R :session :exports both :eval no
#+PROPERTY: header-args:julia :session geilo-winter-school :tangle tmp.jl :exports both :kernel julia-4-threads-1.8
#+EXCLUDE_TAGS: noexport
#+TODO: TODO(t) TASK(q) WARNING(w) | DONE(d) SOLUTION(s)

#+REVEAL_ROOT: file:///home/tor/Projects/mine/presentations/2021-10-31-turing-demo/assets/reveal.js-4.1.0/
#+REVEAL_MATHJAX_URL: file:///home/tor/Projects/mine/presentations/2021-10-31-turing-demo/assets/MathJax-2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML
#+REVEAL_TITLE_SLIDE: <div><div style="margin: -200px auto; opacity: 0.2;"><p><object data="https://turing.ml/dev/assets/images/turing-logo-wide.svg"></object></p></div><h1>Bayesian inference and other things</h1><h2>with the TuringLang ecosystem</h2><p><a href="https://github.com/TuringLang">https://github.com/TuringLang</a></p></div>
#+REVEAL_EXTRA_CSS: custom.css
#+REVEAL_THEME: white
#+REVEAL_PLUGINS: (markdown zoom)
#+HTML_HEAD: <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

#+AUTHOR: Tor Erlend Fjelde
#+TITLE: =Turing.jl=

* Before we begin

Make sure you're in the correct directory

#+begin_src sh
pwd
#+end_src

#+RESULTS:
: /home/tor/Projects/public/Turing-Workshop/2023-Geilo-Winter-School/Part-2-Turing-and-other-things

Then run something like (depending on which OS you are on)

#+begin_src sh :eval no
julia --project
#+end_src

or if you're already in a REPL, do

#+begin_src julia
]activate .
#+end_src

#+RESULTS:
:   Activating project at `~/Projects/public/Turing-Workshop/2023-Geilo-Winter-School/Part-2-Turing-and-other-things`

to activate the project

#+REVEAL: split

And just to check that you're in the correct one

#+begin_src julia
]status
#+end_src

#+RESULTS:
#+begin_example
Project GeiloWinterSchool2023Part2 v0.1.0
Status `~/Projects/public/Turing-Workshop/2023-Geilo-Winter-School/Part-2-Turing-and-other-things/Project.toml`
  [6e4b80f9] BenchmarkTools v1.3.2
  [336ed68f] CSV v0.10.9
  [a93c6f00] DataFrames v1.4.4
  [2b5f629d] DiffEqBase v6.114.1
  [0c46a032] DifferentialEquations v7.6.0
  [31c24e10] Distributions v0.25.80
  [f6369f11] ForwardDiff v0.10.34
  [6fdf6af0] LogDensityProblems v2.1.0
  [996a588d] LogDensityProblemsAD v1.1.1
  [429524aa] Optim v1.7.4
  [37e2e3b7] ReverseDiff v1.14.4 `https://github.com/torfjelde/ReverseDiff.jl#torfjelde/sort-of-support-non-linear-indexing`
  [0bca4576] SciMLBase v1.81.0
âŒƒ [1ed8b502] SciMLSensitivity v7.17.1
  [f3b207a7] StatsPlots v0.15.4
  [fce5fe82] Turing v0.24.0
  [0db1332d] TuringBenchmarking v0.1.1
  [e88e6eb3] Zygote v0.6.55
Info Packages marked with âŒƒ have new versions available and may be upgradable.
#+end_example

Download and install dependencies

#+begin_src julia 
]instantiate
#+end_src

#+RESULTS:
: [32m  âœ“ [39mGeiloWinterSchool2023Part2
:   1 dependency successfully precompiled in 32 seconds. 356 already precompiled.

#+REVEAL: split

And finally, do

#+begin_src julia 
using GeiloWinterSchool2023Part2
#+end_src

#+RESULTS:

to get some functionality I've implemented for the occasion

* The story of a little Norwegian boy

#+REVEAL: split

There once was a little Norwegian boy

#+DOWNLOADED: file:///home/tor/Downloads/471337_3317365246956_1262712540_o.jpg @ 2023-01-18 14:49:24
#+ATTR_HTML: :height 400px
#+ATTR_ORG: :width 600
[[file:.notes/attachments/A_litle_Norwegian_boy/2023-01-18_14-49-24_471337_3317365246956_1262712540_o.jpg]]


#+REVEAL: split

When this little boy was 20 years old, he was working as a parking guard near Preikestolen/Pulpit rock


#+DOWNLOADED: file:///home/tor/Downloads/Preikestolen-plateau-Go-Fjords-Bob-Engelsen-P1026771_kljg5o.jpeg @ 2023-01-18 14:57:08
#+ATTR_HTML: :height 400px
#+ATTR_ORG: :width 600
[[file:.notes/attachments/A_litle_Norwegian_boy/2023-01-18_14-57-08_Preikestolen-plateau-Go-Fjords-Bob-Engelsen-P1026771_kljg5o.jpeg]]


#+REVEAL: split

One day it was raining and there was nobody hiking, which of course mean that the little boy was bored

When his boss wasn't looking, the little 20 year-old boy had an amazing idea

#+begin_quote
Maybe I can use this method of Mr. Bayes I learned a bit about yesteday to model football / Premier League?
#+end_quote

The little boy got very excited and started looking for stuff on the big interwebs

#+REVEAL: split

The little boy came across this

#+DOWNLOADED: file:///tmp/Spectacle.jWiYMk/Screenshot_20230118_144454.png @ 2023-01-18 14:46:02
[[file:.notes/attachments/A_litle_Norwegian_boy/2023-01-18_14-46-02_Screenshot_20230118_144454.png]]

And got _very_ excited

#+REVEAL: split

But at the time, the little boy knew next to _nothing_ about programming

The little boy couldn't write the code to do the inference

#+ATTR_REVEAL: :frag (appear)
Whence the little boy became a _sad_ little boy :(

#+REVEAL: split

But time heals all wounds, and at some point the little boy learned Python

And in Python, the boy found the /probabilistic programming language/ =pymc3=

#+HTML: <div class="fragment (appear)">
#+begin_quote
Maybe I can use =pymc3= to perform inference in that Premier League model?
#+end_quote

And so the sad boy once more became an excited little boy :)
#+HTML: </div>

#+REVEAL: split

But there was a problem

The boy wanted to write a for-loop in his model, but the model didn't want it to be so and complained!

#+ATTR_REVEAL: :frag (appear)
The boy got frustrated and gave up, once more becoming a _sad_ little boy :(

#+HTML: <div class="small-text">

#+ATTR_REVEAL: :frag (appear)
The boy should have known that the computational backend =theano= that was used by =pymc3= at the time couldn't handle a for-loop, and instead he should have used =scan=. But the boy was only 20-something years old; he didn't know.

#+HTML: </div>

#+REVEAL: split

Some years later the boy discovers a programming language called _Julia_

#+HTML: <div class="fragment (appear)">
Julia makes a few promises
#+ATTR_REVEAL: :frag (appear)
1. It's fast. Like /really/ fast.
2. It's interactive; doesn't require full compilation for you to play with it.
3. You don't have to specify types everywhere.
#+HTML: </div>

#+HTML: <div class="fragment (appear)">
The boy thinks

#+begin_quote
Wait, but this sounds like Python but the only difference is that...I CAN WRITE FOR-LOOPS WITHOUT FEELING BAD ABOUT IT?!
#+end_quote

Yes, yes he could

#+ATTR_REVEAL: :frag (appear)
And 3.5 years later, he's still writing for-loops. Well, sort of.
#+HTML: </div>

** But it really is fast                                           :noexport:


#+DOWNLOADED: file:///tmp/Spectacle.jWiYMk/Screenshot_20230118_153122.png @ 2023-01-18 15:31:28
#+CAPTION: https://julialang.org/benchmarks/ (2023-01-18)
#+ATTR_HTML: :height 400px
#+ATTR_ORG: :width 600
[[file:.notes/attachments/A_litle_Norwegian_boy/2023-01-18_15-31-28_Screenshot_20230118_153122.png]]

#+REVEAL: split

And the consequences are
#+ATTR_REVEAL: :frag (appear)
- Even a naive implementation will be fast; and if you want to go faster, you just optimize the code /in Julia/!
  - No need to drop down to C(++)
- âŸ¹ "Every" package is written in Julia, so when you encounter a bug you can more easily debug because /the code is in the same language as you're writing code in/!
- âŸ¹ Same for /extending/ packages!
  - Moreover, thanks to multiple dispatch, you can change a function /on the fly/ to experiment with code you don't even own!
* Why Turing.jl?

#+REVEAL: split

Duh, you use Turing.jl *so you get to use Julia*

#+HTML: <div class="fragment (appear)">

But even in Julia, other PPLS exist

But Turing.jl is very similar to Julia in "philosophy":
- Flexiblility
- Ease-of-use
- Speed (potentially with a bit of effort)

#+HTML: </div>
* Running example

We'll work with an outbreak of influenza A (H1N1) in 1978 at a British boarding school

- 763 male students -> 512 of which became ill
- Reported that one infected boy started the epidemic
- Observations are number of boys in bed over 14 days

Data are freely available in the R package =outbreaks=, maintained as part of the [[http://www.repidemicsconsortium.org/][R Epidemics Consortium]]

#+HTML: <div class="fragment (appear)">

Data + part of the analysis is /heavily/ inspired by https://mc-stan.org/users/documentation/case-studies/boarding_school_case_study.html

Stan definitively beats Turing.jl when it comes to great write-ups like these

#+HTML: </div>

** Getting the data                                                :noexport:
#+begin_src sh
mkdir -p data
#+end_src

#+begin_src R
install.packages("outbreaks")
#+end_src

#+begin_src R
library(outbreaks)
#+end_src

#+begin_src R
influenza_england_1978_school
#+end_src

#+RESULTS:
| 1978-01-22 |   3 |   0 |
| 1978-01-23 |   8 |   0 |
| 1978-01-24 |  26 |   0 |
| 1978-01-25 |  76 |   0 |
| 1978-01-26 | 225 |   9 |
| 1978-01-27 | 298 |  17 |
| 1978-01-28 | 258 | 105 |
| 1978-01-29 | 233 | 162 |
| 1978-01-30 | 189 | 176 |
| 1978-01-31 | 128 | 166 |
| 1978-02-01 |  68 | 150 |
| 1978-02-02 |  29 |  85 |
| 1978-02-03 |  14 |  47 |
| 1978-02-04 |   4 |  20 |

#+begin_src R
write.csv(influenza_england_1978_school, file="data/influenza_england_1978_school.csv")
#+end_src

** Loading into Julia

#+begin_src julia
# Load the dataframe.
using Dates
using DataFrames, CSV

N = 763
data = DataFrame(CSV.File(joinpath("data", "influenza_england_1978_school.csv")));
print(data)
#+end_src

#+RESULTS:
#+begin_example
[1m14Ã—4 DataFrame[0m
[1m Row [0mâ”‚[1m Column1 [0m[1m date       [0m[1m in_bed [0m[1m convalescent [0m
     â”‚[90m Int64   [0m[90m Date       [0m[90m Int64  [0m[90m Int64        [0m
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   1 â”‚       1  1978-01-22       3             0
   2 â”‚       2  1978-01-23       8             0
   3 â”‚       3  1978-01-24      26             0
   4 â”‚       4  1978-01-25      76             0
   5 â”‚       5  1978-01-26     225             9
   6 â”‚       6  1978-01-27     298            17
   7 â”‚       7  1978-01-28     258           105
   8 â”‚       8  1978-01-29     233           162
   9 â”‚       9  1978-01-30     189           176
  10 â”‚      10  1978-01-31     128           166
  11 â”‚      11  1978-02-01      68           150
  12 â”‚      12  1978-02-02      29            85
  13 â”‚      13  1978-02-03      14            47
  14 â”‚      14  1978-02-04       4            20[1m14Ã—4 DataFrame[0m
[1m Row [0mâ”‚[1m Column1 [0m[1m date       [0m[1m in_bed [0m[1m convalescent [0m
     â”‚[90m Int64   [0m[90m Date       [0m[90m Int64  [0m[90m Int64        [0m
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   1 â”‚       1  1978-01-22       3             0
   2 â”‚       2  1978-01-23       8             0
   3 â”‚       3  1978-01-24      26             0
   4 â”‚       4  1978-01-25      76             0
   5 â”‚       5  1978-01-26     225             9
   6 â”‚       6  1978-01-27     298            17
   7 â”‚       7  1978-01-28     258           105
   8 â”‚       8  1978-01-29     233           162
   9 â”‚       9  1978-01-30     189           176
  10 â”‚      10  1978-01-31     128           166
  11 â”‚      11  1978-02-01      68           150
  12 â”‚      12  1978-02-02      29            85
  13 â”‚      13  1978-02-03      14            47
  14 â”‚      14  1978-02-04       4            20
#+end_example

Notice that each of the columns have associated types

#+REVEAL: split

Let's visualize the samples:

#+begin_src julia
using StatsPlots
#+end_src

#+RESULTS:

#+begin_src julia
# StatsPlots.jl provides this convenient macro `@df` for plotting a `DataFrame`.
@df data scatter(:date, :in_bed, label=nothing, ylabel="Number of students in bed")
#+end_src

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/b3509a2dc6a73f94b6bbc412b904063b0490b9d0.svg]]
[[file:./.ob-jupyter/48b3c1e7b7b7baf2402f518d4371d99a431a56ae.svg]]
:END:

* Differential equations

#+REVEAL: split

Suppose we have some function $f$ which describes how a state $x$ evolves wrt. $t$
\begin{equation*}
\dv{x}{t} = f(x, t)
\end{equation*}
which we then need to integrate to obtain the actual state at some time $t$
\begin{equation*}
x(t) = \int_{0}^{t} \dv{x}{t} \dd{t} = \int_{0}^{t} f(x, t) \dd{t}
\end{equation*}

In many interesting scenarios numerical methods are required to obtain $x(t)$

** In Julia
Everything related to differential equations is provided by =DifferentialEquations.jl=

#+REVEAL: split

And I really do mean [[https://docs.sciml.ai/DiffEqDocs/stable/][/everything/]]

#+HTML: <div class="side-by-side">

#+DOWNLOADED: file:///tmp/Spectacle.jWiYMk/Screenshot_20230119_194737.png @ 2023-01-19 19:48:23
[[file:.notes/attachments/Differential_equations/2023-01-19_19-48-23_Screenshot_20230119_194737.png]]

#+DOWNLOADED: file:///tmp/Spectacle.jWiYMk/Screenshot_20230119_194838.png @ 2023-01-19 19:48:41
[[file:.notes/attachments/Differential_equations/2023-01-19_19-48-41_Screenshot_20230119_194838.png]]

#+HTML: </div>

** Example: SIR model
One particular example of an (ordinary) differential equation that you might have seen recently is the *SIR model* used in epidemiology

#+DOWNLOADED: file:///home/tor/Downloads/sir_illu.png @ 2023-01-19 19:56:00
#+ATTR_ORG: :width 600
#+CAPTION: https://covid19.uclaml.org/model.html (2023-01-19)
[[file:.notes/attachments/Differential_equations/2023-01-19_19-56-00_sir_illu.png]]

#+REVEAL: split

The temporal dynamics of the sizes of each of the compartments are governed by the following system of ODEs:
\begin{equation*}
\begin{split}
  \dv{S}{t} &= - \beta S \frac{I}{N} \\
  \dv{I}{t} &= \beta S \frac{I}{N} - \gamma I \\
  \dv{R}{t} &= \gamma I
\end{split}
\end{equation*}
where
- $S(t)$ is the number of people susceptible to becoming infected (no immunity),
- $I(t)$ is the number of people currently infected (and infectious),
- $R(t)$ is the number of recovered people (we assume they remain immune indefinitely),
- $Î²$ is the constant rate of infectious contact between people,
- $\gamma$ the constant recovery rate of infected individuals

#+REVEAL: split

Converting this ODE into code is just

#+begin_src julia
using DifferentialEquations

function SIR!(
    du,  # buffer for the updated differential equation
    u,   # current state
    p,   # parameters
    t    # current time
)
    N = 763  # population
    S, I, R = u
    Î², Î³ = p

    du[1] = dS = -Î² * I * S / N
    du[2] = dI = Î² * I * S / N - Î³ * I
    du[3] = dR = Î³ * I
end
#+end_src

#+RESULTS:
:RESULTS:
: SIR! (generic function with 1 method)
: SIR! (generic function with 1 method)
:END:


Not too bad!

#+REVEAL: split

#+begin_src julia
# Figure out the timespan we're working with.
last(data.date) - first(data.date) + Day(1) # add 1 since we have an observation for every day
#+end_src

#+RESULTS:
:RESULTS:
: 14 days
: 14 days
:END:

That is, we have 14 days of observations.

#+begin_src julia
# Include 0 because that's the initial condition before any observations.
tspan = (0.0, 14.0)

# Initial conditions are:
#   S(0) = N - 1; I(0) = 1; R(0) = 0
u0 = [N - 1, 1, 0.0]
#+end_src

#+RESULTS:
:RESULTS:
: 3-element Vector{Float64}:
:  762.0
:    1.0
:    0.0
: 3-element Vector{Float64}:
:  762.0
:    1.0
:    0.0
:END:

#+REVEAL: split

Now we just need to define the overall problem and we can solve:

#+begin_src julia
# Just to check that everything works, we'll just use some "totally random" values for Î² and Î³:
problem = let Î² = 2.0, Î³ = 0.6
    ODEProblem(SIR!, u0, tspan, (Î², Î³))
end
#+end_src

#+RESULTS:
:RESULTS:
: [38;2;86;182;194mODEProblem[0m with uType [38;2;86;182;194mVector{Float64}[0m and tType [38;2;86;182;194mFloat64[0m. In-place: [38;2;86;182;194mtrue[0m
: timespan: (0.0, 14.0)
: u0: 3-element Vector{Float64}:
:  762.0
:    1.0
:    0.0
: [38;2;86;182;194mODEProblem[0m with uType [38;2;86;182;194mVector{Float64}[0m and tType [38;2;86;182;194mFloat64[0m. In-place: [38;2;86;182;194mtrue[0m
: timespan: (0.0, 14.0)
: u0: 3-element Vector{Float64}:
:  762.0
:    1.0
:    0.0
:END:

#+REVEAL: split

Aaaand

#+begin_src julia
sol = solve(problem)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
retcode: Success
Interpolation: specialized 4th order "free" interpolation, specialized 2nd order "free" stiffness-aware interpolation
t: 23-element Vector{Float64}:
  0.0
  0.0023558376404244326
  0.025914214044668756
  0.11176872871946908
  0.26714420676761075
  0.47653584778586056
  0.7436981238065388
  1.0701182881347182
  1.4556696154809898
  1.8994815718103506
  2.4015425820305163
  2.9657488203418048
  3.6046024613854746
  4.325611232479916
  5.234036476235002
  6.073132270491685
  7.323851265223563
  8.23100744184026
  9.66046960467715
 11.027717843180652
 12.506967592177675
 13.98890399536329
 14.0
u: 23-element Vector{Vector{Float64}}:
 [762.0, 1.0, 0.0]
 [761.9952867607622, 1.003297407481751, 0.001415831756055325]
 [761.9472927630898, 1.036873767352754, 0.015833469557440357]
 [761.7584189579304, 1.1690001128296739, 0.0725809292398516]
 [761.353498610305, 1.4522140137552049, 0.19428737593979384]
 [760.6490369821046, 1.9447820690728455, 0.4061809488225752]
 [759.3950815454128, 2.8210768113583082, 0.7838416432288186]
 [757.0795798160242, 4.437564277195732, 1.4828559067800167]
 [752.6094742865345, 7.552145919430467, 2.8383797940350495]
 [743.573784947305, 13.823077731564027, 5.603137321131049]
 [724.5575481927715, 26.909267078762316, 11.533184728466205]
 [683.6474029897502, 54.51612001957392, 24.836476990675976]
 [598.1841629858786, 109.41164143668018, 55.40419557744127]
 [450.08652743810205, 192.396449154863, 120.51702340703504]
 [259.11626253270623, 256.9925778114915, 246.89115965580237]
 [148.3573731526537, 240.10301213899098, 374.53961470835543]
 [76.52998017846475, 160.6373332952353, 525.8326865263001]
 [55.70519994004921, 108.7634182279299, 598.531381832021]
 [41.39587834423381, 55.09512088924873, 666.5090007665176]
 [35.87067243374374, 27.821838135708532, 699.3074894305479]
 [33.252184333490774, 13.087185981359177, 716.6606296851502]
 [32.08996839417716, 6.105264616193066, 724.8047669896299]
 [32.08428686823946, 6.070415830241046, 724.8452973015196]
#+end_example
#+begin_example
retcode: Success
Interpolation: specialized 4th order "free" interpolation, specialized 2nd order "free" stiffness-aware interpolation
t: 23-element Vector{Float64}:
  0.0
  0.0023558376404244326
  0.025914214044668756
  0.11176872871946908
  0.26714420676761075
  0.47653584778586056
  0.7436981238065388
  1.0701182881347182
  1.4556696154809898
  1.8994815718103506
  2.4015425820305163
  2.9657488203418048
  3.6046024613854746
  4.325611232479916
  5.234036476235002
  6.073132270491685
  7.323851265223563
  8.23100744184026
  9.66046960467715
 11.027717843180652
 12.506967592177675
 13.98890399536329
 14.0
u: 23-element Vector{Vector{Float64}}:
 [762.0, 1.0, 0.0]
 [761.9952867607622, 1.003297407481751, 0.001415831756055325]
 [761.9472927630898, 1.036873767352754, 0.015833469557440357]
 [761.7584189579304, 1.1690001128296739, 0.0725809292398516]
 [761.353498610305, 1.4522140137552049, 0.19428737593979384]
 [760.6490369821046, 1.9447820690728455, 0.4061809488225752]
 [759.3950815454128, 2.8210768113583082, 0.7838416432288186]
 [757.0795798160242, 4.437564277195732, 1.4828559067800167]
 [752.6094742865345, 7.552145919430467, 2.8383797940350495]
 [743.573784947305, 13.823077731564027, 5.603137321131049]
 [724.5575481927715, 26.909267078762316, 11.533184728466205]
 [683.6474029897502, 54.51612001957392, 24.836476990675976]
 [598.1841629858786, 109.41164143668018, 55.40419557744127]
 [450.08652743810205, 192.396449154863, 120.51702340703504]
 [259.11626253270623, 256.9925778114915, 246.89115965580237]
 [148.3573731526537, 240.10301213899098, 374.53961470835543]
 [76.52998017846475, 160.6373332952353, 525.8326865263001]
 [55.70519994004921, 108.7634182279299, 598.531381832021]
 [41.39587834423381, 55.09512088924873, 666.5090007665176]
 [35.87067243374374, 27.821838135708532, 699.3074894305479]
 [33.252184333490774, 13.087185981359177, 716.6606296851502]
 [32.08996839417716, 6.105264616193066, 724.8047669896299]
 [32.08428686823946, 6.070415830241046, 724.8452973015196]
#+end_example
:END:
#+REVEAL: split

We didn't specify a solver

DifferentialEquations.jl uses =AutoTsit5(Rosenbrock32())= by default 

Which is a composition between

- =Tsit5= (4th order Runge-Kutta), and
- =Rosenbrock32= (3rd order stiff solver)

with automatic switching between the two

#+REVEAL: split

=AutoTsit5(Rosenbrock32())= covers many use-cases well, but see

- https://docs.sciml.ai/DiffEqDocs/stable/solvers/ode_solve/
- https://www.stochasticlifestyle.com/comparison-differential-equation-solver-suites-matlab-r-julia-python-c-fortran/

for more info on choosing a solver

#+REVEAL: split

But this is the resulting solution

#+begin_src julia
plot(
    sol,
    linewidth=2, xaxis="Time in days", label=["Suspectible" "Infected" "Recovered"],
    alpha=0.5, size=(500, 300)
)
scatter!(1:14, data.in_bed, label="Data", color="black")
#+end_src

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/caab965077cb841340c151f9bdf7ebb3e5efac69.svg]]
[[file:./.ob-jupyter/680067ac95a7970fe036b77912126f86e60b00fa.svg]]
:END:

Doesn't really match the data; let's do better

#+REVEAL: split

We can also find the optimal values for $\beta$ and $\gamma$ by just minimizing some loss, e.g. sum-of-squares

\begin{equation*}
\ell(\beta, \gamma) = \sum_{i = 1}^{14} \bigg( F(u_0, t_i;\ \beta, \gamma) - y_i \bigg)^2
\end{equation*}

where $\big( y_i \big)_{i = 1}^{14}$ are the observations, $F$ is the integrated system

#+HTML: <div class="fragment (appear)">

First we define the loss

#+begin_src julia
# Define the loss function.
function loss(problem, p)
    # `remake` just, well, remakes the `problem` with `p` replaced.
    problem = remake(problem, p=p)
    # To ensure we get solutions _exactly_ at the timesteps of interest,
    # i.e. every day we have observations, we use `saveat=1` to tell `solve`
    # to save at every timestep (which is one day).
    sol = solve(problem, saveat=1)
    # Extract the 2nd state, the (I)infected, for the dates with observations.
    sol_for_observed = sol[2, 2:15]
    # Compute the sum-of-squares of the infected vs. data.
  #+end_srcsum(abs2.(sol_for_observed - data.in_bed))
end
#+end_src

#+RESULTS:
:RESULTS:
: loss (generic function with 1 method)
: loss (generic function with 1 method)
:END:

#+HTML: </div>

#+REVEAL: split

And the go-to for optimization in Julia is [[https://julianlsolvers.github.io/Optim.jl/stable/][Optim.jl]]

#+begin_src julia
using Optim
# An alternative to writing `y -> f(x, y)` is `Base.Fix1(f, x)` which
# avoids potential performance issues with global variables (as our `problem` here).
opt = optimize(
    p -> loss(problem, p), # function to minimize
    [0, 0],                # lower bounds on variables
    [Inf, Inf],            # upper bounds on variables
    [2.0, 0.5],            # initial values
    Fminbox(NelderMead())  # optimization alg
) 
#+end_src

#+RESULTS:
#+begin_example
,* Status: success

,* Candidate solution
   Final objective value:     4.116433e+03

,* Found with
   Algorithm:     Fminbox with Nelder-Mead

,* Convergence measures
   |x - x'|               = 0.00e+00 â‰¤ 0.0e+00
   |x - x'|/|x'|          = 0.00e+00 â‰¤ 0.0e+00
   |f(x) - f(x')|         = 0.00e+00 â‰¤ 0.0e+00
   |f(x) - f(x')|/|f(x')| = 0.00e+00 â‰¤ 0.0e+00
   |g(x)|                 = 7.86e+04 â‰° 1.0e-08

,* Work counters
   Seconds run:   3  (vs limit Inf)
   Iterations:    4
   f(x) calls:    565
   âˆ‡f(x) calls:   1
#+end_example

#+REVEAL: split

#+begin_src julia
Î²,#+end_srcm.minimizer(opt)
#+end_src

#+RESULTS:
: 2-element Vector{Float64}:
:  1.6692320164955483
:  0.44348639177622445

#+REVEAL: split

#+begin_src julia
# Solve for the obtained parameters.
problem = remake(problem, p=(Î², Î»))
sol = solve(problem)

# Plot the solution.
plot(sol, linewidth=2, xaxis="Time in days", label=["Suspectible" "Infected" "Recovered"], alpha=0.5)
# And the data.
scatter!(1:14, data.in_bed, label="Data", color="black")
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/9126a373c3e60e83f0cbba8ae5cc0cb6d9bd63e1.svg]]

That's better than our /totally/ "random" guess from earlier!

** Example: SEIR model

Now we're going to add another compartment to our SIR model: the /(E)xposed/ state

\begin{equation*}
\begin{split}
  \dv{S}{t} &= - \beta S \frac{I}{N} \\
  \dv{E}{t} &= \frac{\beta I S}{N} - \sigma E \\
  \dv{I}{t} &= \sigma E - \gamma I \\
  \dv{R}{t} &= \gamma I
\end{split}
\end{equation*}

where we've added a new parameter $\sigma$ describing the fraction of people who develop observable symptoms in this time

** TASK Solve the SEIR model using Julia

#+begin_src julia :eval no
function SEIR!(
    du,  # buffer for the updated differential equation
    u,   # current state
    p,   # parameters
    t    # current time
)
    N = 763  # population

    S, E, I, R = u  # have ourselves an additional state!
    Î², Î³, Ïƒ = p     # and an additional parameter!

    # TODO: Implement yah fool!
    du[1] = nothing
    du[2] = nothing
    du[3] = nothing
    du[4] = nothing
end
#+end_src

*BONUS:* Use =Optim.jl= to find minimizers of sum-of-squares

** SOLUTION Solve the SEIR model using Julia

#+begin_src julia
function SEIR!(
    du,  # buffer for the updated differential equation
    u,   # current state
    p,   # parameters
    t    # current time
)
    N = 763  # population
    S, E, I, R = u  # have ourselves an additional state!
    Î², Î³, Ïƒ = p     # and an additional parameter!

    # Might as well cache these computations.
    Î²SI = Î² * S * I / N
    ÏƒE = Ïƒ * E
    Î³I = Î³ * I

    du[1] = -Î²SI
    du[2] = Î²SI - ÏƒE
    du[3] = ÏƒE - Î³I
    du[4] = Î³I
end
#+end_src

#+RESULTS:
: SEIR! (generic function with 1 method)

#+REVEAL: split

#+begin_src julia
problem_seir = let u0 = [N - 1, 0, 1, 0], Î² = 2.0, Î³ = 0.6, Ïƒ = 0.8
    ODEProblem(SEIR!, u0, tspan, (Î², Î³, Ïƒ))
end
#+end_src

#+RESULTS:
: [38;2;86;182;194mODEProblem[0m with uType [38;2;86;182;194mVector{Int64}[0m and tType [38;2;86;182;194mFloat64[0m. In-place: [38;2;86;182;194mtrue[0m
: timespan: (0.0, 14.0)
: u0: 4-element Vector{Int64}:
:  762
:    0
:    1
:    0

#+begin_src julia
sol_seir = solve(problem_seir, saveat=1)
#+end_src

#+RESULTS:
#+begin_example
retcode: Success
Interpolation: 1st order linear
t: 15-element Vector{Float64}:
  0.0
  1.0
  2.0
  3.0
  4.0
  5.0
  6.0
  7.0
  8.0
  9.0
 10.0
 11.0
 12.0
 13.0
 14.0
u: 15-element Vector{Vector{Float64}}:
 [762.0, 0.0, 1.0, 0.0]
 [760.1497035901518, 1.277915971753478, 1.0158871356490553, 0.5564933024456415]
 [757.5476928906271, 2.425869618233348, 1.6850698824327135, 1.341367608706787]
 [753.081189706403, 4.277014534677882, 2.9468385687120784, 2.6949571902067637]
 [745.3234082630842, 7.455598293492679, 5.155811621098981, 5.065181822323938]
 [731.9851682751213, 12.855816151849933, 8.960337047554939, 9.198678525473571]
 [709.5042941973462, 21.77178343781762, 15.384985521594787, 16.338936843241182]
 [672.8733895183619, 35.77263271085456, 25.88133104438007, 28.472646726403138]
 [616.390571176038, 55.97177756967422, 42.09614416178476, 48.54150709250279]
 [536.453596476594, 81.2428045994271, 64.9673325777641, 80.33626634621449]
 [436.43708330634297, 106.04037246704702, 92.9550757379631, 127.56746848864664]
 [329.60092931771436, 121.08020372279418, 120.48402926084937, 191.83483769864185]
 [233.8471941518982, 119.43669383157659, 139.3233304893263, 270.3927815271987]
 [160.88805352426687, 102.7399386960996, 143.3826208089892, 355.9893869706441]
 [111.72261866282292, 79.02493776169311, 132.78384886713565, 439.46859470834806]
#+end_example

#+REVEAL: split

#+begin_src julia
plot(sol_seir, linewidth=2, xaxis="Time in days", label=["Suspectible" "Exposed" "Infected" "Recovered"], alpha=0.5)
scatter!(1:14, data.in_bed, label="Data")
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/49511097d3c51c1b35c1805ae5db86316a8691c9.svg]]

Don't look so good. Let's try Optim.jl again.

#+REVEAL: split

#+begin_src julia
function loss_seir(problem, p)
    problem = remake(problem, p=p)
    sol = solve(problem, saveat=1)
    # NOTE: 3rd state is now the (I)nfectious compartment!!!
    sol_for_observed = sol[3, 2:15]
    return sum(abs2.(sol_for_observed - data.in_bed))
end
#+end_src

#+RESULTS:
: loss_seir (generic function with 1 method)

#+begin_src julia
opt = optimize(Base.Fix1(loss_seir, problem_seir), [0, 0, 0], [Inf, Inf, Inf], [2.0, 0.5, 0.9], Fminbox(NelderMead()))
#+end_src

#+RESULTS:
#+begin_example
,* Status: success (reached maximum number of iterations)

,* Candidate solution
   Final objective value:     3.115978e+03

,* Found with
   Algorithm:     Fminbox with Nelder-Mead

,* Convergence measures
   |x - x'|               = 0.00e+00 â‰¤ 0.0e+00
   |x - x'|/|x'|          = 0.00e+00 â‰¤ 0.0e+00
   |f(x) - f(x')|         = 0.00e+00 â‰¤ 0.0e+00
   |f(x) - f(x')|/|f(x')| = 0.00e+00 â‰¤ 0.0e+00
   |g(x)|                 = 1.77e+05 â‰° 1.0e-08

,* Work counters
   Seconds run:   1  (vs limit Inf)
   Iterations:    3
   f(x) calls:    13259
   âˆ‡f(x) calls:   1
#+end_example

#+REVEAL: split

#+begin_src julia
Î², Î³, Ïƒ = Optim.minimizer(opt)
#+end_src

#+RESULTS:
: 3-element Vector{Float64}:
:  4.853872993924619
:  0.4671485850111774
:  0.8150294098438762

#+begin_src julia
sol_seir = solve(remake(problem_seir, p=(Î², Î³, Ïƒ)), saveat=1)
plot(sol_seir, linewidth=2, xaxis="Time in days", label=["Suspectible" "Exposed" "Infected" "Recovered"], alpha=0.5)
scatter!(1:14, data.in_bed, label="Data", color="black")
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/3f9b86d79f9b94581ddc14503df40ad7853d95b4.svg]]

#+REVEAL: split

#+begin_quote
But...but these are _point estimates_! What about distributions? WHAT ABOUT UNCERTAINTY?!
#+end_quote

No, no that's fair.

Let's do some Bayesian inference then.

BUT FIRST!

** Making our future selves less annoyed

It's annoying to have all these different loss-functions for /both/ =SIR!= and =SEIR!=

#+HTML: <div class="fragment (appear)">

#+begin_src julia
# Abstract type which we can use to dispatch on.
abstract type AbstractEpidemicProblem end

struct SIRProblem{P} <: AbstractEpidemicProblem
    problem::P
    N::Int
end

function SIRProblem(N::Int; u0 = [N - 1, 1, 0.], tspan = (0, 14), p = [#+end_src)
    return SIRProblem(ODEProblem(SIR!, u0, tspan, p), N)
end
#+end_src

#+RESULTS:
: SIRProblem

Then we can just construct the problem as

#+begin_src julia
sir = SIRProblem(N);
#+end_src

#+RESULTS:

#+HTML: </div>

#+REVEAL: split

And to make it a bit easier to work with, we add some utility functions

#+begin_src julia
# General.
parameters(prob::AbstractEpidemicProblem) = prob.problem.p
initial_state(prob::AbstractEpidemicProblem) = prob.problem.u0
population(prob::AbstractEpidemicProblem) = prob.N

# Specializations.
susceptible(::SIRProblem, u::AbstractMatrix) = u[1, :]
infected(::SIRProblem, u::AbstractMatrix) = u[2, :]
recovered(::SIRProblem, u::AbstractMatrix) = u[3, :]
#+end_src

#+RESULTS:
: recovered (generic function with 1 method)

So that once we've solved the problem, we can easily extract the compartment we want, e.g.

#+begin_src julia
sol = solve(sir.problem, saveat=1)
infected(sir, sol)
#+end_src

#+RESULTS:
#+begin_example
15-element Vector{Float64}:
   1.0
   4.026799533924021
  15.824575905720002
  56.779007685250534
 154.4310579906169
 248.98982384839158
 243.67838619968524
 181.93939659551987
 120.64627375763271
  75.92085282572398
  46.58644927641269
  28.214678599716418
  16.96318676577873
  10.158687874394722
   6.070415830241046
#+end_example

** TASK Implement =SEIRProblem=

#+begin_src julia :eval no
struct SEIRProblem <: AbstractEpidemicProblem
    # ...
end

function SEIRProblem end

susceptible
exposed
infected
recovered
#+end_src

** SOLUTION Implement =SEIRProblem=

#+begin_src julia
struct SEIRProblem{P} <: AbstractEpidemicProblem
    problem::P
    N::Int
end

function SEIRProblem(N::Int; u0 = [N - 1, 0, 1, 0.], tspan = (0, 14), p = [4.5, 0.45, 0.8])
    return SEIRProblem(ODEProblem(SEIR!, u0, tspan, p), N)
end

susceptible(::SEIRProblem, u::AbstractMatrix) = u[1, :]
exposed(::SEIRProblem, u::AbstractMatrix) = u[2, :]
infected(::SEIRProblem, u::AbstractMatrix) = u[3, :]
recovered(::SEIRProblem, u::AbstractMatrix) = u[4, :]
#+end_src

#+RESULTS:
: recovered (generic function with 2 methods)

#+REVEAL: split

Now, given a =problem= and a =sol=, we can query the =sol= for the =infected= state without explicit handling of which =problem= we're working with

#+begin_src julia
seir = SEIRProblem(N);
sol = solve(seir.problem, saveat=1)
infected(seir, sol)
#+end_src

#+RESULTS:
#+begin_example
15-element Vector{Float64}:
   1.0
   1.9941817088874336
   6.958582307202902
  23.9262335176065
  74.23638542794971
 176.98368495653585
 276.06126059898344
 293.92632518571605
 249.92836195453708
 189.07578975511504
 134.2373192679034
  91.82578430804273
  61.38108478932363
  40.42264366743211
  26.357816296754425
#+end_example

** Same =loss= for both!

#+begin_src julia
function loss(problem_wrapper::AbstractEpidemicProblem, p)
    # NOTE: Extract the `problem` from `probl#+end_srcr`.
    problem = remake(problem_wrapper.problem, p=p)
    sol = solve(problem, saveat=1)
    # NOTE: Now this is completely general!
    sol_for_observed = infected(problem_wrapper, sol)[2:end]
    return sum(abs2.(sol_for_observed - data.in_bed))
end
#+end_src

#+RESULTS:
: loss (generic function with 2 methods)

#+begin_src julia 
loss(SIRProblem(N), [2.0, 0.6])
#+end_src

#+RESULTS:
: 50257.83978134881

#+begin_src julia 
loss(SEIRProblem(N), [2.0, 0.6, 0.8])
#+end_src

#+RESULTS:
: 287325.105532706

* Bayesian inference

#+REVEAL: split

First off

#+begin_src julia
using Turing
#+end_src

#+RESULTS:

#+REVEAL: split

This dataset really doesn't have too many observations

#+begin_src julia
nrow(data)
#+end_src

#+RESULTS:
: 14

So reporting a single number for parameters is maybe being a /bit/ too confident

#+REVEAL: split

We'll use the following model
\begin{equation*}
\begin{split}
  \beta &\sim \mathcal{N}_{ + }(2, 1) \\
  \gamma &\sim \mathcal{N}_{ + }(0.4, 0.5) \\
  \phi^{-1} &\sim \mathrm{Exponential}(1/5) \\
   y_i &\sim \mathrm{NegativeBinomial2}\big(F(u_0, t_i;\ \beta, \gamma), \phi \big)
\end{split}
\end{equation*}
where 
- $\big( y_i \big)_{i = 1}^{14}$ are the observations, 
- $F$ is the integrated system, and
- $\phi$ is the over-dispersion parameter.

#+REVEAL: split

#+begin_src julia
plot(
    plot(truncated(Normal(2, 1); lower=0), label=nothing, title="Î²"),
    plot(truncated(Normal(0.4, 0.5); lower=0), label=nothing, title="Î³"),
    plot(Exponential(1/5), label=nothing, title="Ï•â»Â¹"),
    layout=(3, 1)
)
#+end_src

#+RESULTS:

#+REVEAL: split

#+begin_src julia
# `NegativeBinomial` already exists, so let's just make an alternative constructor instead.
function NegativeBinomial2(mean, Ï•)
    p = 1/(1 + mean/Ï•)
    r = Ï•
    return NegativeBinomial(r, p)
end
#+end_src

#+RESULTS:
: NegativeBinomial2 (generic function with 1 method)

#+begin_src julia
# Let's just make sure we didn't do something stupid.
Î¼ = 2; Ï• = 3;
dist = NegativeBinomial2(Î¼, Ï•)
# Source: https://mc-stan.org/docs/2_20/functions-reference/nbalt.html
mean(dist) â‰ˆ Î¼ && var(dist) â‰ˆ Î¼ + Î¼^2 / Ï•
#+end_src

#+RESULTS:
: true

#+REVEAL: split

Can be considered a generalization of =Poisson=

#+begin_src julia
Î¼ = 2.0
anim = @animate for Ï• âˆˆ [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 25.0, 100.0]
    p = plot(size=(500, 300))
    plot!(p, Poisson(Î¼); label="Poisson($Î¼)")
    plot!(p, NegativeBinomial2(Î¼, Ï•), label="NegativeBinomial2($Î¼, $Ï•)")
    xlims!(0, 20); ylims!(0, 0.35);
    p
end
gif(anim, "negative_binomial.gif", fps=2);
#+end_src

#+RESULTS:
: [ Info: Saved animation to /home/tor/Projects/public/Turing-Workshop/2023-Geilo-Winter-School/Part-2-Turing-and-other-things/negative_binomial.gif

[[./negative_binomial.gif]]

#+REVEAL: split

#+begin_src julia
@model function sir_model(
    num_days;                                  # Number of days to model
    tspan = (0.0, float(num_days)),            # Timespan to model
    u0 = [N - 1, 1, 0.0],                      # Initial state
    p0 = [2.0, 0.6],                           # Placeholder parameters
    problem = ODEProblem(SIR!, u0, tspan, p0)  # Create problem once so we can `remake`.
)
    Î² ~ truncated(Normal(2, 1); lower=0)
    Î³ ~ truncated(Normal(0.4, 0.5); lower=0)
    Ï•â»Â¹ ~ Exponential(1/5)
    Ï• = inv(Ï•â»Â¹)

    problem_new = remake(problem, p=[Î², Î³])  # Replace parameters `p`.
    sol = solve(problem_new, saveat=1)       # Solve!

    sol_for_observed = sol[2, 2:num_days + 1]  # Timesteps we have observations for.
    in_bed = Vector{Int}(undef, num_days)
    for i = 1:length(sol_for_observed)
        # Add a small constant to `sol_for_observed` to make things more stable.
        in_bed[i] ~ NegativeBinomial2(sol_for_observed[i] + 1e-5, Ï•)
    end

    # Some quantities we might be interested in.
    return (R0 = Î² / Î³, recovery_time = 1 / Î³, infected = sol_for_observed)
end
#+end_src

#+RESULTS:
: sir_model (generic function with 2 methods)

Let's break it down

#+REVEAL: split

#+begin_src julia :eval no
Î² ~ truncated(Normal(2, 1); lower=0)
Î³ ~ truncated(Normal(0.4, 0.5); lower=0)
Ï•â»Â¹ ~ Exponential(1/5)
Ï• = inv(Ï•â»Â¹)
#+end_src

defines our prior

=truncated= is just a way of restricting the domain of the distribution you pass it

#+REVEAL: split

#+begin_src julia :eval no
problem_new = remake(problem, p=[Î², Î³])  # Replace parameters `p`.
sol = solve(problem_new, saveat=1)       # Solve!
#+end_src

We then remake the problem, now with the parameters =[Î², Î³]= sampled above

Remember the =saveat = 1= ensures we get solution at timesteps =[0, 1, 2, ..., 14]=

#+REVEAL: split

Then we extract the timesteps we have observations for

#+begin_src julia :eval no
sol_for_observed = sol[2, 2:num_days + 1]  # Timesteps we have observations for.
#+end_src

and define what's going to be a likelihood (once we add observations)

#+begin_src julia :eval no
in_bed = Vector{Int}(undef, num_days)
for i = 1:length(sol_for_observed)
    # Add a small constant to `sol_for_observed` to make things more stable.
    in_bed[i] ~ NegativeBinomial2(sol_for_observed[i] + 1e-5, Ï•)
end
#+end_src

#+REVEAL: split

Finally we return some values that might be of interest to

#+begin_src julia :eval no
# Some quantities we might be interested in.
return (R0 = Î² / Î³, recovery_time = 1 / Î³, infected = sol_for_observed)
#+end_src

This is useful for a post-sampling diagnostics, debugging, etc.

#+REVEAL: split

#+begin_src julia
model = sir_model(length(data.in_bed))
#+end_src

#+RESULTS:
: Model(
:   args = (:num_days, :tspan, :u0, :p0, :problem)
:   defaults = (:tspan, :u0, :p0, :problem)
:   context = DynamicPPL.DefaultContext()
: )

The model is just another function, so we can call it to check that it works

#+HTML: <div class="fragment (appear)">

#+begin_src julia
model().infected
#+end_src

#+RESULTS:
#+begin_example
14-element Vector{Float64}:
   2.825264961270304
   7.837717092596966
  20.69593030954763
  48.360933746687124
  88.11351800342295
 111.82293478346571
 100.21466601464007
  70.81459707125047
  43.72880969776908
  25.13830589818062
  13.916126106568317
   7.56132370980261
   4.063980443129785
   2.172405735636434
#+end_example

Hey, it does!

#+HTML: </div>

** Is the prior reasonable?

Before we do any inference, we should check if the prior is reasonable

From domain knowledge we know that (for influenza at least)
#+ATTR_REVEAL: :frag (appear)
- $R_0$ is typically between 1 and 2
- =recovery_time= ($1 / \gamma$) is usually ~1 week

#+HTML: <div class="fragment (appear)">

We want to make sure that your prior belief reflects this knowledge while still being flexible enough to accommodate the observations

#+HTML: </div>

#+REVEAL: split

To check this we'll just simulate some draws from our prior model, i.e. the model /without/ conditioning on =in_bed=

There are two ways to sample form the prior

#+HTML: <div class="fragment (appear)">

#+begin_src julia
# 1. By just calling the `model`, which returns a `NamedTuple` containing the quantities of interest
print(model())
#+end_src

#+RESULTS:
: (R0 = 105.51342057750587, recovery_time = 30.1914342607537, infected = [30.609519425500903, 430.59227554161345, 717.7009429592763, 711.5600299723577, 689.0173694690227, 666.5953381596374, 644.879179061704, 623.8694263038603, 603.5441060488636, 583.8809706315462, 564.8584492042928, 546.4556710719582, 528.652445326909, 511.4292389241275])

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

Or by just calling =sample= using =Prior=

#+begin_src julia
# Sample from prior.
chain_prior = sample(model, Prior(), 10_000);
#+end_src

#+RESULTS:
: [32mSampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:00:02[39m

#+HTML: </div>

#+REVEAL: split

#+begin_src julia :eval no
using StatsPlots

"""
    plot_trajectories!(p::Plots.Plot, chain::MCMCChains.Chains; kwargs...)

Plot trajectories in `chain`.

# Keyword arguents
n = 1000, trajectory_color="#BBBBBB", alpha=1/âˆšn
- `n`: number of trajectories to sample. Default: 1000.
- `trajectory_color`: color of the trajectories to use. Default: "#BBBBBB".
- `alpha`: alpha to use when plotting the trajectories. Default: `1/âˆšn`.
- `include_data`: include the data in the plot. Default: `false`.
- `tspan`: 2-tuple containing first and last time represented by `chain`. Default: (1, 14)
"""
function plot_trajectories!(
    p::Plots.Plot, chain::MCMCChains.Chains;
    n = 1000, include_data=false, tspan=(1, 14),
    trajectory_color="#BBBBBB", alpha=1/âˆšn
)
    # Convert the `chain` into a flattened `Array`.
    chain_arr = Array(chain; append_chains=true)  # Shape: [num_chains Ã— num_iterations, num_params]
    # Sample a subset of indices to use.
    total_num_samples = size(chain_arr, 1)
    indices = rand(1:total_num_samples, n)  # Shape: [n, ]
    # Plot 'em!
    chain_arr = chain_arr[indices, :]  # Shape: [n, num_params]
    for in_bed in eachrow(chain_arr)
        plot!(p, tspan[1]:tspan[2], in_bed, alpha=alpha, color=trajectory_color)
    end

    if include_data
        scatter!(p, 1:14, data.in_bed, label="Data", color="black")
    end

    return p
end

"""
    plot_trajectories(chain::MCMCChains.Chains; kwargs...)

Plot trajectories in `chain`.

See [`plot_trajectories!`](@ref) for more info.
"""
plot_trajectories(chain::MCMCChains.Chains; kwargs...) = plot_trajectories!(plot(), chain; kwargs...)
#+end_src

#+REVEAL: split

#+begin_src julia :eval no
"""
    plot_trajectory_quantiles!(p::Plots.Plot, chain::MCMCChains.Chains; kwargs...)

Plot quantiles of trajectories in `chain`.

By default, the 95% quantiles are used.

# Keyword arguments
- q: quantiles to compute (only three ordered values supported). Default: `[0.025, 0.5, 0.975]`.
- `include_data`: if `true`, include the data in the plot. Default: `false`.
- `tspan`: 2-tuple containing first and last time represented by `chain`. Default: (1, 14)
"""
function plot_trajectory_quantiles!(
    p::Plots.Plot, chain::MCMCChains.Chains;
    q=[0.025, 0.5, 0.975], include_data=false, tspan=(1, 14)
)
    # Compute quantiles.
    qs = Array(DataFrame(quantile(chain; q = q))[:, Not(:parameters)]);

    # Plot!
    num_timesteps = size(qs, 1)
    lb, mid, ub = qs[:, 1], qs[:, 2], qs[:, 3]
    plot!(p, tspan[1]:tspan[2], mid, ribbon=(mid - lb, ub - mid))

    if include_data
        scatter!(p, 1:14, data.in_bed, label="Data", color="black")
    end

    return p
end


"""
    plot_trajectory_quantiles(chain::MCMCChains.Chains; kwargs...)

Plot quantiles of trajectories in `chain`.

See [`plot_trajectory_quantiles!`](@ref) for more info.
"""
plot_trajectory_quantiles(chain::MCMCChains.Chains; kwargs...) = plot_trajectory_quantiles!(plot(), chain; kwargs...)
#+end_src

#+REVEAL: split

Let's have a look at the prior predictive

#+begin_src julia
p = plot(legend=false, size=(600, 300))
plot_trajectories!(p, group(chain_prior, :in_bed); n = 1000)
hline!([N], color="red")
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/f67a83891fd5a17f36751d4471f18ea98da28002.svg]]

#+ATTR_REVEAL: :frag (appear)
For certain values we get n#+end_srcinfected /larger/ than the actual population

#+ATTR_REVEAL: :frag (appear)
But this is includes the randomness from =NegativeBinomial2= likelihood

#+ATTR_REVEAL: :frag (appear)
Maybe more useful to inspect the (I)nfected state from the ODE solution?

#+REVEAL: split

We can also look at the =generated_quantities=, i.e. the values from the =return= statement in our model

#+begin_src julia
quantities_prior = generated_quantities(model, MCMCChains.get_sections(chain_prior, :parameters))
print(quantities_prior[1])
#+end_src

#+RESULTS:
: (R0 = 12.278595504743134, recovery_time = 6.8180946580167285, infected = [5.195906118908742, 26.27661208732607, 117.10575290522013, 342.69897192005413, 522.6649315287999, 537.7784190203737, 487.9601076628196, 427.96446786011944, 371.64411414076966, 321.68066769931005, 278.09268310278196, 240.2870268994555, 207.57132528957226, 179.2883407222227])

This is simply a =Matrix= of =NamedTuple=

#+begin_src julia
typeof(quantities_prior)
#+end_src

#+RESULTS:
: Matrix{NamedTuple{(:R0, :recovery_time, :infected), Tuple{Float64, Float64, Vector{Float64}}}} (alias for Array{NamedTuple{(:R0, :recovery_time, :infected), Tuple{Float64, Float64, Array{Float64, 1}}}, 2})

#+REVEAL: split

We can convert it into a =Chains= using a utility function of mine

#+begin_src julia
chain_quantities_prior = to_chains(quantities_prior);

p = plot(legend=false, size=(600, 300))
plot_trajectories!(p, group(chain_quantities_prior, :infected); n = 1000)
hline!([N], color="red")
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/163b853a07bf527c0c99296b0d686d6013037205.svg]]

#+HTML: <div class="x-small-text">

*NOTE:* =to_chains= is not part of "official" Turing.jl because the =return= can contain /whatever/ you want, and so it's not always possible to convert into a =Chains=

#+HTML: </div>

#+REVEAL: split

And the quantiles for the trajectories

#+begin_src julia
p = plot(legend=false, size=(600, 300))
plot_trajectory_quantiles!(p, group(chain_quantities_prior, :infected))
hline!(p, [N], color="red")
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/6ba4d716967abe71e49015edbc0c26baeaa662f1.svg]]

#+REVEAL: split


#+begin_src julia
print(DataFrame(quantile(chain_quantities_prior[:, [:R0, :recovery_time], :])))
#+end_src

#+RESULTS:
: 2Ã—6 DataFrame
:  Row â”‚ parameters     2.5%      25.0%    50.0%    75.0%    97.5%   
:      â”‚ Symbol         Float64   Float64  Float64  Float64  Float64 
: â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
:    1 â”‚ R0             0.548036  2.0974   3.71824  7.38071  66.6828
:    2 â”‚ recovery_time  0.695667  1.20366  1.86041  3.48945  32.4622

Compare to our prior knowledge of $R_0 \in [1, 2]$ and $(1/\gamma) \approx 1$ for influenza

Do we really need probability mass on $R_0 \ge 10$?

** TASK What's wrong with the current prior?

Here's the SIR model

\begin{equation*}
\begin{split}
  \dv{S}{t} &= - \beta S \frac{I}{N} \\
  \dv{I}{t} &= \beta S \frac{I}{N} - \gamma I \\
  \dv{R}{t} &= \gamma I
\end{split}
\end{equation*}

** SOLUTION Recovery time shouldn't be several years

We mentioned recovery time, which is expressed as $1 / \gamma$, is ~1 week

While we're clearly putting non-zero probability on regions near 0, i.e. /long/ recovery times

#+begin_src julia
plot(truncated(Normal(0.4, 0.5); lower=0), label=nothing, title="Î³", size=(500, 300))
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/0e183191a98453b3cdf5f894ee565a5743596fd0.svg]]

We should probably be putting less probability mass near 0

** SOLUTION $\gamma$ should not be larger than 1

\begin{equation*}
\begin{split}
  \dv{S}{t} &= - \beta S \frac{I}{N} \\
  \dv{I}{t} &= \beta S \frac{I}{N} - {\color{red} \gamma I} \\
  \dv{R}{t} &= {\color{red} \gamma I}
\end{split}
\end{equation*}

If $\gamma > 1$ âŸ¹ (R)ecovered increase by /more/ than the (I)nfected

âŸ¹ _healthy people are recovering_

#+REVEAL: :frag (appear)
Now, I'm no epidemiologist, but that doesn't seem right

#+REVEAL: split

Maybe something like

#+begin_src julia
plot(Beta(2, 5), label="new", size=(500, 300))
plot!(truncated(Normal(0.4, 0.5); lower=0), label="old", color="red")
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/5b8ba7e6fad9ec30f5084bd6c74d3a3c22926670.svg]]

- [X] Bounded at 1
- [X] Allows smaller values (i.e. longer recovery time) but rapidly decreases near zero

** SOLUTION What if $\beta > N$?
Then for $t = 0$ we have
\begin{equation*}
\dv{S}{t} \bigg|_{t = 0} = - \beta S \frac{I}{N} > - N (N - 1) \frac{1}{N} = - (N - 1)
\end{equation*}

i.e. we /immediately/ infect everyone on the very first time-step

Also doesn't seem very realistic

#+REVEAL: split

/But/ under our current prior does this matter?

#+begin_src julia
# â„™(Î² > N) = 1 - â„™(Î² â‰¤ N)
1 - cdf(truncated(Normal(2, 1); lower=0), N)
#+end_src

#+RESULTS:
: 0.0

Better yet

#+begin_src julia
quantile(truncated(Normal(2, 1); lower=0), 0.95)
#+end_src

#+RESULTS:
: 3.6559843567138275

i.e. 95% of the probability mass falls below ~3.65

âŸ¹ Current prior for $\beta$ is fine (âœ“)

#+REVEAL: split

Before we change the prior, let's also make it a bit easier to change the prior using =@submodel=

#+HTML: <div class="fragment (appear)">

=@submodel= allows you call models within models, e.g.

#+begin_src julia
@model function A()
    x_hidden_from_B ~ Normal()
    x = x_hidden_from_B + 100
    return x
end

@model function B()
    @submodel x = A()
    y ~ Normal(x, 1)

    return (; x, y)
end
#+end_src

#+RESULTS:
: B (generic function with 2 methods)

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

#+begin_src julia
# So if we call `B` we only see `x` and `y`
println(B()())
#+end_src

#+RESULTS:
: (x = 100.93569737229646, y = 100.91546059962127)

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

#+begin_src julia
# While if we sample from `B` we get the latent variables
println(rand(B()))
#+end_src

#+RESULTS:
: (x_hidden_from_B = -0.7029882097274394, y = 102.07903748938203)

#+HTML: </div>

#+REVEAL: split

And if you want to make sure you avoid clashes of variable-names, we can specify a =prefix=

#+begin_src julia
@model A() = (x ~ Normal(); return x + 100)

@model function B()
    # Given it a prefix to use for the variables in `A`.
    @submodel prefix=:inner x_inner = A()
    x ~ Normal(x_inner, 1)

    return (; x_inner, x)
end
#+end_src

#+RESULTS:
: B (generic function with 2 methods)

#+begin_src julia
print(rand(B()))
#+end_src

#+RESULTS:
: (var"inner.x" = -0.20502868778446368, x = 100.25542669778142)

#+REVEAL: split

=@submodel= is useful as it allows you to:
1. Easy to swap out certain parts of your model.
2. Can re-use models across projects and packages.

When working on larger projects, this really shines

#+REVEAL: split

Equipped with =@submodel= we can replace

#+begin_src julia :eval no
Î² ~ truncated(Normal(2, 1); lower=0)
Î³ ~ truncated(Normal(0.4, 0.5); lower=0)
#+end_src

#+RESULTS:
: eca43919-60be-4de0-9fa9-a745e4ac5588

with

#+begin_src julia :eval no
@submodel p = prior(problem_wrapper)
#+end_src

#+HTML: <div class="fragment (appear)">

where =prior= can be something like

#+begin_src julia
@model function prior_original(problem_wrapper::SIRProblem)
    Î² ~ truncated(Normal(2, 1); lower=0)
    Î³ ~ truncated(Normal(0.4, 0.5); lower=0)

    return [Î², Î³]
end

@model function prior_improved(problem_wrapper::SIRProblem)
    # NOTE: Should probably also lower mean for `Î²` since
    # more probability mass on small `Î³` âŸ¹ `R0 =  Î² / Î³` grows.
    Î² ~ truncated(Normal(1, 1); lower=0)
    # NOTE: New prior for `Î³`.
    Î³ ~ Beta(2, 5)

    return [Î², Î³]
end
#+end_src

#+RESULTS:
: prior_improved (generic function with 2 methods)

#+HTML: </div>

#+REVEAL: split

#+begin_src julia
@model function epidemic_model(
    problem_wrapper::AbstractEpidemicProblem,
    prior  # NOTE: now we just pass the prior as an argument
)
    # NOTE: And use `@submodel` to embed the `prior` in our model.
    @submodel p = prior(problem_wrapper)

    Ï•â»Â¹ ~ Exponential(1/5)
    Ï• = inv(Ï•â»Â¹)

    problem_new = remake(problem_wrapper.problem, p=p)  # Replace parameters `p`.
    sol = solve(problem_new, saveat=1)                  # Solve!

    # Extract the `infected`.
    sol_for_observed = infected(problem_wrapper, sol)[2:end]

    # NOTE: `arraydist` is faster for larger dimensional problems,
    # and it does not require explicit allocation of the vector.
    in_bed ~ arraydist(NegativeBinomial2.(sol_for_observed .+ 1e-5, Ï•))

    Î², Î³ = p[1:2]
    return (R0 = Î² / Î³, recovery_time = 1 / Î³, infected = sol_for_observed, in_bed = in_bed)
end
#+end_src

#+RESULTS:
: epidemic_model (generic function with 2 methods)

#+REVEAL: split

#+HTML: <div class="x-small-text">

Another neat trick is to return early if integration fail

#+HTML: </div>

#+begin_src julia
@model function epidemic_model(
    problem_wrapper::AbstractEpidemicProblem,
    prior  # now we just pass the prior as an argument
)
    # And use `@submodel` to embed the `prior` in our model.
    @submodel p = prior(problem_wrapper)

    Ï•â»Â¹ ~ Exponential(1/5)
    Ï• = inv(Ï•â»Â¹)

    problem_new = remake(problem_wrapper.problem, p=p)  # Replace parameters `p`.
    sol = solve(problem_new, saveat=1)                  # Solve!

    # NOTE: Return early if integration failed.
    if !issuccess(sol)
        Turing.@addlogprob! -Inf  # NOTE: Causes automatic rejection.
        return nothing
    end

    # Extract the `infected`.
    sol_for_observed = infected(problem_wrapper, sol)[2:end]

    # `arraydist` is faster for larger dimensional problems,
    # and it does not require explicit allocation of the vector.
    in_bed ~ arraydist(NegativeBinomial2.(sol_for_observed .+ 1e-5, Ï•))

    Î², Î³ = p[1:2]
    return (R0 = Î² / Î³, recovery_time = 1 / Î³, infected = sol_for_observed)
end
#+end_src

#+RESULTS:
: epidemic_model (generic function with 2 methods)

#+REVEAL: split

Equipped with this we can now easily construct /two/ models using different priors

#+begin_src julia
sir = SIRProblem(N);
model_original = epidemic_model(sir, prior_original);
model_improved = epidemic_model(sir, prior_improved);
#+end_src

#+RESULTS:

but using the same underlying =epidemic_model=

#+begin_src julia
chain_prior_original = sample(model_original, Prior(), 10_000; progress=false);
chain_prior_improved = sample(model_improved, Prior(), 10_000; progress=false);
#+end_src

#+RESULTS:

#+REVEAL: split

Let's compare the resulting priors over some of the quantities of interest

#+REVEAL: split

Let's compare the =generated_quantities=, e.g. $R_0$

#+HTML: <div class="small-text">

#+begin_src julia
chain_quantities_original = to_chains(
    generated_quantities(
        model_original,
        MCMCChains.get_sections(chain_prior_original, :parameters)
    );
);

chain_quantities_improved = to_chains(
    generated_quantities(
        model_improved,
        MCMCChains.get_sections(chain_prior_improved, :parameters)
    );
);
#+end_src

#+RESULTS:

#+HTML: </div>

#+begin_src julia
p = plot(; legend=false, size=(500, 200))
plot_trajectories!(p, group(chain_quantities_original, :infected); n = 100, trajectory_color="red")
plot_trajectories!(p, group(chain_quantities_improved, :infected); n = 100, trajectory_color="blue")
hline!([N], color="red", linestyle=:dash)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/ec83804c27a283b0e15719b58b966e1e90f38ea6.svg]]

#+REVEAL: split

#+begin_src julia
plt1 = plot(legend=false)
plot_trajectory_quantiles!(plt1, group(chain_quantities_original, :infected))
hline!(plt1, [N], color="red", linestyle=:dash)

plt2 = plot(legend=false)
plot_trajectory_quantiles!(plt2, group(chain_quantities_improved, :infected))
hline!(plt2, [N], color="red", linestyle=:dash)

plot(plt1, plt2, layout=(2, 1))
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/e6ef94aa85d70a44eb6f43a9b493a6ccb2c5adce.svg]]

This makes sense: if half of the population is immediately infected âŸ¹ number of infected tapers wrt. time as they recover

#+REVEAL: split

For =model_improved= we then have

#+begin_src julia
print(DataFrame(quantile(chain_quantities_improved[:, [:R0, :recovery_time], :])))
#+end_src

#+RESULTS:
: 2Ã—6 DataFrame
:  Row â”‚ parameters     2.5%      25.0%    50.0%    75.0%    97.5%   
:      â”‚ Symbol         Float64   Float64  Float64  Float64  Float64 
: â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
:    1 â”‚ R0             0.293029  2.2265   4.42959  8.30344  33.7325
:    2 â”‚ recovery_time  1.57205   2.59292  3.82319  6.32029  23.6731

Compare to =model_original=

#+begin_src julia
print(DataFrame(quantile(chain_quantities_original[:, [:R0, :recovery_time], :])))
#+end_src

#+RESULTS:
: 2Ã—6 DataFrame
:  Row â”‚ parameters     2.5%      25.0%    50.0%    75.0%    97.5%   
:      â”‚ Symbol         Float64   Float64  Float64  Float64  Float64 
: â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
:    1 â”‚ R0             0.496952  2.13455  3.77299  7.43143  66.5364
:    2 â”‚ recovery_time  0.695163  1.21459  1.87825  3.51528  29.118

** TASK Make =epidemic_model= work for =SEIRProblem=
1. [ ] Implement a prior which also includes $\sigma$ and execute
   =epidemic_model= with it
2. [ ] Can we make a better prior for $\sigma$? Do we even need one?

#+begin_src julia :eval no
@model function prior_original(problem_wrapper::SEIRProblem)
    # TODO: Implement
end
#+end_src

** SOLUTION
#+begin_src julia
@model function prior_original(problem_wrapper::SEIRProblem)
    Î² ~ truncated(Normal(2, 1); lower=0)
    Î³ ~ truncated(Normal(0.4, 0.5); lower=0)
    Ïƒ ~ truncated(Normal(0.8, 0.5); lower=0)

    return [Î², Î³, Ïƒ]
end
#+end_src

#+RESULTS:
: prior_original (generic function with 4 methods)

#+begin_src julia
model_seir = epidemic_model(SEIRProblem(N), prior_original)
print(model_seir())
#+end_src

#+RESULTS:
: (R0 = 95.80419931156645, recovery_time = 27.304012395796352, infected = [1.6371339022556468, 4.098976186100829, 10.73296565753573, 27.644530276022106, 67.61991732070156, 147.91379180747037, 268.4632180310939, 393.0596786087126, 486.52761445004467, 544.4539174283068, 575.7674250445274, 589.1694092956135, 590.8194898841384, 584.8625962624448])

** WARNING Consult with domain experts
This guy should not be the one setting your priors!

#+ATTR_HTML: :height 400px
#+ATTR_ORG: :width 600
[[file:.notes/attachments/A_litle_Norwegian_boy/2023-01-18_14-49-24_471337_3317365246956_1262712540_o.jpg]]

Get an actual scientist to do that...

** Condition
Now let's actually involve the data

#+HTML: <div class="fragment (appear)">

We can condition a =Model= as so

#+begin_src julia
# Condition on the observations.
model = epidemic_model(SIRProblem(N), prior_improved)
model_conditioned = model | (in_bed = data.in_bed,)
#+end_src

#+RESULTS:
: Model(
:   args = (:problem_wrapper, :prior)
:   defaults = ()
:   context = ConditionContext((in_bed = [3, 8, 26, 76, 225, 298, 258, 233, 189, 128, 68, 29, 14, 4],), DynamicPPL.DefaultContext())
: )

#+HTML: </div>

#+HTML: <div class="fragment (appear)">

You know what time it is: /inference time/!

#+HTML: </div>


** Metropolis-Hastings (MH)

#+begin_src julia
chain_mh = sample(model_conditioned, MH(), MCMCThreads(), 10_000, 4; discard_initial=5_000);
#+end_src

#+RESULTS:

#+begin_src julia
chain_mh
#+end_src

#+RESULTS:
#+begin_example
Chains MCMC chain (10000Ã—4Ã—4 Array{Float64, 3}):

Iterations        = 5001:1:15000
Number of chains  = 4
Samples per chain = 10000
Wall duration     = 9.09 seconds
Compute duration  = 35.0 seconds
parameters        = Î², Î³, Ï•â»Â¹
internals         = lp

Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m naive_se [0m [1m    mcse [0m [1m      ess [0m [1m    rhat [0m [1m e[0m â‹¯
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m  [0m â‹¯

           Î²    1.7321    0.0547     0.0003    0.0029   194.5516    1.0379     â‹¯
           Î³    0.5327    0.0452     0.0002    0.0026   174.0121    1.0091     â‹¯
         Ï•â»Â¹    0.1440    0.0721     0.0004    0.0040   178.1756    1.0198     â‹¯
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m   25.0% [0m [1m   50.0% [0m [1m   75.0% [0m [1m   97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m

           Î²    1.6367    1.6889    1.7309    1.7644    1.8515
           Î³    0.4482    0.4995    0.5329    0.5621    0.6230
         Ï•â»Â¹    0.0420    0.0889    0.1286    0.1801    0.3128
#+end_example

Rhat is /okay-ish/ but not great, and ESS is pretty low innit?

#+begin_src julia
plot(chain_mh; size=(800, 500))
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/a0a1a86169de78133ebe32cbbf92932e715d7b72.svg]]

Eeehh doesn't look the greatest

Difficult to trust these results, but let's check if it at least did
/something/ useful

#+begin_src julia
# We're using the unconditioned model!
predictions_mh = predict(model, chain_mh)
#+end_src

#+RESULTS:
#+begin_example
Chains MCMC chain (10000Ã—14Ã—4 Array{Float64, 3}):

Iterations        = 1:1:10000
Number of chains  = 4
Samples per chain = 10000
parameters        = in_bed[1], in_bed[2], in_bed[3], in_bed[4], in_bed[5], in_bed[6], in_bed[7], in_bed[8], in_bed[9], in_bed[10], in_bed[11], in_bed[12], in_bed[13], in_bed[14]
internals         = 

Summary Statistics
 [1m parameters [0m [1m     mean [0m [1m     std [0m [1m naive_se [0m [1m    mcse [0m [1m        ess [0m [1m    rhat [0m
 [90m     Symbol [0m [90m  Float64 [0m [90m Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m    Float64 [0m [90m Float64 [0m

   in_bed[1]     3.3144    2.2316     0.0112    0.0181   23614.4245    1.0002
   in_bed[2]    10.8587    5.5427     0.0277    0.0822    3161.7697    1.0016
   in_bed[3]    33.9237   15.7850     0.0789    0.3435    1244.8786    1.0023
   in_bed[4]    92.2374   42.1648     0.2108    1.0907     884.2866    1.0028
   in_bed[5]   186.2201   80.9130     0.4046    1.8413    1161.6561    1.0023
   in_bed[6]   247.5569   99.7724     0.4989    1.5551    2849.9161    1.0005
   in_bed[7]   235.2958   93.5116     0.4676    1.1572    6052.0211    1.0002
   in_bed[8]   184.0908   73.5063     0.3675    0.9943    3933.1202    1.0013
   in_bed[9]   131.3395   53.9336     0.2697    0.8872    2474.1635    1.0028
  in_bed[10]    88.3932   37.4812     0.1874    0.7072    1785.3656    1.0037
  in_bed[11]    58.6672   26.0273     0.1301    0.5462    1387.1217    1.0049
  in_bed[12]    38.0125   17.5818     0.0879    0.3822    1423.6268    1.0041
  in_bed[13]    24.5957   12.2420     0.0612    0.2964    1076.1571    1.0052
  in_bed[14]    15.9224    8.4410     0.0422    0.2125    1024.1254    1.0059

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m    25.0% [0m [1m    50.0% [0m [1m    75.0% [0m [1m    97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m

   in_bed[1]    0.0000     2.0000     3.0000     5.0000     8.0000
   in_bed[2]    2.0000     7.0000    10.0000    14.0000    24.0000
   in_bed[3]   10.0000    23.0000    32.0000    42.0000    72.0000
   in_bed[4]   30.0000    63.0000    86.0000   113.0000   193.0000
   in_bed[5]   64.0000   131.0000   174.0000   228.0000   381.0000
   in_bed[6]   91.0000   179.0000   235.0000   301.0000   479.0000
   in_bed[7]   87.0000   171.0000   224.0000   285.0000   454.0000
   in_bed[8]   68.0000   134.0000   175.0000   223.0000   355.0000
   in_bed[9]   46.0000    95.0000   125.0000   160.0000   258.0000
  in_bed[10]   30.0000    63.0000    83.0000   108.0000   176.0000
  in_bed[11]   20.0000    41.0000    55.0000    72.0000   119.0000
  in_bed[12]   12.0000    26.0000    35.0000    47.0000    79.0000
  in_bed[13]    7.0000    16.0000    23.0000    31.0000    53.0000
  in_bed[14]    4.0000    10.0000    15.0000    20.0000    36.0000
#+end_example

#+begin_src julia
plot_trajectories!(plot(legend=false), predictions_mh; include_data=true)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: UndefVarError: data not defined
: 
: Stacktrace:
:  [1] plot_trajectories!(p::Plots.Plot{Plots.GRBackend}, chain::Chains{Float64, AxisArrays.AxisArray{Float64, 3, Array{Float64, 3}, Tuple{AxisArrays.Axis{:iter, StepRange{Int64, Int64}}, AxisArrays.Axis{:var, Vector{Symbol}}, AxisArrays.Axis{:chain, UnitRange{Int64}}}}, Missing, NamedTuple{(:parameters, :internals), Tuple{Vector{Symbol}, Vector{Symbol}}}, NamedTuple{(:start_time, :stop_time), Tuple{Vector{Nothing}, Vector{Nothing}}}}; n::Int64, include_data::Bool, tspan::Tuple{Int64, Int64}, trajectory_color::String, alpha::Float64)
:    @ GeiloWinterSchool2023Part2 ~/Projects/public/Turing-Workshop/2023-Geilo-Winter-School/Part-2-Turing-and-other-things/src/GeiloWinterSchool2023Part2.jl:131
:  [2] top-level scope
:    @ In[111]:1
:END:

#+begin_src julia
plot_trajectory_quantiles!(plot(legend=false, size=(600, 300)), predictions_mh; include_data=true)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: UndefVarError: data not defined
: 
: Stacktrace:
:  [1] plot_trajectory_quantiles!(p::Plots.Plot{Plots.GRBackend}, chain::Chains{Float64, AxisArrays.AxisArray{Float64, 3, Array{Float64, 3}, Tuple{AxisArrays.Axis{:iter, StepRange{Int64, Int64}}, AxisArrays.Axis{:var, Vector{Symbol}}, AxisArrays.Axis{:chain, UnitRange{Int64}}}}, Missing, NamedTuple{(:parameters, :internals), Tuple{Vector{Symbol}, Vector{Symbol}}}, NamedTuple{(:start_time, :stop_time), Tuple{Vector{Nothing}, Vector{Nothing}}}}; q::Vector{Float64}, include_data::Bool, tspan::Tuple{Int64, Int64})
:    @ GeiloWinterSchool2023Part2 ~/Projects/public/Turing-Workshop/2023-Geilo-Winter-School/Part-2-Turing-and-other-things/src/GeiloWinterSchool2023Part2.jl:171
:  [2] top-level scope
:    @ In[112]:1
:END:

Okay, it's not /completely/ useless, but my trust-issues are still
present.

Metropolis-Hastings have disappointed me one too many times before.

Before we proceed, let's just make some functions for the visualizations

** So instead, let's go =NUTS=
That's right, we're reaching to the *No U-Turn sampler (NUTS)*

*** TODO

[[https://chi-feng.github.io/mcmc-demo/app.html][https://chi-feng.github.io/mcmc-demo/app.html]]

** 

#+BEGIN_QUOTE
Wooaah there! =NUTS= requires gradient information!

How are you going to get that through that =solve=?
#+END_QUOTE

Good question, voice in my head

I'm obviously not going to it myself

** Automatic differentiation (AD) in Julia
- [[https://github.com/JuliaDiff/ForwardDiff.jl][ForwardDiff.jl]]: forward-mode AD /(default in Turing.jl)/
- [[https://github.com/JuliaDiff/ReverseDiff.jl][ReverseDiff.jl]]: tape-based reverse-mode AD
- [[https://github.com/FluxML/Zygote.jl][Zygote.jl]]: source-to-source reverse-mode AD
- And more...

Up-and-coming

- [[https://github.com/EnzymeAD/Enzyme.jl][Enzyme.jl]]: Julia bindings for [[https://github.com/EnzymeAD/Enzyme.jl][Enzyme]] which ADs LLVM (low-level)
- [[https://github.com/JuliaDiff/Diffractor.jl][Diffractor.jl]]: experimental mixed-mode AD meant to replace Zygote.jl

Of importance
- [[https://github.com/JuliaDiff/ChainRulesCore.jl][ChainRulesCore.jl]]: light-weight package for defining rules, compatible with many of the above

*Important*

#+BEGIN_QUOTE
When you write code, you don't have to make a choice which one you
want to use!
#+END_QUOTE

All the (stable) ones, will (mostly) work

/But/ how you write code will affect performance characteristics

Takes a bit of know-how + a bit of digging to go properly "vroom!"

** Differentiating through =solve=
With that being said, differentiating through numerical =solve= is not necessarily trivial to do efficiently

There are numerous ways of approaching this problem

[[file:.notes/attachments/Bayesian_inference/2023-01-22_12-30-07_Screenshot_20230122_122936.png]]

[[https://arxiv.org/abs/1812.01892][https://arxiv.org/abs/1812.01892]] is /great/ resource

#+HTML: <div class="fragment (appear)">

But this is why we have [[https://github.com/SciML/SciMLSensitivity.jl][=SciMLSensitivity.jl=]]

[[https://docs.sciml.ai/SciMLSensitivity/stable/manual/differential_equation_sensitivities/#Choosing-a-Sensitivity-Algorithm][SciMLSensitivity.jl docs]] also provides a great overview of different approaches

#+HTML: </div>

#+begin_src julia
using SciMLSensitivity
#+end_src

#+RESULTS:
: 911ff56c-149b-431c-8362-84c07e10b24c

let's you  make use of the =sensealg= keyword in =solve= to choose
whatever approach you need

#+begin_src julia :eval no
solve()
#+end_src

It offers

1. /Discrete sensitivity analysis/ or the /"Direct" method/: just use
   =ForwardDiff.Dual= in the =solve=.
2. /Continuous local sensitivity analysis (CSA)/: extends the original
   system such that the =solve= gives you both the solution and the the
   gradient simultaenously.
3. /Adjoint methods/: construct a backwards system whose solution gives
   us the gradient.

   - Here you can use anything from symbolically derived to "AD-derived"
     jacobians.

** Back to being =NUTS=
   :PROPERTIES:
   :CUSTOM_ID: back-to-being-nuts
   :END:

#+begin_src julia
chain = sample(model_conditioned, NUTS(0.8), MCMCThreads(), 1000, 4);
#+end_src

#+begin_src julia
chain
#+end_src

Muuuch better! Both ESS and Rhatc is looking good

#+begin_src julia
plot(chain; size=(800, 500))
#+end_src

[[./.ob-jupyter/f198b9606c06a59ed28e9000e05a436b045fda5c.png]]

#+begin_src julia
# Predict using the results from NUTS.
predictions = predict(model, chain)
#+end_src

#+begin_src julia
plot_trajectories!(plot(legend=false, size=(600, 300)), predictions; n = 1000, include_data=true)
#+end_src

[[./.ob-jupyter/5818a2844c7cdbb2eaa4ad8c8cf1e0756273a161.png]]

#+begin_src julia
plot_trajectory_quantiles!(plot(legend=false, size=(600, 300)), predictions; include_data=true)
#+end_src

[[./.ob-jupyter/813c664e623143776099024e0e736e4fe49800d5.png]]

** Simulation-based calibration (SBC)
[[https://arxiv.org/abs/1804.06788][Talts et. al. (2018)]]
   :PROPERTIES:
   :CUSTOM_ID: simulation-based-calibration-sbc-talts-et-al-2018
   :END:

1. Sample from prior (\theta_1, \dots, \theta_n \sim p(\theta)).
2. Sample datasets (\mathcal{D}_i \sim p(\cdot \mid \theta_i)) for (i =
   1, \dots, n).
3. Obtain (approximate) (p(\theta \mid \mathcal{D}_i)) for (i = 1,
   \dots, n).

For large enough (n), the "combination" of the posteriors should recover
the prior!

"Combination" here usually means computing some statistic and comparing
against what it should be

[[file:.notes/attachments/Bayesian_inference/2023-01-22_12-09-24_Screenshot_20230122_120848.png]]

That's very expensive â†’ in practice we just do this once or twice

#+begin_src julia
# Sample from the conditioned model so we don't get the `in_bed` variables too
using Random  # Just making usre the numbers of somewhat interesting
rng = MersenneTwister(43);
test_values = rand(rng, NamedTuple, model_conditioned)
#+end_src

Now we condition on those values and run once to generate data

#+begin_src julia
model_test = model | test_values
#+end_src

#+begin_src julia
in_best_test = rand(rng, model_test).in_bed;
#+end_src

Next, inference!

#+begin_src julia
model_test_conditioned = model | (in_bed = in_best_test,)
#+end_src

#+begin_src julia
# Let's just do a single chain here.
chain_test = sample(model_test_conditioned, NUTS(0.8), 1000);
#+end_src

Did we recover the parameters?

#+begin_src julia
ps = []
for sym in [:Î², :Î³, :Ï•â»Â¹]
    p = density(chain_test[:, [sym], :])
    vline!([test_values[sym]])
    push!(ps, p)
end
plot(ps..., layout=(3, 1), size=(600, 400))
#+end_src

[[./.ob-jupyter/c7f5262cc2962e2920df901926b9172e5bbfb589.png]]

** TODO Choosing AD backend in Turing.jl
   :PROPERTIES:
   :CUSTOM_ID: todo-choosing-ad-backend-in-turingjl
   :END:


* Benchmarking                                                     :noexport:
#+begin_src julia
using BenchmarkTools
using TuringBenchmarking
#+end_src

#+RESULTS:

#+begin_src julia
using ReverseDiff, Zygote
#+end_src

#+RESULTS:

#+begin_src julia
suite = TuringBenchmarking.make_turing_suite(
    model_conditioned;
    adbackends=[Turing.Essential.ForwardDiffAD{40,true}(),]
);
run(suite)
#+end_src

#+RESULTS:
: 2-element BenchmarkTools.BenchmarkGroup:
:   tags: []
:   "linked" => 2-element BenchmarkTools.BenchmarkGroup:
: 	  tags: []
: 	  "evaluation" => Trial(29.706 Î¼s)
: 	  "Turing.Essential.ForwardDiffAD{40, true}()" => Trial(116.476 Î¼s)
:   "not_linked" => 2-element BenchmarkTools.BenchmarkGroup:
: 	  tags: []
: 	  "evaluation" => Trial(30.364 Î¼s)
: 	  "Turing.Essential.ForwardDiffAD{40, true}()" => Trial(139.420 Î¼s)

** Model v2
#+begin_src julia
using SciMLSensitivity
#+end_src

#+RESULTS:

#+begin_src julia
value(x) = DiffEqBase.value(x)
value(x::AbstractArray{<:ReverseDiff.TrackedReal}) = map(value, x)
#+end_src

#+RESULTS:
: value (generic function with 2 methods)

#+begin_src julia
@model function sir_model_v2(
    num_days;
    # Default parameters to set up the problem.
    tspan = (0.0, float(num_days)),
    u0 = [N - 1, 1, 0.0],
    problem = ODEProblem(SIR!, u0, tspan, [0.001, 0.01])
)
    Î² ~ truncated(Normal(2, 1); lower=0)
    Î³ ~ truncated(Normal(0.4, 0.5); lower=0)
    Ï•â»Â¹ ~ Exponential(1/5)
    Ï• = inv(Ï•â»Â¹)

    problem_new = remake(problem, p=[Î², Î³])
    sol = solve(problem_new, saveat=1)
    sol_for_observed = sol[2, 1:14]

    # Observe.
    # Add a small constant to `sol_for_observed` to make things more stable.
    # TODO: Speed up even further using `lazyarray`.
    in_bed ~ arraydist(NegativeBinomial2.(Ï•, sol_for_observed .+ 1e-5))

    # Some quantities we might be interested in.
    return (R = Î² / Î³, recovery_time = 1 / Î³, in_bed = in_bed)
end
#+end_src

#+RESULTS:
: sir_model_v2 (generic function with 2 methods)

#+begin_src julia
model_v2 = sir_model_v2(length(data.in_bed))
model_v2_conditioned = model_v2 | (in_bed = data.in_bed, )
#+end_src

#+RESULTS:
: Model(
:   args = (:num_days, :tspan, :u0, :problem)
:   defaults = (:tspan, :u0, :problem)
:   context = ConditionContext((in_bed = [3, 8, 26, 76, 225, 298, 258, 233, 189, 128, 68, 29, 14, 4],), DynamicPPL.DefaultContext())
: )

#+begin_src julia
# Load the different AD packages.
using ForwardDiff: ForwardDiff
using ReverseDiff: ReverseDiff
using Zygote: Zygote

using SciMLSensitivity
#+end_src

#+RESULTS:

#+begin_src julia
# Load package for convenient benchmarking.
using TuringBenchmarking
#+end_src

#+begin_src julia
suite = TuringBenchmarking.make_turing_suite(
    model_conditioned;
    adbackends=[
        TuringBenchmarking.ForwardDiffAD{40,true}(),
    ]
)
run(suite)
#+end_src

#+RESULTS:
: 2-element BenchmarkTools.BenchmarkGroup:
:   tags: []
:   "linked" => 2-element BenchmarkTools.BenchmarkGroup:
: 	  tags: []
: 	  "evaluation" => Trial(27.966 Î¼s)
: 	  "Turing.Essential.ForwardDiffAD{40, true}()" => Trial(114.943 Î¼s)
:   "not_linked" => 2-element BenchmarkTools.BenchmarkGroup:
: 	  tags: []
: 	  "evaluation" => Trial(29.664 Î¼s)
: 	  "Turing.Essential.ForwardDiffAD{40, true}()" => Trial(136.706 Î¼s)

#+begin_src julia
suite = TuringBenchmarking.make_turing_suite(
    model_v2_conditioned;
    adbackends=[
        TuringBenchmarking.ForwardDiffAD{40,true}(),
    ]
)
run(suite)
#+end_src

#+RESULTS:
: 2-element BenchmarkTools.BenchmarkGroup:
:   tags: []
:   "linked" => 2-element BenchmarkTools.BenchmarkGroup:
: 	  tags: []
: 	  "evaluation" => Trial(19.393 Î¼s)
: 	  "Turing.Essential.ForwardDiffAD{40, true}()" => Trial(95.070 Î¼s)
:   "not_linked" => 2-element BenchmarkTools.BenchmarkGroup:
: 	  tags: []
: 	  "evaluation" => Trial(19.801 Î¼s)
: 	  "Turing.Essential.ForwardDiffAD{40, true}()" => Trial(122.377 Î¼s)

Here it doesn't make much of a difference because we're working with so few observations, but indeed =model_v2= is somewhat faster than =model= for both evaluation and gradient computation (using =ForwardDiff=).

* TODO Showing off
** =Gibbs=

Simple linear regression model

#+begin_src julia
using LinearAlgebra: I

@model function linear_regression(X)
    num_params = size(X, 1)
    Î² ~ MvNormal(ones(num_params))
    ÏƒÂ² ~ InverseGamma(2, 3)
    y ~ MvNormal(vec(Î²' * X), ÏƒÂ² * I)
end

# Generate some dummy data.
X = randn(2, 10_000)
lin_reg = linear_regression(X)
true_vals = rand(lin_reg)

# Condition.
lin_reg_conditioned = lin_reg | (y = true_vals.y,)
#+end_src

#+RESULTS:
: Model(
:   args = (:X,)
:   defaults = ()
:   context = ConditionContext(
:     (y = [0.3242032989648078, 1.0927828698345088, -0.7332477681370562, -5.062789065066403, -0.5510236918192273, 3.849292275620334, -3.1453844688196386, -1.0761023645067103, -0.35920155367909423, 1.1632762258546507  â€¦  1.557561970476609, 3.575676520486435, 0.07694907753939595, -2.102027398588007, -8.469490043524004, 3.4605534596978993, 4.7196306430050665, -1.7976651698542838, -0.6009882061621101, 1.397842282973244],),
:     DynamicPPL.DefaultContext()
:   )
: )

#+REVEAL: split

Can of course just use =HMC= on it

#+begin_src julia
chain_hmc = sample(lin_reg_conditioned, HMC(1e-3, 32), 1_000);
MCMCChains.ess_rhat(chain_hmc)
#+end_src

#+RESULTS:
:RESULTS:
: [32mSampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:00:02[39m
: ESS
:  [1m parameters [0m [1m      ess [0m [1m    rhat [0m [1m ess_per_sec [0m
:  [90m     Symbol [0m [90m  Float64 [0m [90m Float64 [0m [90m     Float64 [0m
: 
:         Î²[1]   433.7061    1.0007      155.9533
:         Î²[2]   434.2894    1.0014      156.1630
:           ÏƒÂ²   749.5058    1.0003      269.5095
: 
:END:

But, how about we combine *Elliptical Slice Sampling (=ESS=)*, which is very effective on Gaussian priors, together /with/ =HMC= on $\sigma^2$ (which is non-Gaussian and so ESS doesn't apply)

#+begin_src julia
chain_ess_hmc = sample(lin_reg_conditioned, Gibbs(ESS(:Î²), HMC(1e-3, 32, :ÏƒÂ²)), 1_000);
MCMCChains.ess_rhat(chain_ess_hmc)
#+end_src

#+RESULTS:
:RESULTS:
: Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:00:02
: ESS
:   parameters        ess      rhat   ess_per_sec 
:       Symbol    Float64   Float64       Float64 
: 
:         Î²[1]    68.1919    1.0140       32.6433
:         Î²[2]   170.8266    1.0017       81.7744
:           ÏƒÂ²   156.3485    1.0043       74.8437
: 
:END:

This is much improved (depending on which random seed you're using)!

* Concluding remarks
** TODO Julia: The Good, the Bad, and the Ugly

An honest take from a little Norwegian boy

*** The Good
- Speed
- Composability (thank you multiple dispatch)
- No need to tie yourself to an underlying computational framework (looking at you Python)
- Interactive
- /Transparency/

Most of these you have already, but the transparency is maybe not something we've seen too much of

#+REVEAL: split

#+begin_src julia
f(x) = 2x
#+end_src

#+RESULTS:
: f (generic function with 1 method)

#+begin_src julia
f(1)
#+end_src

#+RESULTS:
: 2

#+REVEAL: split

You can inspect the type-inferred and lowered code

#+begin_src julia
@code_typed f(1)
#+end_src

#+RESULTS:
: CodeInfo(
: [90m1 â”€[39m %1 = Base.mul_int(2, x)[36m::Int64[39m
: [90mâ””â”€â”€[39m      return %1
: ) => Int64

#+REVEAL: split

You can inspect the LLVM code

#+begin_src julia
@code_llvm f(1)
#+end_src

#+RESULTS:
: [90m;  @ In[2]:1 within `f`[39m
: [95mdefine[39m [36mi64[39m [93m@julia_f_2659[39m[33m([39m[36mi64[39m [95msignext[39m [0m%0[33m)[39m [0m#0 [33m{[39m
: [91mtop:[39m
: [90m; â”Œ @ int.jl:88 within `*`[39m
:    [0m%1 [0m= [96m[1mshl[22m[39m [36mi64[39m [0m%0[0m, [33m1[39m
: [90m; â””[39m
:   [96m[1mret[22m[39m [36mi64[39m [0m%1
: [33m}[39m

#+REVEAL: split

And even the resulting machine code

#+begin_src julia
@code_native f(1)
#+end_src

#+RESULTS:
#+begin_example
	.text
	.file	"f"
	.globl	julia_f_2696                    # -- Begin function julia_f_2696
	.p2align	4, 0x90
	.type	julia_f_2696,@function
julia_f_2696:                           # @julia_f_2696
; â”Œ @ In[2]:1 within `f`
	.cfi_startproc
# %bb.0:                                # %top
; â”‚â”Œ @ int.jl:88 within `*`
	leaq	(%rdi,%rdi), %rax
; â”‚â””
	retq
.Lfunc_end0:
	.size	julia_f_2696, .Lfunc_end0-julia_f_2696
	.cfi_endproc
; â””
                                        # -- End function
	.section	".note.GNU-stack","",@progbits
#+end_example

It really just depends on which level of "I hate my life" you're currently at

*** The Bad

*** The Ugly

*** Overall

* Resources
- https://mc-stan.org/users/documentation/case-studies/boarding_school_case_study.html

* Debugging                                                        :noexport:
- [[/home/tor/.julia/packages/SciMLSensitivity/DSyJO/src/reversediff.jl::67]]
  - [ ] Add definition for =AbstractArray{<:ReverseDiff.TrackedReal}=?
- [[/home/tor/.julia/packages/SciMLSensitivity/DSyJO/src/reversediff.jl::99]]
  - Adjoint definition
- [[~/.julia/packages/ReverseDiff/YkVxM/src/tracked.jl::77]]
  - Complains because =IndexStyle= is incorrect

* Hacks                                                            :noexport:
:PROPERTIES:
:header-args:julia: :session geilo-winter-school :tangle utils.jl :exports both
:END:
** ReverseDiff.jl
*** Allow support of linear indexing
#+begin_src julia :eval no
Pkg.add(url="https://github.com/torfjelde/ReverseDiff.jl", rev="torfjelde/sort-of-support-non-linear-indexing")
#+end_src

#+RESULTS:
#+begin_example
    Updating git-repo `https://github.com/torfjelde/ReverseDiff.jl`
    Updating registry at `~/.julia/registries/General`
    Updating git-repo `https://github.com/JuliaRegistries/General.git`
   Resolving package versions...
  No Changes to `~/Projects/public/Turing-Workshop/2023-Geilo-Winter-School/03-advanced-turing/Project.toml`
  No Changes to `~/Projects/public/Turing-Workshop/2023-Geilo-Winter-School/03-advanced-turing/Manifest.toml`
Precompiling project...
  âœ“ Turing
  âœ“ TuringBenchmarking
  2 dependencies successfully precompiled in 18 seconds. 356 already precompiled.
#+end_example

** SciMLSensitivity.jl
*** DONE Allow =AbstractArray{<:ReverseDiff.TrackedReal}=
#+begin_src julia :eval no
# Also allow `AbstractArray{<:ReverseDiff.TrackedReal}`.
# PR: https://github.com/SciML/SciMLSensitivity.jl/pull/769
using ReverseDiff: ReverseDiff

using DiffEqBase: DiffEqBase
using SciMLSensitivity: SciMLSensitivity
import SciMLSensitivity: AbstractOverloadingSensitivityAlgorithm

function DiffEqBase.solve_up(
    prob::DiffEqBase.DEProblem,
    sensealg::Union{AbstractOverloadingSensitivityAlgorithm,Nothing},
    u0::AbstractArray{<:ReverseDiff.TrackedReal},
    p::AbstractArray{<:ReverseDiff.TrackedReal},
    args...;
    kwargs...
)
    DiffEqBase.solve_up(prob, sensealg, reduce(vcat, u0), reduce(vcat, p), args...; kwargs...)
end

function DiffEqBase.solve_up(
    prob::DiffEqBase.DEProblem,
    sensealg::Union{AbstractOverloadingSensitivityAlgorithm,Nothing},
    u0,
    p::AbstractArray{<:ReverseDiff.TrackedReal},
    args...;
    kwargs...
)
    DiffEqBase.solve_up(prob, sensealg, u0, reduce(vcat, p), args...; kwargs...)
end

function DiffEqBase.solve_up(
    prob::DiffEqBase.DEProblem,
    sensealg::Union{AbstractOverloadingSensitivityAlgorithm,Nothing},
    u0::AbstractArray{<:ReverseDiff.TrackedReal},
    p,
    args...;
    kwargs...
)
    DiffEqBase.solve_up(prob, sensealg, reduce(vcat, u0), p, args...; kwargs...)
end
#+end_src

#+RESULTS:

*** TODO Allow =TrackedArray(::ODESolution)= (depends on https://github.com/JuliaDiff/ReverseDiff.jl/pull/216)
#+begin_src julia :eval no
if isdefined(ReverseDiff, :supports_linear_indexing)
    using DiffEqBase
    ReverseDiff.supports_linear_indexing(::DiffEqBase.ODESolution) = true
end
#+end_src

#+RESULTS:

** DONE TuringBenchmarking.jl
#+begin_src julia :eval no
Pkg.add(url="https://github.com/torfjelde/TuringBenchmarking.jl.git")
#+end_src

#+RESULTS:
:     Updating git-repo `https://github.com/torfjelde/TuringBenchmarking.jl.git`
:    Resolving package versions...
:   No Changes to `~/Projects/public/Turing-Workshop/2023-Geilo-Winter-School/03-advanced-turing/Project.toml`
:   No Changes to `~/Projects/public/Turing-Workshop/202#+begin_srcter-School/03-advanced-turing/Manifest.toml`

** TODO DistributionsAD.jl
#+begin_src julia :eval no
Pkg.add(url="https://github.com/TuringLang/DistributionsAD.jl.git", rev="torfjelde/lazy-array-perf")
#+end_src

#+RESULTS:
#+begin_example
    Updating git-repo `https://github.com/TuringLang/DistributionsAD.jl.git`
   Resolving package versions...
    Updating `~/Projects/public/Turing-Workshop/2023-Geilo-Winter-School/03-advanced-turing/Project.toml`
  [ced4e74d] ~ DistributionsAD v0.6.44 `https://github.com/TuringLang/DistributionsAD.jl.git#torfjelde/lazy-array-perf` â‡’ v0.6.44 `https://github.com/TuringLang/DistributionsAD.jl.git#torfjelde/lazy-array-perf`
    Updating `~/Projects/public/Turing-Workshop/2023-Geilo-Winter-School/03-advanced-turing/Manifest.toml`
  [ced4e74d] ~ DistributionsAD v0.6.44 `https://github.com/TuringLang/DistributionsAD.jl.git#torfjelde/lazy-array-perf` â‡’ v0.6.44 `https://github.com/TuringLang/DistributionsAD.jl.git#torfjelde/lazy-array-perf`
Precompiling project...
  âœ“ DistributionsAD
  âœ“ AdvancedVI
  âœ“ Turing
  âœ“ TuringBenchmarking
  4 dependencies successfully precompiled in 26 seconds. 352 already precompiled.
#+end_example

** MCMCChains.jl
*** Converting into a =Matrix{<:NamedTuple}= into =MCMCChains.Chains=
#+begin_src julia :eval no
using Turing

function unravel(x::NamedTuple, vns=Iterators.map(DynamicPPL.VarName, keys(x)))
    vn_iter = Iterators.flatten(DynamicPPL.TestUtils.varname_leaves(vn, get(x, vn)) for vn in vns)
    return vn_iter, Iterators.map(Base.Fix1(get, x), vn_iter)
end

"""
    to_chains(results; exclude, include)

Return a `MCMCChains.Chains` constructed from `results`.

If `exclude` is specified, then those keys will be excluded.
If `include` is specified, then those keys will be included.
"""
function to_chains(results::AbstractVector{<:NamedTuple}; exclude=nothing, include=nothing)
    @assert !isempty(results)

    # TODO: Handle ragged arrays.
    # Probably best done my first just mapping everything to flatten dictionaries, e.g.
    #
    #     x_vns, x_vals = unravel(x)
    #     OrderedDict(zip(map(Symbol, x_vns), x_vals))
    #
    # (or using Dictionaries.jl for better perf), and then basically `hcat`ing these
    # and insertin `missing` where necessary.

    example = first(results)

    # Construct names once.
    syms = collect(keys(example))
    # Filter out if desired.
    if !isnothing(include)
        syms = filter(âˆˆ(include), syms)
    end
    if !isnothing(exclude)
        syms = filter(âˆ‰(exclude), syms)
    end
    # Convert to `VarName`.
    names = collect(first(unravel(example, map(DynamicPPL.VarName, syms))))

    # Extract the values.
    vals = mapreduce(hcat, results) do x
        # NOTE: Providing the `names` here assumes every sample has the same variables.
        collect(last(unravel(x, names)))
    end
    vals_transposed = transpose(vals)
    # Shape: iterations Ã— num_vars Ã— num_chains
    chain_array = reshape(vals_transposed, size(vals_transposed)..., 1)
    return MCMCChains.Chains(chain_array, names)
end

function to_chains(results::AbstractMatrix{<:NamedTuple}; kwargs...)
    return reduce(MCMCChains.chainscat, map(col -> to_chains(col; kwargs...), eachcol(results));)
end
#+end_src

#+RESULTS:
: to_chains (generic function with 2 methods)

#+begin_src julia :eval no
# TODO: Make PR to DPPL.
function DynamicPPL.TestUtils.varname_leaves(vn::DynamicPPL.VarName, val::NamedTuple)
    iter = Iterators.map(keys(val)) do sym
        lens = DynamicPPL.Setfield.PropertyLens{sym}()
        DynamicPPL.TestUtils.varname_leaves(vn âˆ˜ lens, get(val, lens))
    end
    return Iterators.flatten(iter)
end
#+end_src

#+RESULTS:

#+begin_src julia :eval no
to_chains([(x = 1, y = [2,2], z = (w = [5],)), (x = 3, y = [4,4], z = (w = [6],))])
#+end_src

#+RESULTS:
#+begin_example
Chains MCMC chain (2Ã—4Ã—1 reshape(transpose(::Matrix{Int64}), 2, 4, 1) with eltype Int64):

Iterations        = 1:1:2
Number of chains  = 1
Samples per chain = 2
parameters        = x, y[1], y[2], z.w[1]

Summary Statistics
  parameters      mean       std   naive_se      mcse       ess      rhat 
      Symbol   Float64   Float64    Float64   Float64   Missing   Missing 

           x    2.0000    1.4142     1.0000    1.0000   missing   missing
        y[1]    3.0000    1.4142     1.0000    1.0000   missing   missing
        y[2]    3.0000    1.4142     1.0000    1.0000   missing   missing
      z.w[1]    5.5000    0.7071     0.5000    0.5000   missing   missing

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           x    1.0500    1.5000    2.0000    2.5000    2.9500
        y[1]    2.0500    2.5000    3.0000    3.5000    3.9500
        y[2]    2.0500    2.5000    3.0000    3.5000    3.9500
      z.w[1]    5.0250    5.2500    5.5000    5.7500    5.9750
#+end_example

** Turing.jl
* Debug                                                            :noexport:
** https://github.com/JuliaDiff/ReverseDiff.jl/pull/216?notification_referrer_id=NT_kwDOAKj85LM1MzQ1OTA1MDM5OjExMDc0Nzg4
#+begin_src julia :eval no
using ReverseDiff: ReverseDiff

import SciMLBase
import DiffEqBase
import DiffEqBase: solve_up
ReverseDiff.@grad function solve_up(prob, sensealg, u0, p, args...; kwargs...)
    out = DiffEqBase._solve_adjoint(prob, sensealg, ReverseDiff.value(u0),
                                    ReverseDiff.value(p),
                                    SciMLBase.ReverseDiffOriginator(), args...; kwargs...)
    function actual_adjoint(_args...)
        original_adjoint = out[2](_args...)
        if isempty(args) # alg is missing
            tuple(original_adjoint[1:4]..., original_adjoint[6:end]...)
        else
            original_adjoint
        end
    end
    out[1], actual_adjoint
end

function ReverseDiff.TrackedArray(sol::DiffEqBase.ODESolution)
    DiffEqBase.ODESolution(
        TrackedArray(sol.u),
        sol.u_analytic,
        sol.errors,
        sol.t,
        sol.k,
        sol.prob,
        sol.alg,
        sol.interp,
        sol.dense,
        sol.tslocation,
        sol.destats,
        sol.alg_choice,
        sol.retcode
    )
end

expr = @macroexpand ReverseDiff.@grad function f(x; y=1)
    return x, identity
end
expr |> Base.remove_linenums!

# This ends up being called in the forward pass (see the expansion of the `ReverseDiff.@grad` above),
# and so we could potentially overload this `track` to call to just track the internal solution instead.
# FIXME: Currently doesn't work because a) `sol.u isa Vector{<:Vector}` and b) `ODESolution(::Vector{<:TrackedVector})`
# doesn't work either.
function ReverseDiff.track(::ODESolution{T,N}, tp::Vector{ReverseDiff.AbstractInstruction}=ReverseDiff.InstructionTape()) where {T,N}

    u_tracked = map(Base.Fix2(ReverseDiff.track, tp), sol.u)
    Ttracked = eltype(eltype(u_tracked))  # TODO: Infer from `T` instead?
    DiffEqBase.ODESolution{Ttracked,N}(
        u_tracked,
        sol.u_analytic,
        sol.errors,
        sol.t,
        sol.k,
        sol.prob,
        sol.alg,
        sol.interp,
        sol.dense,
        sol.tslocation,
        sol.destats,
        sol.alg_choice,
        sol.retcode
    )
end

function ReverseDiff.track(::typeof(solve_up), _args...; kwargs...)
    # TODO: Insert adjoint def here.
    sol_up_pullback(args...; kwargs...) = 0, identity
    args = (x,)
    tp = ReverseDiff.tape(args...)
    output_value, back = sol_up_pullback(args...; kwargs...)
    output = ReverseDiff.track(output_value, tp)
    ReverseDiff.record!(
        tp,
        ReverseDiff.SpecialInstruction,
        solve_up,
        args,
        output,
        (back, sol_up_pullback, kwargs)
    )
    return output
end

problem_tracked = remake(problem, p=ReverseDiff.track([0.001, 0.01]))
sol_tracked = solve(problem_tracked)

DiffEqBase.ODESolution(
    map(ReverseDiff.track, sol.u),
    sol.u_analytic,
    sol.errors,
    sol.t,
    sol.k,
    sol.prob,
    sol.alg,
    sol.interp,
    sol.dense,
    sol.tslocation,
    sol.destats,
    sol.alg_choice,
    sol.retcode
)
typeof(sol)

sol.u


#+end_src

#+RESULTS:

** https://github.com/TuringLang/Turing.jl/issues/1934
#+begin_src julia :eval no
Pkg.develop(path="/home/tor/Projects/public/ReverseDiff.jl/")
#+end_src

#+RESULTS:
: [32m[1m   Resolving[22m[39m package versions...
: [32m[1m    Updating[22m[39m `~/Projects/public/Turing-Workshop/2023-Geilo-Winter-School/03-advanced-turing/Project.toml`
:  [90m [37e2e3b7] [39m[93m~ ReverseDiff v1.14.4 â‡’ v1.14.4 `~/Projects/public/ReverseDiff.jl`[39m
: [32m[1m    Updating[22m[39m `~/Projects/public/Turing-Workshop/2023-Geilo-Winter-School/03-advanced-turing/Manifest.toml`
:  [90m [37e2e3b7] [39m[93m~ ReverseDiff v1.14.4 â‡’ v1.14.4 `~/Projects/public/ReverseDiff.jl`[39m

#+begin_src julia :eval no
using ForwardDiff: ForwardDiff
using ReverseDiff: ReverseDiff
using Zygote: Zygote

# Also allow `AbstractArray{<:ReverseDiff.TrackedReal}`.
# PR: https://github.com/SciML/SciMLSensitivity.jl/pull/769
using SciMLSensitivity: SciMLSensitivity
import SciMLSensitivity: AbstractOverloadingSensitivityAlgorithm

# Allow 
const RT = AbstractArray{<:ReverseDiff.TrackedReal}
function DiffEqBase.solve_up(prob::DiffEqBase.DEProblem,
                             sensealg::Union{AbstractOverloadingSensitivityAlgorithm,
                                             Nothing}, u0::RT,
                             p::RT, args...; kwargs...)
    DiffEqBase.solve_up(prob, sensealg, reduce(vcat, u0), reduce(vcat, p), args...; kwargs...)
end

function DiffEqBase.solve_up(prob::DiffEqBase.DEProblem,
                             sensealg::Union{AbstractOverloadingSensitivityAlgorithm,
                                             Nothing}, u0, p::RT,
                             args...; kwargs...)
    DiffEqBase.solve_up(prob, sensealg, u0, reduce(vcat, p), args...; kwargs...)
end

function DiffEqBase.solve_up(prob::DiffEqBase.DEProblem,
                             sensealg::Union{AbstractOverloadingSensitivityAlgorithm,
                                             Nothing}, u0::RT, p,
                             args...; kwargs...)
    DiffEqBase.solve_up(prob, sensealg, reduce(vcat, u0), p, args...; kwargs...)
end
#+end_src

#+begin_src julia :eval no
problem = ODEProblem(SIR!, u0, tspan, [0.001, 0.01])
sol = solve(problem)

problem_tracked = remake(problem, p=ReverseDiff.track([0.001, 0.01]))
# problem_tracked = ODEProblem(SIR!, u0, tspan, ReverseDiff.track([0.001, 0.01]))
sol_tracked = solve(problem_tracked)
sol_tracked = DiffEqBase.solve_up(problem_tracked, nothing, u0, ReverseDiff.track([0.001, 0.01]))

sol_tracked

ReverseDiff.value(sol_tracked)

f(p) = sum(solve(remake(problem, p=p)))

x = [1.75,0 .7]
ReverseDiff.gradient(f, x)
first(Zygote.gradient(f, [0.1, 0.1])) â‰ˆ ReverseDiff.gradient(f, [0.1, 0.1])

ForwardDiff.gradient(f, [0.001, 0.01])

#+end_src

#+begin_src julia :eval no
using LogDensityProblems, LogDensityProblemsAD
using SciMLSensitivity

@model demo() = (x ~ truncated(Normal(); lower=0); y ~ Normal(x, 1))

m = demo() | (y = 1.0,)
f = Turing.LogDensityFunction(m)

LogDensityProblems.logdensity_and_gradient(ADgradient(:ReverseDiff, f; compile=Val(true)), f.varinfo[:])

function Turing.LogDensityFunction(model::DynamicPPL.Model)
    return Turing.LogDensityFunction(
        DynamicPPL.VarInfo(model),
        model,
        DynamicPPL.SampleFromPrior(),
        DynamicPPL.DefaultContext(),
    )
end

f = Turing.LogDensityFunction(model_v2_conditioned)
LogDensityProblems.logdensity_and_gradient(ADgradient(:ReverseDiff, f; compile=Val(true)), f.varinfo[:])
LogDensityProblems.logdensity_and_gradient(ADgradient(:ReverseDiff, h; compile=Val(false)), f.varinfo[:])


LogDensityProblems.logdensity_and_gradient(ADgradient(:ReverseDiff, h; compile=Val(false)), f.varinfo[:])

LogDensityProblems.logdensity(::typeof(h), Î¸::AbstractVector) = h(Î¸)
LogDensityProblems.capabilities(::typeof(h)) = LogDensityProblems.LogDensityOrder{0}()
LogDensityProblems.dimension(::typeof(h)) = 2
LogDensityProblems.logdensity_and_gradient(ADgradient(:ReverseDiff, h; compile=Val(true)), vi[:][1:2])
LogDensityProblems.logdensity_and_gradient(ADgradient(:ReverseDiff, h; compile=Val(false)), vi[:][1:2])


âˆ‚f = ADgradient(:ReverseDiff, f; compile=Val(true))
LogDensityProblems.logdensity_and_gradient(âˆ‚f, vi[:])

LogDensityProblems.logdensity_and_gradient(ADgradient(:ReverseDiff, f), vi[:])
LogDensityProblems.logdensity_and_gradient(ADgradient(:ForwardDiff, f), vi[:])

LogDensityProblems.logdensity_and_gradient(ADgradient(:ReverseDiff, f), vi[:])
LogDensityProblems.logdensity_and_gradient(ADgradient(:ForwardDiff, f), vi[:])

âˆ‚f = ADgradient(:ReverseDiff, f)
LogDensityProblems.logdensity_and_gradient(âˆ‚f, vi[:])

Zygote.gradient(loss, [0.001, 0.01])
ReverseDiff.gradient(loss, [0.001, 0.01])

problem = ODEProblem(SIR!, u0, tspan, ReverseDiff.track([0.001, 0.01]))
problem.f(zero(u0), u0, ReverseDiff.track([0.001, 0.01]), 1)
problem = ODEProblem(SIR!, u0, tspan, [0.001, 0.01])
sol = solve(problem)
Array(sol)
sol[2, 1:14]
typeof(sol)

?solve

@code_warntype solve(problem)

import SciMLBase
import DiffEqBase
import DiffEqBase: solve_up
ReverseDiff.@grad function solve_up(prob, sensealg, u0, p, args...; kwargs...)
    out = DiffEqBase._solve_adjoint(prob, sensealg, ReverseDiff.value(u0),
                                    ReverseDiff.value(p),
                                    SciMLBase.ReverseDiffOriginator(), args...; kwargs...)
    function actual_adjoint(_args...)
        original_adjoint = out[2](_args...)
        if isempty(args) # alg is missing
            tuple(original_adjoint[1:4]..., original_adjoint[6:end]...)
        else
            original_adjoint
        end
    end
    Array(out[1]), actual_adjoint
end

Base.IndexStyle(::ODESolution) = IndexCartesian()

problem = ODEProblem(SIR!, u0, tspan, [0.001, 0.01])
sol = solve(problem)

problem_tracked = remake(problem, p=ReverseDiff.track([0.001, 0.01]))
problem_tracked = ODEProblem(SIR!, u0, tspan, ReverseDiff.track([0.001, 0.01]))
sol_tracked = DiffEqBase.solve_up(problem_tracked, nothing, u0, ReverseDiff.track([0.001, 0.01]))
sol_tracked.value.retcode

sol_tracked.deriv

ReverseDiff.gradient(loss, [0.001, 0.01])

IndexStyle(sol)
typeof(sol)
typeof(sol_tracked)
typeof(sol_tracked.value)
print(sol_tracked.value)

size(sol_tracked)

@which IndexStyle(sol)

IndexStyle(typeof(sol))
IndexStyle(Matrix{Float64})

IndexStyle(sol.u)

function h(Î¸)
    sol = solve(remake(problem, p=[Î¸[1], Î¸[2]]), saveat=1)
    # @assert ReverseDiff.value(sol) isa ODESolution
    sum(sol)
end
ReverseDiff.gradient(h, [0.001, 0.01])

p_tracked = ReverseDiff.track([0.001, 0.01])
vcat(p_tracked[1], p_tracked[2])
[p_tracked[1], p_tracked[2]]

# ATTEMPT
import SciMLSensitivity: AbstractOverloadingSensitivityAlgorithm

const RT = AbstractArray{<:ReverseDiff.TrackedReal}
function DiffEqBase.solve_up(prob::DiffEqBase.DEProblem,
                             sensealg::Union{AbstractOverloadingSensitivityAlgorithm,
                                             Nothing}, u0::RT,
                             p::RT, args...; kwargs...)
    DiffEqBase.solve_up(prob, sensealg, reduce(vcat, u0), reduce(vcat, p), args...; kwargs...)
end

function DiffEqBase.solve_up(prob::DiffEqBase.DEProblem,
                             sensealg::Union{AbstractOverloadingSensitivityAlgorithm,
                                             Nothing}, u0, p::RT,
                             args...; kwargs...)
    DiffEqBase.solve_up(prob, sensealg, u0, reduce(vcat, p), args...; kwargs...)
end

function DiffEqBase.solve_up(prob::DiffEqBase.DEProblem,
                             sensealg::Union{AbstractOverloadingSensitivityAlgorithm,
                                             Nothing}, u0::RT, p,
                             args...; kwargs...)
    DiffEqBase.solve_up(prob, sensealg, reduce(vcat, u0), p, args...; kwargs...)
end

ReverseDiff.collect(p_tracked)
vcat(p_tracked[1], p_tracked[2])

reduce(vcat, [p_tracked[1], p_tracked[2]])


## ReverseDiff.jl debugging
tape = ReverseDiff.GradientTape(f, (f.varinfo[:], ))
compiled_tape = ReverseDiff.compile(tape)
compiled_tape.tape.tape
compiled_tape.forward_exec
compiled_tape.reverse_exec[end]()
#+end_src

** ReverseDiffDebugUtils.jl
#+begin_src julia :eval no
using Pkg; Pkg.activate(mktempdir())
Pkg.develop("GraphGraphviz")
Pkg.develop("ReverseDiffDebugUtils")
# Pkg.add(name="Distributions", version="0.25.76")
using GraphGraphviz, ReverseDiffDebugUtils
using Turing, LogDensityProblems, LogDensityProblemsAD
using ReverseDiff

function Turing.LogDensityFunction(model::DynamicPPL.Model)
    return Turing.LogDensityFunction(
        DynamicPPL.VarInfo(model),
        model,
        DynamicPPL.SampleFromPrior(),
        DynamicPPL.DefaultContext(),
    )
end

### Setup ###
function sim(I, P)
    yvec = Vector{Int}(undef, I * P)
    ivec = similar(yvec)
    pvec = similar(yvec)

    beta = rand(Normal(), I)
    theta = rand(Normal(), P)

    n = 0
    for i in 1:I, p in 1:P
        n += 1
        ivec[n] = i
        pvec[n] = p
        yvec[n] = rand(BernoulliLogit(theta[p] - beta[i]))
    end

    return yvec, ivec, pvec, theta, beta
end

P = 1000
y, i, p, _, _ = sim(20, P);

### Turing ###
# naive implementation
@model function irt_naive(y, i, p; I = maximum(i), P = maximum(p))
    theta ~ filldist(Normal(), P)
    beta ~ filldist(Normal(), I)

    for n in eachindex(y)
        y[n] ~ Bernoulli(logistic(theta[p[n]] - beta[i[n]]))
    end
end

# performant model
@model function irt(y, i, p; I = maximum(i), P = maximum(p))
    theta ~ filldist(Normal(), P)
    beta ~ filldist(Normal(), I)
    Turing.@addlogprob! sum(logpdf.(BernoulliLogit.(theta[p] .- beta[i]), y))

    return (; theta, beta)
end

# Instantiate
model = irt(y, i, p);

â„“ = Turing.LogDensityFunction(model)
Î¸ = â„“.varinfo[:]
f = Base.Fix1(LogDensityProblems.logdensity, â„“)

plotgraphviz(f, Î¸; display=true)

# Check if ForwardDiff breaks.
using UnPack
beta, theta = model();
x = vcat(theta, beta)

function evaluate(model::Turing.Model, x)
    @unpack y, i, p, I, P = model.args
    theta, beta = x[1:P], x[P + 1:end]
    return sum(logpdf.(BernoulliLogit.(theta[p] - beta[i]), y))
end

using ForwardDiff
ForwardDiff.gradient(Base.Fix1(evaluate, model), x)

## ReverseDiff
# pre-record a GradientTape for `f` using inputs of shape 100x100 with Float64 elements
const f_tape = GradientTape(f, (rand(100, 100), rand(100, 100)))

# compile `f_tape` into a more optimized representation
const compiled_f_tape = compile(f_tape)

# some inputs and work buffers to play around with
a, b = rand(100, 100), rand(100, 100)
inputs = (a, b)
results = (similar(a), similar(b))
all_results = map(DiffResults.GradientResult, results)
cfg = GradientConfig(inputs)


julia> tape = ReverseDiff.GradientTape(prob, (Î¸,));

julia> ctape = ReverseDiff.compile(tape);

julia> inputs = (Î¸,);

julia> results = (similar(Î¸), );

julia> cfg = ReverseDiff.GradientConfig(inputs);

julia> ReverseDiff.gradient!(results, ctape, inputs)
([2.1135668756925874, 6.145143149741308, 1.48401765986875, 0.6022383996781915, -0.5138652356820355, -8.103361690307636, 7.128312271724506, 12.584105906811907, -0.7254654355635614, 5.5804778350827045  â€¦  -380.5829914906268, -123.15954482703319, 24.61418749530236, 307.33373871567056, 355.8706214939681, 117.79962719545168, 137.4162416377521, 121.1634598283426, 89.1211227217943, 47.44409385216913],)


# Using `DistributionsAD.flatten` to address performance.
using Distributions, DistributionsAD
using ConstructionBase
using ConcreteStructs

"""
    get_logpdf_expr(Tdist)

Return a flattened method for computing the logpdf of `Tdist`.
"""
function get_logpdf_expr(Tdist)
    x = gensym()
    fnames = fieldnames(Tdist)
    func = Expr(:->, 
                Expr(:tuple, fnames..., x), 
                Expr(:block,
                     Expr(:call, :logpdf,
                          Expr(:call, :($(Tdist)), fnames...),
                          x,
                          )
                     )
                )
    return :(flatten(::Type{<:$Tdist}) = $func)
end

make_logpdf_closure(::Type{D}) where {D} = (x, args...) -> logpdf(D(args...), x)

# 1. Use `flatten` to extract a, well, flattened `logpdf`.
eval(get_logpdf_expr(BernoulliLogit))

# 2. [OPTIONAL] Use `StructArrays.jl` to avoid the initial call to the constructor entirely.

# 3. Define a "fast" logpdf method.
@generated function fast_logpdf(
    dist::Product{V,D,<:StructVector{<:Any,<:NamedTuple{names}}},
    x::AbstractArray
) where {V,D<:UnivariateDistribution,names}
    # Get the flatten expression.
    f = flatten(D)

    args = [:(dist.v.$n) for n in names]
    return :(sum($f.($(args...), x)))
end

# HACK: Constructor which doesn't apply the schema.
function StructArrayNoSchema(::Type{T}, cols::C) where {T, C<:StructArrays.Tup}
    N = isempty(cols) ? 1 : ndims(cols[1])
    StructArrays.StructArray{T, N, typeof(cols)}(cols)
end

@generated function fast_logpdf(
    dist::Product{V,D,<:StructVector{<:Any,<:NamedTuple{names}}},
    x::AbstractArray
) where {V,D<:UnivariateDistribution,names}
    # Get the flatten expression.
    f = make_logpdf_closure(D)

    args = [:(dist.v.$n) for n in names]
    return :(sum($f.(x, $(args...))))
end

@generated function fast_logpdf(
    dist::Product{V,D,<:StructVector{<:Any,<:NTuple{N}}},
    x::AbstractArray
) where {V,D<:UnivariateDistribution,N}
    # Get the flatten expression.
    f = make_logpdf_closure(D)

    args = [:(StructArrays.component(dist.v, $i)) for i = 1:N]
    return :(sum($f.(x, $(args...))))
end


# 4. Convenience method for constructing `StructArray` without 
function DistributionsAD.arraydist(::Type{D}, args...) where {D<:Distribution}
    return DistributionsAD.arraydist(D, args)
end
DistributionsAD.arraydist(::Type{D}; args...) where {D<:Distribution} = DistributionsAD.arraydists(D, NamedTuple(args))
function DistributionsAD.arraydist(d::Type{D}, args::NamedTuple) where {D<:Distribution}
    return DistributionsAD.arraydist(StructArrayNoSchema(d, args))
end
function DistributionsAD.arraydist(d::Type{D}, args::Tuple) where {D<:Distribution}
    return DistributionsAD.arraydist(StructArrayNoSchema(d, args))
end

# 5. Type-piracy so we can make use of `~`.
function Distributions.logpdf(dist::Product{<:Any,<:UnivariateDistribution,<:StructVector}, x::AbstractVector{<:Real})
    return fast_logpdf(dist, x)
end


@model function irt_vroom(y, i, p; I = maximum(i), P = maximum(p))
    theta ~ filldist(Normal(), P)
    beta ~ filldist(Normal(), I)
    y ~ arraydist(BernoulliLogit, theta[p] - beta[i])

    return (; theta, beta)
end
model = irt_vroom(y, i, p);
suite = TuringBenchmarking.make_turing_suite(
    model,
    adbackends = [TuringBenchmarking.ForwardDiffAD{40}(), TuringBenchmarking.ReverseDiffAD{true}()]
);
run(suite)
#+end_src

** Implementation of ABC

#+begin_src julia :eval no
using Turing: OrderedDict
using Turing.DynamicPPL: AbstractPPL

@model function demo()
    x ~ Normal()
    y ~ Normal(x, 1)
end

model = demo() | (y = 2.0, )

function split_latent_data(d::OrderedDict, data_variable)
    ks = collect(keys(d))
    data_keys = filter(ks) do k
        AbstractPPL.subsumes(data_variable, k)
    end
    Î¸ = map(Base.Fix1(getindex, d), filter(âˆ‰(data_keys), ks))
    data = map(Base.Fix1(getindex, d), data_keys)

    return Î¸, data
end


struct ABC{F,V,T} <: AbstractMCMC.AbstractSampler
    stat::F
    data_variable::V
    threshold::T
end

ABC(data_var, threshold) = ABC(identity, data_var, threshold)

compute_distance(sampler::ABC, data_true, data_candidate) = mean(abs2.(data_true .- data_candidate))

function AbstractMCMC.step(rng::Random.AbstractRNG, model::DynamicPPL.Model, sampler::ABC; kwargs...)
    # NOTE: Only works if you've used the `model | (x=..., )` conditioning functionality.
    data_true = get(DynamicPPL.conditioned(model), sampler.data_variable)

    joint_model = DynamicPPL.decondition(model, sampler.data_variable)
    d = rand(Turing.OrderedDict, joint_model)
    # Figure out which variables represents data.
    Î¸, _ = split_latent_data(d, sampler.data_variable)
    return Î¸, Î¸
end

function AbstractMCMC.step(rng::Random.AbstractRNG, model::DynamicPPL.Model, sampler::ABC, Î¸_current; kwargs...)
    # NOTE: Only works if you've used the `model | (x=..., )` conditioning functionality.
    data_true = get(DynamicPPL.conditioned(model), sampler.data_variable)

    joint_model = DynamicPPL.decondition(model, sampler.data_variable)
    d = rand(Turing.OrderedDict, joint_model)
    Î¸_candidate, data_candidate = split_latent_data(d, sampler.data_variable)

    dist = compute_distance(sampler, data_true, data_candidate)

    # TODO Use `threshold` from sampler.
    threshold = 0.1
    Î¸_next = dist < threshold ? Î¸_candidate : Î¸_current

    return Î¸_next, Î¸_next
end

function AbstractMCMC.bundle_samples(
    samples::AbstractVector{<:AbstractVector{<:Real}}, model::DynamicPPL.Model, sampler::ABC, ::Any, ::Type{MCMCChains.Chains};
    param_names=missing, discard_initial=0, thinning=1
)
    # Check if we received any parameter names.
    if ismissing(param_names)
        param_names = [Symbol(:param_, i) for i in 1:length(keys(samples[1]))]
    else
        # Generate new array to be thread safe.
        param_names = Symbol.(param_names)
    end

    return MCMCChains.Chains(samples, param_names, (parameters = param_names,); start=discard_initial + 1, thin=thinning)
end
#+end_src

#+RESULTS:

*** Testing it
#+begin_src julia :eval no
rng = Random.MersenneTwister(42)
spl = ABC(@varname(y), 0.1)
Î¸, _ = AbstractMCMC.step(rng, model, spl)
Î¸, _ = AbstractMCMC.step(rng, model, spl, Î¸);
Î¸
#+end_src

#+RESULTS:
: 1-element Vector{Float64}:
:  -0.3010545352363764


#+begin_src julia :eval no
samples = AbstractMCMC.sample(model, spl, 10_000)
chain = AbstractMCMC.bundle_samples(samples, model, spl, first(samples), MCMCChains.Chains; param_names=[:x])
#+end_src

#+RESULTS:
#+begin_example
Chains MCMC chain (10000Ã—1Ã—1 Array{Float64, 3}):

Iterations        = 1:1:10000
Number of chains  = 1
Samples per chain = 10000
parameters        = x

Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m naive_se [0m [1m    mcse [0m [1m      ess [0m [1m    rhat [0m
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m  Float64 [0m [90m Float64 [0m

           x    0.9199    0.7270     0.0073    0.0340   395.2847    0.9999

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m   25.0% [0m [1m   50.0% [0m [1m   75.0% [0m [1m   97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m

           x   -0.4766    0.4041    0.9247    1.3853    2.2981
#+end_example

#+begin_src julia :eval no
plot(chain)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b12d5fdf6ab17538f62e35571fde7d083632df95.svg]]
